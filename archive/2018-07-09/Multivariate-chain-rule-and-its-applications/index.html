<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="en,zh-Hans,ja,Latn,default">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" media="all" href="/lib/Han/dist/han.min.css?v=3.3">




<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Monda,Arial,STXingkai,"华文行楷",STKaiti,“华文楷体”,STFangsong,Lato:300,300italic,400,400italic,700,700italic|Roboto Slab,STKaiti,"华文楷体":300,300italic,400,400italic,700,700italic|Arial,STXingkai,华文行楷,STKaiti,华文楷体,STFangsong,华文仿宋:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Gradient Descent,Chain Rule,Neural Network,Joacbian,Backpropagation," />










<meta name="description" content="学习目标   应用多元链式法则对复合函数求梯度 解释神经网络的结构和其代表的函数 解释神经网络的结构和其代表的函数 编程实现反向传播算法   多元函数链式法则（Chain Rule）上节课讲过基于 Chain rule 我们可以求出3元复合函数的全导数(参考第二课，《多元函数的导数》)。现在我们将这概念拓展到 n元函数： 存在 n元实值函数 $f(\mathbf{x})=f(x_1,x_2,…,x">
<meta name="keywords" content="Gradient Descent,Chain Rule,Neural Network,Joacbian,Backpropagation">
<meta property="og:type" content="article">
<meta property="og:title" content="三.多元链式法则及其应用">
<meta property="og:url" content="https://asiagood.github.io/archive/2018-07-09/Multivariate-chain-rule-and-its-applications/index.html">
<meta property="og:site_name" content="逝川">
<meta property="og:description" content="学习目标   应用多元链式法则对复合函数求梯度 解释神经网络的结构和其代表的函数 解释神经网络的结构和其代表的函数 编程实现反向传播算法   多元函数链式法则（Chain Rule）上节课讲过基于 Chain rule 我们可以求出3元复合函数的全导数(参考第二课，《多元函数的导数》)。现在我们将这概念拓展到 n元函数： 存在 n元实值函数 $f(\mathbf{x})=f(x_1,x_2,…,x">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://asiagood.github.io/archive/2018-07-09/Multivariate-chain-rule-and-its-applications/simplenn.png">
<meta property="og:image" content="https://asiagood.github.io/archive/2018-07-09/Multivariate-chain-rule-and-its-applications/simplenn_details.png">
<meta property="og:image" content="https://asiagood.github.io/archive/2018-07-09/Multivariate-chain-rule-and-its-applications/1hiddenlayernn.png">
<meta property="og:image" content="https://asiagood.github.io/archive/2018-07-09/Multivariate-chain-rule-and-its-applications/mlps.png">
<meta property="og:image" content="https://asiagood.github.io/archive/2018-07-09/Multivariate-chain-rule-and-its-applications/minimization1.png">
<meta property="og:image" content="https://asiagood.github.io/archive/2018-07-09/Multivariate-chain-rule-and-its-applications/minimization2.png">
<meta property="og:image" content="https://asiagood.github.io/archive/2018-07-09/Multivariate-chain-rule-and-its-applications/basic_nntraining.png">
<meta property="og:image" content="https://asiagood.github.io/archive/2018-07-09/Multivariate-chain-rule-and-its-applications/mlps.png">
<meta property="og:image" content="https://asiagood.github.io/archive/2018-07-09/Multivariate-chain-rule-and-its-applications/gradient_meaning.png">
<meta property="og:image" content="https://asiagood.github.io/archive/2018-07-09/Multivariate-chain-rule-and-its-applications/3layernn.png">
<meta property="og:updated_time" content="2018-07-16T10:46:06.651Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="三.多元链式法则及其应用">
<meta name="twitter:description" content="学习目标   应用多元链式法则对复合函数求梯度 解释神经网络的结构和其代表的函数 解释神经网络的结构和其代表的函数 编程实现反向传播算法   多元函数链式法则（Chain Rule）上节课讲过基于 Chain rule 我们可以求出3元复合函数的全导数(参考第二课，《多元函数的导数》)。现在我们将这概念拓展到 n元函数： 存在 n元实值函数 $f(\mathbf{x})=f(x_1,x_2,…,x">
<meta name="twitter:image" content="https://asiagood.github.io/archive/2018-07-09/Multivariate-chain-rule-and-its-applications/simplenn.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://asiagood.github.io/archive/2018-07-09/Multivariate-chain-rule-and-its-applications/"/>





  <title>三.多元链式法则及其应用 | 逝川</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">逝川</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">此间且为等闲事，花开花谢不知年</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-主页">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            主页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-归档">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-关于">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="st-search-show-outputs">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://asiagood.github.io/archive/2018-07-09/Multivariate-chain-rule-and-its-applications/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="popoblue">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="逝川">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">三.多元链式法则及其应用</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-09T11:47:08+08:00">
                2018-07-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/课程笔记/" itemprop="url" rel="index">
                    <span itemprop="name">课程笔记</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/课程笔记/Mathematics-for-Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Mathematics for Machine Learning</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/课程笔记/Mathematics-for-Machine-Learning/Multivariate-Calculus/" itemprop="url" rel="index">
                    <span itemprop="name">Multivariate Calculus</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/archive/2018-07-09/Multivariate-chain-rule-and-its-applications/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/archive/2018-07-09/Multivariate-chain-rule-and-its-applications/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/archive/2018-07-09/Multivariate-chain-rule-and-its-applications/" class="leancloud_visitors" data-flag-title="三.多元链式法则及其应用">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <p>学习目标</p>
<blockquote>
<ol>
<li>应用多元链式法则对复合函数求梯度</li>
<li>解释神经网络的结构和其代表的函数</li>
<li>解释神经网络的结构和其代表的函数</li>
<li>编程实现反向传播算法</li>
</ol>
</blockquote>
<h1 id="多元函数链式法则（Chain-Rule）"><a href="#多元函数链式法则（Chain-Rule）" class="headerlink" title="多元函数链式法则（Chain Rule）"></a>多元函数链式法则（Chain Rule）</h1><p>上节课讲过基于 Chain rule 我们可以求出3元复合函数的全导数(参考第二课，《多元函数的导数》)。现在我们将这概念拓展到 n元函数：</p>
<p>存在 n元实值函数 $f(\mathbf{x})=f(x_1,x_2,…,x_n)$，而且每个自变量$x_i(i=1,…,n)$都是另外一个变量 t 的函数，即： $x_i = x_i(t)$. 现在，我们需要计算 $\frac{df}{dt}=$ ?</p>
<p>$$<br>\text{Gradient }=\frac{\partial f}{\partial \mathbf{x}} = \begin{bmatrix}\frac{\partial f}{\partial x_1}\\ \frac{\partial f}{\partial x_2}\\ \vdots\\ \frac{\partial f}{\partial x_n} \end{bmatrix},<br>\text{derivatives of each component of } \mathbf{x}\text{ with respect to t is }\frac{d\mathbf{x}}{dt}=<br>\begin{bmatrix}<br>\frac{dx_1}{dt} \\<br>\frac{dx_2}{dt}\\<br>\vdots \\<br>\frac{dx_n}{dt}<br>\end{bmatrix}<br>$$</p>
<p>回想之前我们采用链式法则求解 total derivatives的方式，我们有：</p>
<p>$$<br>\begin{align}<br>\frac{df}{dt} &amp;= \frac{\partial f}{\partial x_1}\frac{dx_1}{dt} + \frac{\partial f}{\partial x_2}\frac{dx_2}{dt} +\cdots +<br>\frac{\partial f}{\partial x_n} \frac{dx_n}{dt} \\<br>&amp;=  \color{blue}{\frac{\partial f}{\partial \mathbf{x}}} \cdot \color{green}{\frac{d\mathbf{x}}{dt}}(\nabla_{\mathbf{x}}f(\mathbf{x})\cdot\frac{d{\mathbf{x}}}{dt})\\<br>&amp;= \color{blue}{\mathbf{J}_f(\mathbf{x})} \color{green}{\mathbf{J_x}(t)}<br>\end{align}<br>$$</p>
<p><strong>说明：</strong></p>
<ol>
<li><p>对于n元实值函数来说，gradient 都是column vector！因此上式中可以使用 “点积”：$\cdot$</p>
</li>
<li><p>对于n元实值函数来说，gradient 是 Jacobian的转置，这毋庸置疑。使用Jacobian的时候，采用的是矩阵乘法来表示计算。</p>
</li>
<li><p>可对于<strong>向量值函</strong>数来说，<strong>“gradient” 是无法定义的，这个时候只能使用Jacobian来解决</strong>。因此上式中$\frac{d\mathbf{x}}{dt}$ 其实说的是$\mathbf{J}_{\mathbf{x}}(t)$: 即 Jacobian of $\mathbf{x=x}(t)$, $\mathbf{x}$ 是向量，实际上，</p>
</li>
<li><p>$$<br>\mathbf{x=x}(t) \Leftrightarrow<br>\begin{cases}<br>x_1=x_1(t) \\<br>x_2=x_2(t)\\<br>\vdots \\<br>x_n = x_n(t)<br>\end{cases}<br>$$</p>
</li>
</ol>
<p>上面公式中$\frac{d\mathbf{x}}{dt}$ 是处于方便理解的目的而产生的表达式，是不严格的，其语义就是：向量 $\mathbf{x}$ 的每个 component 对 $t$ 求导，然后将结果组成一个列向量；这也就是 Jacobian本身的含义。</p>
<h1 id="More-complex-链式法则依然有效"><a href="#More-complex-链式法则依然有效" class="headerlink" title="More complex-链式法则依然有效"></a>More complex-链式法则依然有效</h1><p>之前我们介绍的Chain Rule 都是针对2层嵌套函数的，实际上链式法则对多层嵌套也是适用的- Chain Rule fit more than two links. e.g. there is 3 links, 3 means multiple.</p>
<p><u>Example 1</u>:</p>
<p>函数$f(x)=5x$, 而且 $x(u)=1-u$, $u(t) = t^2$.</p>
<p>这个复合函数的links 为：$f\rightarrow x \rightarrow u \rightarrow t$, </p>
<ul>
<li>有3层links（嵌套）</li>
<li>且$f,x,u$ 都为<strong>实值函数</strong>。</li>
</ul>
<p>此时我们依照之前2层嵌套的思路有如下结果：</p>
<p>$$<br>\frac{df}{dt}= \frac{d f}{dx}\cdot \frac{dx}{du} \cdot \frac{du}{dt} =5 \times (-1)\times 2t = -10t<br>$$</p>
<p><u>Example 2</u>:</p>
<p>存在函数 $f(\mathbf{x}(\mathbf{u}(t)))$, 其中：</p>
<ul>
<li>$f(\mathbf{x}) = f(x_1,x_2)$</li>
<li>$\mathbf{x}(\mathbf{u})=\begin{bmatrix}x_1(\mathbf{u}) \\ x_2(\mathbf{u}) \end{bmatrix}=\begin{bmatrix}x_1(u_1,u_2)\\ x_2(u_1,u_2) \end{bmatrix}$</li>
<li>$\mathbf{u}(t) = \begin{bmatrix}u_1(t) \\ u_2(t) \end{bmatrix}$</li>
</ul>
<p>这个函数的特点如下：</p>
<ul>
<li>有3层links</li>
<li>函数$f$ 是<strong>实值函数</strong>: $\mathbf{x},\mathbf{u}$ 是vector valued function-<strong>向量值函数</strong></li>
</ul>
<p>这些特点如下图所示，<br>$$<br>f \rightarrow \begin{cases} x_1 \rightarrow \begin{cases}u_1 \rightarrow t \\ u_2 \rightarrow t \end{cases} \\ x_2 \rightarrow \begin{cases}u_1\rightarrow t \\ u_2 \rightarrow t \end{cases}     \end{cases} \Leftrightarrow<br>f \rightarrow \mathbf{x} \rightarrow \mathbf{u} \rightarrow t<br>$$<br>解决思路一致：<br>$$<br>\frac{df}{dt} =\color{blue} {\frac{\partial f}{\partial \mathbf{x}}} \color{green}{\frac{\partial \mathbf{x}}{\partial \mathbf{u}}} \color{purple}{\frac {d \mathbf{u}}{dt}}=\color{blue}{\begin{bmatrix} \frac{\partial f}{\partial x_1} &amp; \frac{\partial f}{\partial x_2}\end{bmatrix}}<br>\color{green}{\begin{bmatrix} \frac{\partial x_1}{\partial u_1} &amp;  \frac{\partial x_1}{\partial u_2}\\ \frac{\partial x_2}{\partial u_1} &amp; \frac{\partial x_2}{\partial u_2} \end{bmatrix}}<br>\color{purple}{\begin{bmatrix}\frac{du_1}{dt} \\ \frac{du_2}{dt} \end{bmatrix}}<br>$$<br><strong>遗留问题：</strong></p>
<ol>
<li>上式中$\frac{\partial f}{\partial \mathbf{x}}$ 怎么写成行的形式，变成 Jacobian了？</li>
<li>如果用Jacobian 为什么要用 gradient 的记号？</li>
<li>概念和记号上，时而用gradient时而用 Jacobian,为什么？</li>
</ol>
<p><strong>原因：</strong></p>
<ol>
<li>用 gradient的记号是因为从视觉角度便于理解整个过程。</li>
<li>对于n元函数（无论 实值函数 or 向量值函数），<strong>Jacobian 都是兼容 gradient，而且更加通用</strong>。</li>
<li>Jacobian 相比于 gradient 表示更一般的情况- vector valued function。</li>
<li>原则上：<font color="blue">“如果 gradient 够用就用gradient；如果gradient不够用则用 Jacobian”</font></li>
</ol>
<p><strong>从上述分析过程我们可以总结此类问题下链式法则通用计算方法为 ：</strong><br>$$<br>\frac{df}{dt} = \color{blue}{\mathbf{J}_f(\mathbf{x})} \color{green}{\mathbf{J_x(u)}} \color{purple}{\mathbf{J_u}(t)}<br>$$<br>其中，乘法的意思就是 “矩阵乘法”。</p>
<p><strong>总结：</strong></p>
<ol>
<li>对于使用Chain Rule对嵌套函数求导的时候，<strong>只要嵌套的函数中有一个是 vector-valued 的函数，那么使用Jacobian 来描述几乎是唯一的选择</strong>，因为此时我们不用考虑乘法是“数乘”，“dot product” or “matrix multiplication”, 此刻都<strong>统一为矩阵乘法</strong>；否则会陷入各种“乘法不兼容的麻烦”中去。</li>
<li><strong>同时也由此可见，Jacobian 确实是比 gradient 更加通用的概念，用来表示计算过程更加方便。</strong></li>
</ol>
<h1 id="神经网络（Neural-Networks）"><a href="#神经网络（Neural-Networks）" class="headerlink" title="神经网络（Neural Networks）"></a>神经网络（Neural Networks）</h1><p>先介绍一个简单的神经网络</p>
<h2 id="Two-Layer-NN"><a href="#Two-Layer-NN" class="headerlink" title="Two Layer NN"></a>Two Layer NN</h2><p><img src="simplenn.png" alt=""></p>
<p>如上图所示，这是一个简单的2层神经网络，<strong>没有中间层- hidden layer</strong>.</p>
<ul>
<li>输入为：向量 $\mathbf{a}^{(0)}=\begin{bmatrix}a_0^{(0)}\\ a_1^{(0)} \\ a_2^{(0)} \end{bmatrix}$.</li>
<li>输出为：向量 $\mathbf{a}^{(1)} = \begin{bmatrix} a^{(1)} _0 \\ a^{(1)} _1 \end{bmatrix}$</li>
<li>Weights: 矩阵 $\mathbf{W^{(1)}}= \begin{bmatrix}\mathbf{w_0}^T\\ \mathbf{w_1}^T \end{bmatrix}\in \mathbb{R^{2\times 3}}$</li>
<li>Bias: 向量 $\mathbf{b^{(1)}}=\begin{bmatrix} b_0 \\ b_1 \end{bmatrix}$</li>
</ul>
<p>为了简化起见，我们采用Matrix-vector 的形式描述这个simple neural network:<br>$$<br>\mathbf{a^{(1)}} = \sigma (\mathbf{W^{(1)}} \mathbf{a^{(0)}}+ \mathbf{b^{(1)}})<br>$$</p>
<h2 id="Details-of-2-layer-NN"><a href="#Details-of-2-layer-NN" class="headerlink" title="Details of 2-layer NN"></a>Details of 2-layer NN</h2><p>上面用简洁的 matrix-vector 的形式介绍了一个简单的2层神经网络,现在补充一些细节。</p>
<p>一个2层的NN，其<strong>输入：n scalars 输出：m scalars</strong>，见下图，</p>
<p><img src="simplenn_details.png" alt=""></p>
<h2 id="3-Layers-NN-1-hidden-layer"><a href="#3-Layers-NN-1-hidden-layer" class="headerlink" title="3 Layers NN-1 hidden layer"></a>3 Layers NN-1 hidden layer</h2><p><img src="1hiddenlayernn.png" alt=""></p>
<h2 id="Fully-Connected-Feedforward-NN"><a href="#Fully-Connected-Feedforward-NN" class="headerlink" title="Fully Connected Feedforward NN"></a>Fully Connected Feedforward NN</h2><p><img src="mlps.png" alt=""></p>
<h1 id="Training-NN"><a href="#Training-NN" class="headerlink" title="Training NN"></a>Training NN</h1><p>用NN工作的基本流程如下：</p>
<ol>
<li><p><strong>设计网络结构</strong>，e.g. FCNNs, RNNs, CNNs etc.</p>
</li>
<li><p><strong>初始化参数</strong>（weights and biases）</p>
</li>
<li><p><strong>设计 Cost function and Optimization</strong>: update all the weights and biases to minimize the Cost function to best match our training data with the ground truth-labels.</p>
<ul>
<li>用 Backpropagation 计算 gradient.</li>
<li>Gradient descent 更新所有参数($\mathbf{W,b}$) to minimize the Cost function.</li>
<li>得到最后最优参数， i.e. 使得 Cost function 取最小值的参数 $(\mathbf{W^<em>,b^</em>})$</li>
</ul>
<blockquote>
<p>这个阶段主要做 Training NN，因此也可以叫做 Training（从人的角度） or Learning（从算法/机器的角度）。</p>
</blockquote>
</li>
<li><p><strong>Make Prediction</strong></p>
<blockquote>
<p>在新的测试数据上做预测</p>
<p>这个阶段又叫做 Test.</p>
</blockquote>
</li>
</ol>
<p>下图可以看出Training NN 的大致过程，</p>
<p><img src="minimization1.png" alt=""></p>
<p><img src="minimization2.png" alt=""></p>
<p><strong>整个Training Process中最为核心的部分就是</strong>：</p>
<p>通过Backpropagation（本质就是multivarite chain rule） 计算gradient；然后使用 Gradient descent 来寻找使得 Cost function 最小的参数。</p>
<blockquote>
<p>We need to build the Jacobian by gathering together the partial derivatives of the cost function with respect to all of the revelant variables - weights and biases.</p>
</blockquote>
<p>结合之前的知识我们简单地对上图做个分析，</p>
<ul>
<li>我们通过迭代更新parameters 来使得 Cost function 的值不断变小，最终达到最小值，此时的 weights 和 biases 就是我们最终想要的。</li>
<li>所谓更新parameters，就是指 利用gradient descent算法使Cost function 的gradient不断下降（计算gradient处的那个点 对应的Cost function的值），不断调整 parameters的大小</li>
<li>gradient 和 参数有什么关系？参数 weights and biases 才是 Cost function的变量，神经网络的输入以及label是常量。因此，gradient is matrices and vectors of partial derivatives of the Cost funtion with resprect to weights and biases. </li>
</ul>
<p><u>Example 3</u>：</p>
<p>我们现在以一个最简单的NN为例：<strong>一个 input neuron，一个 output neuron</strong>.</p>
<p>如图，</p>
<p><img src="basic_nntraining.png" alt=""></p>
<p>简要分析：</p>
<ul>
<li><p>Cost function 是 square loss，衡量模型输出 $a^{(1)}$ 与训练数据的真实标签 $y$之间的平方差。我们的目的是最小化这个函数使得：模型的输出与真实的结果越接近越好。</p>
</li>
<li><p>我们有两个 Chain expressions，均有三个links. 通过他们我们可以计算得到 gradient(Jacobian) of Cost function with repsect to weights and biases. 从而使得我们可以使用梯度下降算法来找到使 Cost function取最小值时的参数 $w^<em>,b^</em>$.</p>
</li>
<li>$C = (a^{(1)}-y)^2$ 是<strong>针对一个 specific training example而言的</strong>，如果有多个examples 应该将它们的损失函数加起来作为整个模型的损失韩式，即：</li>
</ul>
<p>$$<br>C = \frac{1}{N} \sum_k^N C_k<br>$$</p>
<p>其中 $C_k$ 是第$k$ 个training example 的 Cost function；$N$ 是 number of examples in training data; $C$ is the total Cost function of the training data.</p>
<p><u>Example</u> 4:</p>
<p>当我们增加更多的layers 和neurons的时候，事情会变得稍微复杂一些，见下图：</p>
<p><img src="mlps.png" alt=""></p>
<p>但是基本的过程依然如同我们training 上面那个最简单的NN一样。</p>
<p>可以参考一个关于 Training neural network的练习题<a href="https://www.coursera.org/learn/multivariate-calculus-machine-learning/quiz/NrGhK/training-neural-networks" target="_blank" rel="noopener">link</a></p>
<h1 id="关于梯度下降"><a href="#关于梯度下降" class="headerlink" title="关于梯度下降"></a>关于梯度下降</h1><p>所谓”Gradient descent” ，可以参考下图：</p>
<p><img src="gradient_meaning.png" alt=""></p>
<p>其包含以下这些方面：</p>
<ol>
<li>Cost function 上用以计算 gradient 的点的位置在下降，其实也就是说该点处的函数值在减小。</li>
<li>对于n元实值函数来说，其Gradient是一个vector。gradient vector 的长度或者叫范数，是衡量函数在某一点处变化的快慢；gradient 的方向指明了函数值增加/上升最快的方向，那么其反方向就指明了函数值减小/下降 最快的方向。</li>
<li>gradient 是函数对各个自变量偏导数的向量表示，所以它是自变量空间或者叫parameter space（对神经网络来说）中的一个向量，这个向量的方向即为函数值上升最快的方向。</li>
</ol>
<p><font color="red"><strong>综上所述，所谓gradient descent的本质含义就是：在与gradient相反的方向上改变自变量的取值，会使得函数值变小</strong>。</font> 下面稍做解释：</p>
<ul>
<li>要梯度相反的方向，对gradient 直接取负就好，得到 $-\nabla_{\mathbf{x}}f(\mathbf{x})$.</li>
</ul>
<p>$$<br>-\nabla_{\mathbf{x}}f(\mathbf{x}) = \begin{bmatrix}-\frac{\partial f}{\partial x_1} \\ -\frac{\partial f}{\partial x_2}\\<br>\vdots \\<br>-\frac{\partial f}{\partial x_n}<br>\end{bmatrix}<br>$$</p>
<p>注意：我们根据偏导数的概念知道$\frac{\partial f}{\partial x_i}$ 是指在$x_i$ 这个自变量的方向上，函数值变化的快慢（沿着$x_i$ 轴方向）。那么$-\frac{\partial f}{\partial x_i}$ 就是沿着$-x_i$ 轴方向函数值变化的快慢。</p>
<ul>
<li>根据梯度的定义: 我们知道沿着负梯度函数值减小。函数值的变换本质上是由于自变量取值的变化，我们也知道自变量变化可以表示为：$x\rightarrow x+\Delta x$.  对n元函数来说，是按照如下方式： </li>
</ul>
<p>$$<br>\begin{align}<br>x_1 := x_1 + \Delta x_1\\<br>x_2 := x_2 + \Delta x_2\\<br>\vdots \\<br>x_n := x_n + \Delta x_n<br>\end{align}<br>$$</p>
<p>我们只要使得每个 $\Delta x_i$ 的方向是沿着$-x_i$ 轴的方向（即与 $-\frac{\partial f}{\partial x_i}$ 同向），那么我们就可以保证所有自变量都沿着负梯度方向变化，此时：函数值降低。根据上面内容，我们只要保证$\Delta x_i$ 是 -$\frac{\partial f}{\partial x_i}$ 或者是它的正数倍就可以达到我们的目的。换句话说即，<br>$$<br>x_1 := x_1 +\alpha(-\frac{\partial f}{\partial x_1})\\<br>x_2 := x_2 + \alpha(-\frac{\partial f}{\partial x_2})\\<br>\vdots\\<br>x_n := x_n + \alpha (-\frac{\partial f}{\partial x_n})\\<br>\text{Where } \alpha &gt; 0<br>$$<br>将上述公式写成向量形式，<br>$$<br>\begin{align}<br>&amp;\color{blue}{\mathbf{x} := \mathbf{x} - \alpha \nabla_\mathbf{x}f(\mathbf{x})}\\<br>&amp;\text{or another equal expression} \\<br>&amp;\color{blue}{\mathbf{x} := \mathbf{x} - \alpha \frac{\partial f}{\partial \mathbf{x}}} \\<br>\end{align}<br>$$<br>上式就是我们常见的 gradient descent算法中参数的更新公式。对于神经网络来说，即：<br>$$<br>\begin{align}<br>&amp;\text{For weights}\\<br>&amp;\color{blue}{\mathbf{W} := \mathbf{W} - \alpha \nabla_\mathbf{W}C(\mathbf{W})} \Leftrightarrow<br>\color{blue}{\mathbf{W} := \mathbf{W} - \alpha \frac{\partial C}{\partial \mathbf{W}}}\\<br>&amp;\text{and for biases} \\<br>&amp;\color{blue}{\mathbf{b} := \mathbf{b} - \alpha \nabla_\mathbf{b}C(\mathbf{b})} \Leftrightarrow\color{blue}{\mathbf{b} := \mathbf{b} - \alpha \frac{\partial C}{\partial \mathbf{b}}} \\<br>\end{align}<br>$$<br><strong>计算梯度$\nabla C$</strong> 的算法就是基于multivariate链式法则的 <strong>Backpropagation</strong>，它是NN context 下Gradient descent 的<strong>核心</strong>。</p>
<p>这里有一个很好的关于梯度下降的讲解视频, 值得认真学习、反复钻研 <a href="https://www.youtube.com/watch?v=IHZwWFHWa-w" target="_blank" rel="noopener">link</a></p>
<h1 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h1><p>我们现在以包含一个hidden layer 的3层神经网络为例分析backpropagation算法是如何计算梯度的。<font color="red"><u>Example 4</u>:</font><br>神经网络如下图</p>
<p><img src="3layernn.png" alt=""></p>
<p><strong>输入</strong>：向量 $\mathbf{a}^{(0)}\in \mathbb{R}^4$</p>
<p><strong>隐层：</strong>向量 $\mathbf{a}^{(1)}\in \mathbb{R}^3$</p>
<p><strong>输出</strong>：向量 $\mathbf{a}^{(2)}\in \mathbb{R}^2$</p>
<p>Layer 1的参数：$\mathbf{W}^{(1)}\in \mathbb{R}^{3\times 4},\mathbf{b}^{(1)}\in \mathbb{R}^3$</p>
<p>Layer 2的参数：$\mathbf{W}^{(2)}\in \mathbb{R}^{2\times 3} ,\mathbf{b}^{(2)}\in \mathbb{R}^2$</p>
<p><strong>For a specific training example, Vector form of Cost function</strong>: $C_k = \Vert \mathbf {a}^{(2)} - \mathbf{y} \Vert^2$.</p>
<p>如果我们想计算Cost function对于最后一层(layer 2)的 weights 和 bias的<u>梯度</u>（或者不严谨地叫做<u>偏导数</u>），与之前流程一样：<br>$$<br>\begin{align}<br> \frac{\partial C_k}{\partial \mathbf{W}^{(2)}} &amp;= \frac{\partial C_k}{\partial \mathbf{a}^{(2)}}  \frac{\partial \mathbf{a}^{(2)}}{\partial \mathbf{z}^{(2)}}  \frac{\partial \mathbf{z}^{(2)}}{\partial \mathbf{W}^{(2)}} \\<br>\frac{\partial C_k}{\partial \mathbf{b}^{(2)}} &amp;= \frac{\partial C_k}{\partial \mathbf{a}^{(2)}}  \frac{\partial \mathbf{a}^{(2)}}{\partial \mathbf{z}^{(2)}}  \frac{\partial \mathbf{z}^{(2)}}{\partial \mathbf{b}^{(2)}}<br>\end{align}<br>$$<br>如果我们要计算 Cost function 对于<strong>前一层</strong>(layer 1) 参数的梯度，我们可以使用下面表达式，<br>$$<br>\begin{align}<br> \frac{\partial C_k}{\partial \mathbf{W}^{(1)}} &amp;= \frac{\partial C_k}{\partial \mathbf{a}^{(2)}}  \frac{\partial \mathbf{a}^{(2)}}{\partial \mathbf{a}^{(1)}} \frac{\partial \mathbf{a}^{(1)}}{\partial \mathbf{z}^{(1)}} \frac{\partial \mathbf{z}^{(1)}}{\partial \mathbf{W}^{(1)}} \\<br>\frac{\partial C_k}{\partial \mathbf{b}^{(2)}} &amp;= \frac{\partial C_k}{\partial \mathbf{a}^{(2)}}  \frac{\partial \mathbf{a}^{(2)}}{\partial \mathbf{a}^{(1)}}  \frac{\partial \mathbf{a}^{(1)}}{\partial \mathbf{z}^{(1)}} \frac{\partial \mathbf{z}^{(1)}}{\partial \mathbf{b}^{(1)}}<br>\end{align}<br>$$<br>其中 $\frac{\partial \mathbf{a}^{(2)}}{\partial \mathbf{a}^{(1)}}$ 可以被展开为:<br>$$<br>\frac{\partial \mathbf{a}^{(2)}}{\partial \mathbf{a}^{(1)}}=\frac{\partial \mathbf{a}^{(2)}}{\partial \mathbf{z}^{(2)}} \frac{\partial \mathbf{z}^{(2)}}{\partial \mathbf{a}^{(1)}}<br>$$</p>
<p><strong>我们可以将上述过程推广到求损失函数对任何层梯度的情况</strong>，<br>$$<br>\frac{\partial C_k}{\partial \mathbf{W}^{(i)}} = \frac{\partial C_k}{\partial \mathbf{a}^{(L)}} \color{blue} {\underbrace{\frac{\partial \mathbf{a}^{(L)}}{\partial \mathbf{a}^{(L-1)}} \frac{\partial \mathbf{a}^{(L-1)}}{\partial \mathbf{a}^{(L-2)}} \cdots \frac{\partial \mathbf{a}^{(i+1)}}{\partial \mathbf{a}^{(i)}}}_{\text{from layer L to layer i}} }\frac{\partial \mathbf{a}^{(i)}}{\partial \mathbf{z}^{(i)}} \frac{\partial \mathbf{z}^{(i)}}{\partial \mathbf{W}^{(i)}} \\<br>\frac{\partial C_k}{\partial \mathbf{b}^{(i)}} = \frac{\partial C_k}{\partial \mathbf{a}^{(L)}} \color{blue}{\underbrace{\frac{\partial \mathbf{a}^{(L)}}{\partial \mathbf{a}^{(L-1)}} \frac{\partial \mathbf{a}^{(L-1)}}{\partial \mathbf{a}^{(L-2)}} \cdots \frac{\partial \mathbf{a}^{(i+1)}}{\partial \mathbf{a}^{(i)}}}_{\text{from layer L to layer i}}} \frac{\partial \mathbf{a}^{(i)}}{\partial \mathbf{z}^{(i)}} \frac{\partial \mathbf{z}^{(i)}}{\partial \mathbf{b}^{(i)}}<br>$$</p>
<p>其中$\frac{\partial \mathbf{a}^{(j)}}{\partial \mathbf{a}^{(j-1)}}$ 可以被展开为：<br>$$<br>\frac{\partial \mathbf{a}^{(j)}}{\partial \mathbf{a}^{(j-1)}} = \frac{\partial \mathbf{a}^{(j)}}{\partial \mathbf{z}^{(j)}} \frac{\partial \mathbf{z}^{(j)}}{\partial \mathbf{a}^{(j-1)}}<br>$$</p>
<p>以上，就是Backpropagation的全过程。需要注意的几点是：</p>
<ul>
<li>上面公式中我们都用了偏导数的符号$\frac{\partial}{\partial}$, 但根据本节课之前的内容我们应该明白这些偏导数符号表达的是 <strong>Jacobian</strong>。</li>
<li>乘法应该是<strong>matrix multiplication</strong></li>
</ul>
<p>因为只有基于上述两点，以上整个过程涉及的公式才能将：</p>
<ol>
<li>$f:\mathbb{R}^n \rightarrow \mathbb{R}$, i.e. $\nabla_\mathbf{x}f(\mathbf{x})=(\mathbf{J}_f(\mathbf{x}))^T \in \mathbb{R}^{n\times 1} $</li>
<li>$f:\mathbb{R}^{m\times n} \rightarrow \mathbb{R}$, i.e. $\nabla_{\mathbf{x}}f(\mathbf{x}) \in \mathbb{R}^{m\times n}$</li>
<li>$f:\mathbb{R}^n \rightarrow \mathbb{R}^m$,i.e. $\mathbf{J_f(x)} \in \mathbb{R}^{m\times n}$</li>
<li>$f:\mathbb{R} \rightarrow \mathbb{R}^m$,i.e. $\mathbf{J_f}(x) \in \mathbb{R}^{m\times 1}$</li>
</ol>
<p><strong>这四种函数值与自变量的组合情况统一在一组公式之下，因为：Jacobian 完全兼容 gradient，更加具有通用性；缺点就是Jacobian不如gradient的表示式那么直观。</strong></p>
<h1 id="动手练习"><a href="#动手练习" class="headerlink" title="动手练习"></a>动手练习</h1><p>这是一个练习 Backpropagation的notebook-<a href="https://hub.coursera-notebooks.org/user/ajbcqdfqzhzvvstjfsyaas/notebooks/Backpropagation.ipynb#Backpropagation" target="_blank" rel="noopener">link</a>.</p>
<p>Learning by doing!</p>
<p>Learning by teaching!</p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>坚持原创分享，您的支持是我继续创作的动力!</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="popoblue WeChat Pay"/>
        <p>WeChat Pay</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="popoblue Alipay"/>
        <p>Alipay</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:</strong>
    popoblue
  </li>
  <li class="post-copyright-link">
    <strong>Post link:</strong>
    <a href="https://asiagood.github.io/archive/2018-07-09/Multivariate-chain-rule-and-its-applications/" title="三.多元链式法则及其应用">https://asiagood.github.io/archive/2018-07-09/Multivariate-chain-rule-and-its-applications/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice: </strong>
    All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Gradient-Descent/" rel="tag"># Gradient Descent</a>
          
            <a href="/tags/Chain-Rule/" rel="tag"># Chain Rule</a>
          
            <a href="/tags/Neural-Network/" rel="tag"># Neural Network</a>
          
            <a href="/tags/Joacbian/" rel="tag"># Joacbian</a>
          
            <a href="/tags/Backpropagation/" rel="tag"># Backpropagation</a>
          
        </div>
      

      
      
        <div class="post-widgets">
        

        

        
          
          <div id="needsharebutton-postbottom">
            <span class="btn">
              <i class="fa fa-share-alt" aria-hidden="true"></i>
            </span>
          </div>
        
        </div>
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/archive/2018-07-07/Multivariate-calculus/" rel="next" title="二.多元微积分">
                <i class="fa fa-chevron-left"></i> 二.多元微积分
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/archive/2018-07-12/Taylor-series-and-linearisation/" rel="prev" title="四.泰勒级数和线性化">
                四.泰勒级数和线性化 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">popoblue</p>
              <p class="site-description motion-element" itemprop="description">知行合一，一个功夫！</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">45</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">127</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/yazhouhao" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:me@yazhouhao.com" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://twitter.com/yazhouhao" target="_blank" title="Twitter">
                    
                      <i class="fa fa-fw fa-twitter"></i>Twitter</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.facebook.com/yazhouhao" target="_blank" title="FB Page">
                    
                      <i class="fa fa-fw fa-facebook"></i>FB Page</a>
                </span>
              
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#多元函数链式法则（Chain-Rule）"><span class="nav-number">1.</span> <span class="nav-text">多元函数链式法则（Chain Rule）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#More-complex-链式法则依然有效"><span class="nav-number">2.</span> <span class="nav-text">More complex-链式法则依然有效</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#神经网络（Neural-Networks）"><span class="nav-number">3.</span> <span class="nav-text">神经网络（Neural Networks）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Two-Layer-NN"><span class="nav-number">3.1.</span> <span class="nav-text">Two Layer NN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Details-of-2-layer-NN"><span class="nav-number">3.2.</span> <span class="nav-text">Details of 2-layer NN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Layers-NN-1-hidden-layer"><span class="nav-number">3.3.</span> <span class="nav-text">3 Layers NN-1 hidden layer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fully-Connected-Feedforward-NN"><span class="nav-number">3.4.</span> <span class="nav-text">Fully Connected Feedforward NN</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Training-NN"><span class="nav-number">4.</span> <span class="nav-text">Training NN</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#关于梯度下降"><span class="nav-number">5.</span> <span class="nav-text">关于梯度下降</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Backpropagation"><span class="nav-number">6.</span> <span class="nav-text">Backpropagation</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#动手练习"><span class="nav-number">7.</span> <span class="nav-text">动手练习</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2017 &mdash; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">popoblue</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count"></span>
  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.3</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'fgig2ECgFUUIzHQxQ6UEGBsw-gzGzoHsz',
        appKey: 'qwf4KMNBL2n522kimTswJY9w',
        placeholder: 'Just go go',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  




  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.3"></script>



  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("fgig2ECgFUUIzHQxQ6UEGBsw-gzGzoHsz", "qwf4KMNBL2n522kimTswJY9w");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">

  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>

  <script>
    
      pbOptions = {};
      
          pbOptions.iconStyle = "box";
      
          pbOptions.boxForm = "horizontal";
      
          pbOptions.position = "bottomCenter";
      
          pbOptions.networks = "Linkedin,Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-postbottom', pbOptions);
    
    
  </script>

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
