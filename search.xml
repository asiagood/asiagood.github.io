<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[自学]]></title>
    <url>%2Farchive%2F2019-03-26%2F%E8%87%AA%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[想要什么学什么，为什么如何自学？参考一片在深度学习领域自学的文章 [^ apple ] 自学经历 Asdasd [^ apple ]: Here is the textnote.]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>自学</tag>
        <tag>自我提升</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[古典的智慧]]></title>
    <url>%2Farchive%2F2019-03-26%2F%E5%8F%A4%E5%85%B8%E7%9A%84%E6%99%BA%E6%85%A7%2F</url>
    <content type="text"><![CDATA[韩国相关 房玄龄说韩国 “彼高丽者，边夷贱类，不足待以仁义，不可责以常理。” 《世祖实录》 且贱隶久为人役，常有未逞之心，若一朝得志，则多反噬其主。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>历史</tag>
        <tag>古典</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019时间线]]></title>
    <url>%2Farchive%2F2019-03-14%2FTimeline2019%2F</url>
    <content type="text"><![CDATA[#三月 21 读论文做笔记 Cascading Multiway Attenntion for Document-level Sentiment Classification Joint Learning for Targeted Sentiment Analysis 笔记待做，因为还没完全理解。主要是对于sequence labeling的问题没有理解。 Knowledge-enriched Two-Layered Attention Network for Sentiment Analysis UC Berkeley 2019 Introduction to Deep Learning 课程 ad ad ad 概率教程阅读与总结 ad a 22 读论文做笔记- 今天的主题是：如何将知识引入NNmodel，提升情感分类模型。 A Lexicon-Based Supervised Attention Model for Neural Sentiment Analysis ad Ad]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>日志</tag>
        <tag>Timeline</tag>
        <tag>记录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[值得记在心底的事]]></title>
    <url>%2Farchive%2F2019-03-13%2FThing-worth-remembering%2F</url>
    <content type="text"><![CDATA[个人价值 任何你想达到的成就，都必须通过自己去争取。 保持一个好身体 工作是最大的修行 工作中要全神贯注，当然你可以换换脑经以便更加有利于工作，但必须服务于工作。 建立自己的心流模式 工作中获得成功的要诀有三条： 努力工作 坚持不懈 高度自律 任何值得学习的事情，你都可以通过学习学会它，无一例外。 家庭生活 父母是最亲密的人生守护者，超越子女；伴侣是最亲密的战友和朋友，超越其他任何人；子女是你未来的希望。 照顾年迈的父母、给伴侣最深的安全感、培养教育子女，是任何一个成年人最大的责任，没有之一。 财务管理 善于管理自己的财产，不要乱花！ 投资 制定计划 分配比例 长久操作而不是短期投机 想办法赚更多的钱。只要是合法的赚钱，就是值得尊敬的。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>重要的事</tag>
        <tag>信念</tag>
        <tag>道理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试攻略]]></title>
    <url>%2Farchive%2F2019-03-11%2F%E9%9D%A2%E8%AF%95%E6%94%BB%E7%95%A5%2F</url>
    <content type="text"><![CDATA[在校时的准备 必须刷题~多刷，反复刷，保持手感和记忆。 面试前的准备 尽早开始，千万不要等到自己感觉完美的时候再开始！ 简历一定要多投：海投！ 面试技巧 电面 和HR能否流利交流 要和对方沟通（眼神，手势等肢体语言），而不要自顾自地输出！ 投简历的网站 Linkedin 脉脉 Indeed]]></content>
      <categories>
        <category>工作</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>找工作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[几个相似的分布]]></title>
    <url>%2Farchive%2F2019-02-27%2FSome-Similar-Destribution%2F</url>
    <content type="text"><![CDATA[概率统计中有几个常见的分布，他们在机器学习中应用非常广泛。他们之间有着非常密切的联系，但很多时候我们往往分不清楚，这篇文章着重讲清以下几个内容： 各个分布的定义 对应的随机实验 共同点 区别 伯努利分布-Bernoulli Distribution二项分布-Binomial Distribution多元伯努利分布-Multinoulli Distribution多项式分布-Multinomial Distribution总结]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Distribution</tag>
        <tag>分布</tag>
        <tag>Binomial</tag>
        <tag>Multinomial</tag>
        <tag>Bernoulli</tag>
        <tag>Multinoulli</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学术论文写作方法与技巧]]></title>
    <url>%2Farchive%2F2019-02-22%2FMethods-and-skills-on-writing-research-papers%2F</url>
    <content type="text"><![CDATA[这篇文章是阅读清华大学刘洋老师在2014年第十届全国机器翻译研讨会上所做的有关学术写作与论文发表的报告(download)的笔记，并且将领域从机器翻译换到了情感分类，以下为正文。 此外，这篇文章处在持续完善过程中 ing… 论文发表流程 确定方向： 情感分类 确定问题：利用情感词典加强文本的情感表示 确定思路：将辞典作为“监督信息” 来监督注意力生成过程 确定方法：用辞典形成的门监督attention权重。 实验验证：数据集、基线系统、评测指标 撰写论文：投稿一个近期会议（2个月、至多3个月之内的会议，切忌时间太久！） 选择方向 热门方向 优点：可利用的资源（论文，代码，教程，etc.）很多，相对容易上手。 缺点：千军万马过独木桥，你的点子可能早已经被别人想到。 冷门方向 优点：还是价值洼地，可摘取的低枝果实很多。 缺点：资源较少，入门相对费力。 选择的智慧就在于切中以下几个要点以及作出平衡（需要有经验的导师指导或者自己天赋异禀）： 重要问题、重大挑战 自己非常有兴趣 即将成为热门（这个需要有判断力和深厚的学术功底才可以做到，比如你的导师） 高风险性（自己可否承担？） 解决问题 独立思维 先思考，再去查找文献互相印证 语言学意义 具有语言学理论、心理学理论的支撑，符合语言学、心理学角度的直觉。 数学意义 使用数学工具做形式化，不臆造公式。 简洁优美 简单、干净、优美 确定思路与方法像外行一样思考，像内行一样实践。 思考 实践 境界 外行 专家 独树一帜、炉火纯青 专家 专家 经验丰富、男脱窠臼 外行 外行 天马行空、眼高手低 专家 外行 思维僵化、束手无策 论文撰写概述写论文时什么最重要? 审稿人时如何审稿的？你认为应该是这样的： 审稿人是领域专家，无所不知。打印出来，认真仔细读你的文章，反复琢磨句子含义，推敲公式。在花了审稿人大量时间之后，终于明白了你工作的意义并认可它，最后决定给你一个border line或以上的分数。 而真实情况下，审稿人往往是这样做的： 他很可能不是领域专家，一直很忙拖着没有审稿，在deadline到来之前一天需要完成n篇文章的审稿任务。 他往往先看题目、摘要，扫一下 Introduction以便了解你做什么，然后直接翻到最后找核心实验结果（性能好不好？），然后基本确定录还会不录（也许只需要5～10分钟！）。如果决定录，剩下的就是写一些赞美的话，指出一些次要的小毛病。如果决定❌，下面的过程就是仔细看中间的部分找理由拒了。 总结起来就是：第一印象定录拒，5分钟内说服审稿人！ 来自微博上的佐证， 观念必须转变以作者为核心整理工作 $\Longrightarrow$ 以读者为核心阐述工作 信息的呈现符合读者的认知习惯 深入浅出 引人入胜 让读者可以最快速地找到想要的信息 尽量降低读者的理解难度 合理恰当地综合使用信息要素：图-&gt;曲线-&gt;表-&gt;正文-&gt;公式 语言简洁明快，切忌超长复杂句式频繁出现！ 尽量提高读者阅读时的舒适度 思想新颖、符合直觉 组织合理、逻辑严密、论证充分 文笔优美、排版美观 阅读与写作的关系阅读与写作的区别：层次分明，角度不同决定优先级不同，如下图所示。 作者在写文章时，要将思想-信息-论文完美地统一起来，而不要让读者感到各个部分时割裂的。在写作时大致应当遵循下述规律： 对自己的思想做个摘要简化，得出最核心的部分 通过实验得出那些可以反应自己思想的信息 用论文的形式承载这些信息，合理组织以清楚、充分地表达作者思想 降低信息理解难度是关键 论文撰写技巧这部分，按照一篇完整学术论文的组成部分，分别介绍各个部分的写作技巧。 标题写作技巧-Title标题的重要性 如何查看浩若烟海的文献？ 根据标题过滤 50% 根据摘要再过滤 20% 根据介绍再过滤 20% 剩下的10%，才是需要仔细看的论文 例子： 用一句话概括你所做的工作 考虑搜索引擎的影响，包含关键字 可以适当地别出心裁 也要冒一定风险 摘要的写作技巧-Abstract 核心：几句话概括你的工作！ 误区 力图把所有细节说清楚 用很专业的术语来描述 出现数学符号！ 用语要简单，让外行能看懂！！！ 经典示例 介绍的写作技巧-Introduction 比题目和摘要要更进一步，用几段话说清楚你的工作 要点是充分论证你所做工作的必要性和重要性，要让审稿人认同并且迫不及待想往下看。 兴文逻辑严密，论证充分。 经典逻辑最 常见的逻辑: 说明问题是什么❓ 简单罗列前人工作 描述我们的工作 此外更好的逻辑: 说明问题是什么❓ 目前最好的工作面临什么挑战❓ 我们的方法可以缓解上述挑战 例子： 问题 挑战 我们的工作 段落的写法 每一个段落都有一个论断性的中心句 其余部分都是支撑句，围绕中心句展开论证 前人工作 具体数据 支撑句之间可分类组织 段尾可加上衔接句 例子： 中心句与支撑句 衔接句 支撑句论证要严密 新技巧 首页放置一个图或者表，让读者一目了然你所做的工作 不要去写 “This paper is organnized as follows. Section 2…”, 而是直接列出自己的贡献。 信息元素的易理解程度 读者潜意识里会优先选择更加易理解的信息元素 首页加图表 信息流的变化 图和表的重要性 图和表示论文的骨架，争取要让读者按照顺序看就能理解论文的主要思想，不用通过正文才能看懂。 一般第一遍看，都会看图、找例子 然后翻到后面找主要结果 再从头看正文 把论文的元素放在最应该被放在的地方，符合读者的认知习惯，降低理解难度 直接列出自己的贡献 全局连贯性 方法的写作技巧（Method）如何描述自己的方法至关重要，方法好而阐述的不好并不能让审稿人简单明了地理解你的核心思想，照样免不了被拒的命运! 不要一上来就描述你的工作，可以先介绍背景知识（往往就是baseline） 有利于降低初学者或其他领域学者的理解难度 有利于对introduction 中的论文做更加详细的解释 有利于对比baseline和自己的方法 Running Example 是超级利器 英语不好，感觉自己说不清楚？用例子！ 全篇统一使用换一个running example，用来阐述你的方法（甚至是baseline） 围绕着running example, 展开描述你的工作 审稿人能从 running example中更舒服地了解你的工作，相反地，读正文会花掉他/她更多的时间 看完running example，审稿人便能知道核心思想。 方法描述中的逻辑顺序 杜绝如下 错误的 顺序 上来就是形式化描述 紧接着，开始解释数学符号 采用如下正确的顺序 首先给出running example 然后利用 running example，用通俗语言描述你的想法 最后才是形式化描述 每个公式都有语言学意义，都来自你的直觉和想法，直接告诉审稿人，不要让他/她自己去揣摩!!! 例子 实验的写作技巧（Experiment）实验设计这是最重要的，因为这是做实验、写文章的根基。 公认的标准数据和state-of-the-art系统 实验先辅后主 辅助实验（开发集）：参数的影响 主实验（测试集）：证明显著超过baseline 必须有显著性检验（词条之前，经常缺失！） 不辞辛劳，做到极致 迭代方向 minimum $\Longrightarrow$ solid $\Longrightarrow$ maximum 例子： 先辅后主 用图的误区❌ 用表的误区❌ Caption包含充分的信息 最好能直接看懂图，而不用去看正文！ 相关工作的写作技巧（Related work）首先要做到的是，明明白白，分清对错、禁忌。 错误❌ 正确✅ 没有引用重要论文（可以直接作为rejection的理由） 向审稿人显示你对本领域具有全面深刻的把握 简单的罗列和堆砌，缺乏深刻到位的评论 通过与前人工作的对比凸显你的工作的创新性 通过批评乃至攻击前人工作，证明自己的创新 为读者梳理领域的发展脉络，获得全局性的认知 例子： 讲明研究方法最初灵感来源 方法的传承与创新（！） 附录的写作技巧 并非必须，但是对于读者深入理解你的工作有帮助，并且往往非常形式化 证明 ”鸡肋“ 恰当地使用附录能显著提升论文的可读性，降低审稿人信息获取的难度。 例子 写作常见文法和格式问题常见问题大致分为以下几个类型： 句子过长 经常使用被动句 结构松散、口语化 不定冠词和定冠词的使用 公示后面文字的缩进 引用的写法 引以为戒的重要例子： 句子过长 被动句式+弱动词 结构松散+口语化+缺乏力度 a 还是 an？用不用 the？ 公式的缩进 其他问题 论文中每个数学符号都应当找得到定义，除非众所周知，比如：$\pi$ . 永远不要不加说明的使用数学符号。 要避免数学符号冲突，使用符号列表 不要生造术语，尤其是中式译法，尽量符合惯例 集成所有信息元素，排版美观和专业 提高英语写作的窍门 找著名学者（尤其是 native speaker）的的论文钻研，学习句式和词汇用法，做笔记！ 写作时手边放一部纸质的词典，经常翻看 拿不准的地方找Google：双引号查询 例子 句式和造句 利用搜索引擎 必须掌握的工具 Latex 强烈建议用 LaTex代替Word Bibtex 自动生成参考文献列表 MetaPost 编程画矢量图 例子：MetaPost 英文写作进阶一本严肃、高质量的关于科技写作的书是你所必须的。 时间管理和获得反馈 由粗到细 截稿前一个月开始写 每隔两天改一次 听取不同背景读者的反馈意见 专家：专业意见 非专家：发现信息壁垒 写到极致，完成完美精致的艺术品 总结最后这部分对之前所涉及的内容做一个概括： 写论文的本质是分享思想，传递信息 信息的传递必须符合读者-审稿人的认知习惯 全心全意为读者服务，降低阅读难度，提高阅读快感 细节决定成败 不要本末倒置：创新至上，技法为辅！ 建议，每次写一篇新的文章，从最早的确定问题和思路开始，就应当严格依照此文进行规划和写作，力求做到精益求精，把自己的思想传递给读者-审稿人！]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>Writing</tag>
        <tag>写作</tag>
        <tag>论文发表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[四. PCA]]></title>
    <url>%2Farchive%2F2018-07-28%2FPrincipal-Component-Analysis%2F</url>
    <content type="text"><![CDATA[简介 We can think of dimensionality reduction as a way of compressing data with some loss, similar to jpg or mp3. Principal Component Analysis (PCA) is one of the most fundamental dimensionality reduction techniques that are used in machine learning. In this module, we use the results from the first three modules of this course and derive PCA from a geometric point of view. Within this course, this module is the most challenging one, and we will go through an explicit derivation of PCA plus some coding exercises that will make us a proficient user of PCA. PCA 和 Orthogonal Projection, Orthogonal complements，Orthogonal decompostiion, SVD, Eigen vector and Eigen value, Covariance Matrix, Variance, Optimization(maximization or minimization), lower-rank approximation,encoder-decoder 等诸多概念都有着很密切的关系！ 可以从不同的角度、进行多方位的阐释。 学习目标 总结 PCA 线性代数角度 统计角度 优化角度 用代码实现PCA 探讨当PCA用在 Hight-dimensional 数据时的性质 两个很有用的资料 关于vector space的定义：link. 关于Orthogonal complement: link ; 关于Orthogonal decomposition（正交分解）link. Orthogonal complement and Orthogonal decomposition（正交补和正交分解）关键概念为： If we look at $n$-dimensional vector space $V$ and a $k$-dimensional subspace $W\subset V$, then the orthogonal complement $W^\perp$ is an $(n-k)$-dimensional subspace of $V$ and contains all vectors that are orthogonal to every vector in $W$. Every vector $\mathbf x \in V$ can be (uniquely) decomposed into $\mathbf x = \sum_{i=1}^k \lambda_i \mathbf w_i + \sum_{j=1}^{n-k}\psi_j \mathbf w^\perp_j, \lambda_i, \psi_j \in \mathbb R,$where $\mathbf w_1, \mathbf w_2,…, \mathbf w_k $ is a basis of $W$ and $\mathbf w_1,\mathbf w_2,…,\mathbf w_{n-k}$ is a basis of $W^\perp$. PCA 推导A key idea behind the PCA, is to use orthogonal projections to find a lower representation of the data and retain as much information as possible. 即：利用正交投影找到高维数据的低维表示，同时保证尽量不损失太多有用的信息。 我们的数据集为： $\mathbf X \in \mathbb R^{m\times n}= \lbrace \mathbf x_1,…,\mathbf x_m\rbrace$[^1] where $\mathbf x_i \in \mathbb R^n \text{ for } i=1,…,$m. i.e. Design matrix $\mathbf X$ contains $m$ examples and each with $n$ features. PCA 的问题设置和目标PCA的目的：是“高效地” 将 design matrix 中的向量 $\mathbf x \in \mathbb R^n$ 用低维向量 $\widetilde {\mathbf x} \in \mathbb R^c$ 表示, 其中 $c&lt;n$； 从而最终将 design matrix $\color{blue}{\mathbf X\in \mathbb R^{m\times n} \longrightarrow \mathbf X^\prime\in\mathbb R^{m\times c}}$. 回忆上节课的知识： 根据“正交补和正交分解” 小节的知识，For every element $\mathbf x \in \mathbb R^n$, 一定可以唯一的分解为如下形式：$$\begin{align}&amp;\color{blue}{\mathbf x = \widetilde{\mathbf x} + \widetilde{\mathbf x}^\perp}, \text{where: }\\&amp;\widetilde{\mathbf x}\in W:\text{orthogonal projection of }\mathbf x \text{ onto } W. \\&amp;W: \text{ is a c-dimensional subspace of } \mathbb R^n.\\&amp;\widetilde {\mathbf x}^\perp: \text{orthogonal projection of } \mathbf x \text{ onto } W^\perp.\\&amp;W^\perp:\text{ is a (n-c)-dimensional subspace of } \mathbb R^n.\\&amp;c: \text{ is a hyperparameter which denote dimensions you want to transform/compress.}\end{align}$$ 基于以上，我们假设和约定 The basis vectors of $W$ is $\lbrace \mathbf b_1,…,\mathbf b_c \rbrace$ and the basis vectors of $W^\perp$ is $\lbrace \mathbf b^\perp_1,…, \mathbf b^\perp_{n-c} \rbrace$, where every $\mathbf b_i (i=1,…,c)$ and $\mathbf b^\perp_j ( j=1,…,n-c) \in \mathbb R^n$. We denote the $\mathbf B = [ \mathbf b_1,…,\mathbf b_c ]\in\mathbb R^{n\times c}, \mathbf B^\perp = [\mathbf b^\perp_1,…,\mathbf b^\perp_{n-c}] \in \mathbb R^{n\times(n-c)}$. 由此我们可以得到如下式子：$$\begin{align}\mathbf x &amp;= \color{blue}{\widetilde{\mathbf x} }+ \color{purple}{\widetilde{\mathbf x}^\perp} \in \mathbb R^n\\&amp; = \color{blue}{\sum_{i=1}^c \lambda_i \mathbf b_i}+ \color{purple}{\sum_{j=1}^{n-c}\psi_j \mathbf b^\perp_j}\\&amp;=\color{blue}{\mathbf B\lambda}+ \color{purple}{\underbrace{\mathbf B^\perp\mathbf \psi}_{residual/diff} }\end{align}$$ $\mathbf b_1,…, \mathbf b_c$ Span the principal subspace $W$. 尽管 $\widetilde{\mathbf x} \in \mathbb R^n$, 但它lives in c-dimensional subspace: $\mathbb R^c$, 因此只需要 c 个 coordinates：$\lambda_1,…,\lambda_c$ 就可以表示它。 根据上一课-“正交投影”的内容，我们知道：$$\widetilde{\mathbf x} = \underbrace{\mathbf B(\mathbf B^T\mathbf B)^{-1}\mathbf B^T}_{P} \mathbf x =\mathbf P\mathbf x \\$$ 同时：$$\widetilde{\mathbf x}= \mathbf B \underbrace{(\mathbf B^T\mathbf B)^{-1}\mathbf B^T\mathbf x}_{\lambda} = \mathbf B \lambda\\$$矩阵 $\mathbf P$ 是 projection matrix，向量 $\mathbf \lambda \in \mathbb R^c$ 是 $\widetilde{\mathbf x}$ 的 “coordinates” or “codes”. 我们将 $\widetilde{\mathbf x}^\perp =\text{difference/error}= \mathbf x - \widetilde{\mathbf x}$ 视为 “error/difference”, PCA 的obejctive就是使其尽可能小。 因此，PCA just want to use $\widetilde{\mathbf x}$ to approximate the original vector $\mathbf x$，i.e. $\mathbf x \approx \widetilde{\mathbf x}$. and meanwhile, retain the error as small as possible. Remark: both of them is still $\in \mathbb R^n$. PCA 算法至此似乎告一段落了，因为我们最终得出了对向量 $\mathbf x \in \mathbb R^n$ 近似 $\widetilde{\mathbf x} \in \mathbb R^n$， 而 $\widetilde{\mathbf x}$ 可以由其coordinate $\lambda$ 唯一表示（以 $\mathbf B$ 的列为basis vector），那么我们就可以用一个低维的 $\lambda$ 去代替原始的特征向量 $\mathbf x$, 图示如下：$$\begin{align}&amp;\color{red}{\mathbf x} \approx \color{red}{\widetilde{\mathbf x}}\in\mathbb R^n \longleftrightarrow \color{blue}{\lambda}\in \mathbb R^c \text{, so}\\&amp;\color{red}{\mathbf x}\in\mathbb R^n \longrightarrow \color{blue}{\lambda} \in \mathbb R^c. \\&amp; \text{dimension reduced!}\end{align}$$Remark: PCA 就是将向量 $\mathbf x \in \mathbb R^n$ 正交分解为 $\widetilde{\mathbf x} + \widetilde{\mathbf x}^\perp$, 然后“丢弃” $\widetilde{\mathbf x}^\perp$ 将其看做 “error”，并使其最小化从而得到满足需求的 $\widetilde{\mathbf x}$ 及其coordinates $\lambda$, 最终以 $\lambda$ 代替 original vector $\mathbf x$ 来参与后续的计算。 现在的困难是：$\mathbf B$ 和 $\lambda$ 到底如何确定呢（ $\lambda $ 也由 $\mathbf x$ 和 $\mathbf B$ 决定)？向量 $\mathbf x$ 所在的空间$\mathbb R^n$ 无穷多的子空间，选取不同的子空间就对应不同的矩阵 $\mathbf B$, 自然对应很多的 $\widetilde{\mathbf x}$! 因此，我们必须回答以下问题才能最终完成整个PCA的计算过程： 如何选取合适的subspace $W$ (i.e. 如何选取basis vectors $\mathbf b_1,…,\mathbf b_c$),以得到具体的 coordinates $\lambda$ ($\lambda_1,…,\lambda_c$), 才能使得 “误差” 最小？这个所谓的“小” 如何来衡量呢？ 我们需要一个objective，我们optimize它可以为我们解答上述问题。 Objective of PCA现在，我们暂时忘记之前的几个几何概念（如：正交投影，正交补， etc.）与PCA的联系，从PCA的目标函数出发去得到合适的参数： a lower dimensional subspace: basis vectors are $\mathbf b_1,…,\mathbf b_c$ coordinates corresponding to each original vector (example in data set). 事实上，minimize objective 之后得到的参数，恰恰符合之前纯粹几何角度看待PCA的情况： 对一个向量的所有分解中，投影（正交分解）的结果与original vector 的“误差”最小（用2-norm衡量）。 把 original vector $\mathbf x$ 投影到其上的那些 lower dimensional subspaces ，恰恰就是使得整个数据集的 variance最大的那些 subspace。 也就是说，PCA可以同时从几何与优化的角度解释；而且优化的结果完全包含并符合几何解释。 总之，如果纯粹从几何角度创造PCA的话，我们只能回答“分解必须是正交分解（使用正交投影）”，仅仅到这一步。 但无法解决 subspace 的选取问题，即：沿着些 subspace（方向）进行投影，才能保证所有 original vectors 在所有的可能子空间（无穷多）上的投影所得到的 projected vector 与 original vector 之间 difference/ error 的2-norm 之和最小！ 回忆PCA的目标：“将高维向量 $\mathbf x \in \mathbb R^n$ , 投影到一个lower-dimensional subspace of $\mathbb R^n$ 得到 $\widetilde{\mathbf x}$ 的同时保留足够多的信息（损失足够少的信息）。” 记住，$\widetilde{\mathbf x}$ is still $\in \mathbb R^n$, but it lives in a c-dimensional subspace: $\mathbb R^c$. 这里我们用2-norm 来衡量两个向量之间difference的大小。那么根据上小节的内容， “损失最小” 即：使得 $\Vert\mathbf x - \widetilde{\mathbf x}\Vert ^2$ 最小。 我们的数据集 (design matrix) 为：$\mathbf X \in \mathbb R^{m\times m}=\lbrace \mathbf x_1,…,\mathbf x_n\rbrace$, where each $\mathbf x_k (k=1,…,m)\in \mathbb R^n$. 综合1，2，PCA完整的 objective 有如下三种具体形式： Sum of 2-norm of difference:$$\mathcal L(\mathbf B,\lambda) = \frac{1}{m}\sum_{k=1}^m \Vert {\mathbf x}_k - \widetilde{\mathbf x}_k\Vert ^2 \\$$ Orthogonal projection, say $\mathbf {\widetilde{x}}$ could be represented as a linear combination of the basis vectors of which the projected onto subspaces.$$\mathcal L(\mathbf B,\lambda)=\frac{1}{m}\sum_{k=1}^m\Vert \mathbf x_k - \sum_{i=1}^c\lambda_{ik} \mathbf b_i \Vert^2$$ We can rewrite the form of linear combination as “Matrix-Form”: $$\mathcal L(\mathbf B,\lambda) = \frac{1}{m}\sum_{k=1}^m \Vert {\mathbf x}_k - \mathbf B\lambda_k \Vert ^2$$ 符号说明： $\mathbf x_k$ : k-th vector in design matrix, i.e. k-th example in datasets. $\widetilde{\mathbf x}_k$ : the projection of $\mathbf x_k$ onto the c-dimensional subspace $\mathbb R^c$. $\mathbf b_i$ : the i-th basis vector of the subspace $\mathbb R^c$. $\lambda_{ik}$ : the coordinate that corresponding to the i-th basis vector $\mathbf b_i$ for $\mathbf x_k$. We also make two general assumptions: Centering the data: $E[\mathbf X] = \mathbf 0$. Means that: $\mathbf x_k := \mathbf x_k - \mathbf \mu$ ($\in \mathbb R^n$.) The basis vectors $\mathbf b_1,..,\mathbf b_c$ are orthonormal. Optimization for PCA: Remark: 和一般地优化问题-“设计objective，minimize/maximize 这个objective function，从而求出optimal parameters” 的流程相同；但从 geometry 角度出发推导、解释、实现PCA的时候，我们是这样思考的： 首先，几何上我们知道有一个lower-dimensional subspace, 并且我们要将 original vector $\mathbf x$ 正交投影到这个subspace 上，得到 $\widetilde{\mathbf x}$。 基于1，根据正交补 and 正交分解的知识，我们知道：$\mathbf x - \widetilde{\mathbf x}$ 就是一个original vector 和 orthogonal projected vector 的距离(误差 or 残差)。 $$\begin{align}&amp;\underbrace{\text{min}}{\lambda} \mathcal L \Rightarrow x\\&amp;\frac{\partial \mathcal L}{\partial \lbrace \lambda{ik},\mathbf b_i\rbrace} = 0\\&amp;where: i=1,…,c; k=1,…,m\end{align}$$ 利用多元函数链式法则, 我们得到： $$\frac{\partial \mathcal L}{\partial \lbrace \lambda_{ik},\mathbf b_i\rbrace} =\frac{\partial \mathcal L}{\partial \widetilde{\mathbf x}_k}\frac{\partial \widetilde{\mathbf x}k}{\partial \lbrace \lambda{ik},\mathbf b_i\rbrace}$$ 对于每个original vector而言， PCA 算法参考资料 关于orthogonal decomposition 关于orthogonal complement 关于PCA Almost all things you need to know about PCA. Asad [^1]: 这里可能使得读者在概念上产生一定的混淆，实际上这里 design matrix 采用了两种表示方式，第一种即矩阵表示，行的个数代表 # of examples, 列的个数代表 # of features/dimensions； 第二种是采用集合的表示方式，其中每个元素都是design matrix 中的一行值构成的列向量。这仅仅是符号上的选择，无关乎问题本质。]]></content>
      <categories>
        <category>课程笔记</category>
        <category>Mathematics for Machine Learning</category>
        <category>PCA</category>
      </categories>
      <tags>
        <tag>PCA</tag>
        <tag>Dimensonality Reduction</tag>
        <tag>Compressing</tag>
        <tag>Changing of Basis</tag>
        <tag>Geometric</tag>
        <tag>Covariance Matrix</tag>
        <tag>Random Vector</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三. 正交投影]]></title>
    <url>%2Farchive%2F2018-07-23%2FOrthogonal-Projections%2F</url>
    <content type="text"><![CDATA[简介 In this module, we will look at orthogonal projections of vectors, which live in a high-dimensional vector space, onto lower-dimensional subspaces. This will play an important role in the next module when we derive PCA. We will start off with a geometric motivation of what an orthogonal projection is and work our way through the corresponding derivation. We will end up with a single equation that allows us to project any vector onto a lower-dimensional subspace. However, we will also understand how this equation came about. As in the other modules, we will have both pen-and-paper practice and a small programming example with a jupyter notebook. 学习目标 基于不同的 Inner Product 计算正交投影 深刻理解正交投影是分解！ Relate projections to the reconstruction error and compute it 写代码将 image data 投影到 a 2-dimensional 子空间（高维$\rightarrow $ 低维） High dimension data 经常难以分析、可视化，并且有时数据的关键信息只隐含在其中几个维度中，其他大多数维度并没有蕴含有价值的信息。我们压缩数据的时候，会损失一些信息，理想的状况是：我们保留了那些含有关键信息的维度，而把大量无关的维度去除掉，从而在信息损失度较小的情况下压缩数据，以便用来分析、可视化。 一个核心的 dimensionality reduction 的方法是 PCA， 其中起关键作用的操作就是：向量的正交投影， i.e. Orthogonal Projection. 正交投影（orthogonal projection） 也是矩阵正交化的基础，也即：Gram-Schimt过程的基础。 我们平时所讲的“投影”，不加说明的情况下通常就是指正交投影。 Projection onto 1-D subspaces Figure 1: 2-D vector projection onto 1-D subspace 先对上图中一些基本概念做一说明： $ \mathbf x \text{ is a vector that } \in \mathbb R ^2$ $ \mathbf U \text{ is a 1-D subspace in } \mathbb R^2$ $\mathbf b \text{ is the basis vector of subpace } \mathbf U \text{ and that we want to project x onto.}$ So, every vector in U can be represented as linear combination of $\boldsymbol b$ , i.e. $\beta \boldsymbol b$ $\mathbf u \text{ is any vector in U, i.e. } \mathbf u = \beta \mathbf b $ $\color{purple}{\text{The pruple line }} \text{is the difference vector of } \mathbf x \text{ and the vectors on } \mathbf U $ $\color{purple}{\text{difference vector is : }} diff=\mathbf x - \mathbf u$. 从图中我们发现： 向量 $\mathbf x$ 在 $\mathbf U$上的正交投影是$\mathbf U$ 中所有向量中与 $\mathbf x$ 距离最近的向量。 此时，$\mathbf x$ 与 其在 $\mathbf U$ 上的正交投影之差- $diff$ 与 $\mathbf U$ 垂直，$diff$ 的长度最小。 我们将 $\mathbf x$ 在 $\mathbf U$ 上的 orthogonal projection 记为: $\pi_{\mathbf U}(\mathbf x)$. 关于$\pi_{\mathbf U}(\mathbf x)$ 有如下2个等式成立: $\pi_{\mathbf U}(\mathbf x) \in \mathbf U \Rightarrow \exists \lambda \in \mathbb R:\pi_{\mathbf U} (\mathbf x) = \lambda \mathbf b (\text{ as }\pi_{\mathbf U}\mathbf x \in \mathbf U)$ $\lambda$ is coordinate of orthogonal projection of $\mathbf x$ onto $\mathbf U$ with respect to basis vector $\mathbf b$. $\langle \mathbf b, \color{purple}{\mathbf x - \pi_{\mathbf U}\mathbf x \rangle} = 0 (\text{ orthogonality})$ 我们可以借此，不过度依赖几何概念、完全从线性代数的视角，只用Inner product的概念就可以求出向量 $\mathbf x$ 在 $\mathbf U$ 中的正交投影$\pi_{\mathbf U}(\mathbf x)$， 如下：$$\begin{align}1,2&amp;\Rightarrow \langle \mathbf b, \mathbf x\rangle - \langle \mathbf b, \pi_{\mathbf U}(\mathbf x )\rangle = 0 \\&amp; \Leftrightarrow \langle \mathbf b, \mathbf x\rangle - \langle \mathbf b, \lambda \mathbf b \rangle = 0\\&amp; \Leftrightarrow \langle \mathbf b, \mathbf x\rangle - \lambda \Vert \mathbf b \Vert ^2 = 0\\&amp; \Leftrightarrow \lambda = \frac{ \langle \mathbf b, \mathbf x\rangle}{\Vert \mathbf b\Vert^2}\\&amp;\Rightarrow \pi_{\mathbf U}(\mathbf x) = \lambda\mathbf b = \underbrace{ \frac{ \langle \mathbf b, \mathbf x\rangle}{\Vert \mathbf b\Vert^2}}_{\mathbf \lambda} \mathbf b\end{align}$$ 如果我们选择 dot product 作为我们的 Inner product, 则有： $$\pi_{\mathbf U}(\mathbf x) =\frac{\mathbf x^T \mathbf b}{\Vert \mathbf b\Vert^2}\mathbf b =\frac{ \mathbf b^T \mathbf x}{\Vert \mathbf b\Vert^2}\mathbf b = \color{blue}{\frac{ \mathbf b\mathbf b^T }{\Vert \mathbf b\Vert^2}} \mathbf x = \color{blue}{\underbrace{\frac{ \mathbf b\mathbf b^T }{\mathbf b^T \mathbf b}}_{\mathbf P}}\mathbf x$$The blue part in above last equation is so called $\color{blue}{\text{Projection Matrix}}$, which could projection any vector in $\mathbf R^2$ onto 1-D subspace $\mathbf U$, and Projection Matrix is symmetric. 总结一下： Orthogonal Projection of $\mathbf x$ is also a vector in $\mathbb R^2$, and meanwhile, it is also a vector in 1-D subspace $\mathbf U$. But we never need two coordinates to represent it, we just need only one coordinate $\lambda$. $\lambda$ is the coordinate of orthogonal projection of $\mathbf x$ onto $\mathbf U$ with respect to b which is the basis vector of $\mathbf U$. The difference of vector $\mathbf x \in \mathbb R^2$ and its orthogonal projection is orthogonal to subspace $\mathbf U$- this is where the name “orthogonal projection” came from. The orthogonal projection is the specific vector in $\mathbf U$ that is closest to $\mathbf x$ in $\mathbb R^2$. Namely, the difference of $\mathbf x$ and its orthogonal projection is the smallest among the distances of the vectors(points) in $\mathbf U$ and $\mathbf x$. Formally, $\pi_{\mathbf U}(\mathbf x)$ is closest to $\mathbf x$. From the perspective of difference/distances between $\mathbf x$ and any vectors in $\mathbf U$, that is, the specific difference $\mathbf x -\pi_{\mathbf U}(\mathbf x)$ is the smallest among all differences $\mathbf x - \mathbf u $, where $\mathbf u \in \mathbf U$. Difference has some other names such as reconstruction error, residual, error, etc. In machine learning context, it is called loss or cost function; in optimization context, it is called objective function. Projection onto higher dimensional subspaces Figure 2: Projection onto a two-dimensional subspace $\mathbf U$ with basis $\mathbf {b_1,b_2}$. The projection $ \pi_{\mathbf U}(\mathbf x)$ of $\mathbf x \in \mathbb R^3$ onto $\mathbf U$ can be expressed as a linear combination of $\mathbf {b_1,b_2}$ and the displacement vector $\mathbf x - \pi_{\mathbf U}(\mathbf x)$ is orthogonal to both $\mathbf b_1$ and $\mathbf b_2$. In the following, we will look at orthogonal projections of vectors $\mathbf x \in \mathbb R^n$ onto higher-dimensional subspaces $\mathbf U \subseteq \mathbb R^n$ with $dim(\mathbf U)=m \ge 1$. An illustration is given in Figure 2. 我们假设 $(\mathbf {b_1,b_2,…,b_m})$ 是 $\mathbf U$ 的一组基。 Projections $\pi_{\mathbf U}(\mathbf x)$ onto $\mathbf U$ are elements of $\mathbf U$，因此， 他们可以被表示成 $(\mathbf {b_!,b_2,…,b_m})$ 的线性组合，使得 $\pi_{\mathbf U}(\mathbf x) = \sum_{i=1}^m \lambda_i \mathbf b_i$. As in the 1-D case, we follow a three-step precedure to find the projection $\mathbf p=\pi_{\mathbf U}(\mathbf x)$ and the projection matrix $\mathbf P_\pi$: Find the coordinates $\lambda_1,\lambda_2,…,\lambda_m$ of the projection (with respect to the basis of $\mathbf U$), such that the linear combination$$\begin{align}&amp;\pi_{\mathbf U}(\mathbf x) = \mathbf p = \sum_{i=1}^m\lambda_i \mathbf b_i =\mathbf {B\lambda},\\&amp;\mathbf B = [\mathbf{b_1,b_2,…,b_m}] \in \mathbb R^{n \times m} ,\quad \mathbf \lambda = [\lambda_1,\lambda_2,…,\lambda_m]^T \in \mathbb R^m\end{align}$$is closest to $\mathbf x \in \mathbb R^n​$. As in the 1-D case, “closest” means “minimum distance”, which implies that the vector connecting $\mathbf p \in \mathbf U​$ and $\mathbf x \in \mathbb R^n​$ is orthogonal to all basis vectors of $\mathbf U​$. Therefore, we obtain $m​$ simultaneous conditions (assuming the dot product as the inner product)$$\begin{align}\langle \mathbf {b_1, x - p}\rangle = \mathbf b_1^T(\mathbf {x-p}) =0\\\langle \mathbf {b_2, x - p}\rangle =\mathbf b_2^T (\mathbf {x-p}) = 0\\\vdots \\\langle \mathbf {b_m, x - p}\rangle = \mathbf b_m^T (\mathbf{x-p}) =0\end{align}$$which, with $\mathbf{p = B\lambda}​$, can be written as$$\begin{align}\mathbf b_1^T (\mathbf{x - B\lambda}) = 0\\\vdots\\\mathbf b_m^T (\mathbf{x - B\lambda}) = 0\end{align}$$such that we obtain a homogeneous lienar equation system$$\begin{align}\begin{bmatrix}\mathbf b_1^T\\\vdots \\\mathbf b_m^T\end{bmatrix}\begin{bmatrix}\\mathbf {x -B\lambda}\\\end{bmatrix}= \mathbf 0&amp;\Leftrightarrow \mathbf B^T (\mathbf {x - B\lambda}) = \mathbf 0 \\&amp;\Leftrightarrow \mathbf B^T \mathbf {B\lambda} = \mathbf B^T \mathbf x\end{align}$$The last equation is called normal equation. Since $\mathbf {b_1,b_2,…,b_m}$ are a basis of $\mathbf U$ and( $n \ge m$ ), therefore, linearly dependent, $\mathbf B^T \mathbf B \in \mathbb R^{m \times m}$ is regular and invertable. This allows us to solve for the optimal coefficients/coordinates$$\mathbf{\lambda} = (\mathbf B^T\mathbf B)^{-1}\mathbf B^T \mathbf x$$The matrix $(\mathbf B^T\mathbf B)^{-1}\mathbf B^T$ is also called the pseudo-inverse of $\mathbf B$, which can be computed for non-square matrices $\mathbf B$. It only requires that $\mathbf B^T\mathbf B$ is positive definite, which is the case if $\mathbf B$ is full-rank.[^1] In fact, the matrix $\mathbf B^T\mathbf B \in \mathbb R^{m \times m}$ is called Gram Matrix, no matter what $\mathbf B$ is (not necessarily symmetric or even square) is always positive semi-definite; if $n\ge m$ (and we assume for convenience that $\mathbf B$ is full rank.) , then $\mathbf B^T \mathbf B$ is positive definite. Find the orthogonal projection $\pi_{\mathbf U}(\mathbf x) =\mathbf p \in \mathbf U$. We already established that $\mathbf p = \mathbf {B\lambda}$. Therefore, we obtain $$\mathbf p = \color{blue}{\mathbf B(\mathbf B^T \mathbf B)^{-1}\mathbf B^T} \mathbf x$$ Find the projection matrix $\mathbf P_{\pi}$. From above equation, we can immediately see that the projection matrix that solves $\mathbf P_{\pi}\mathbf x = \mathbf p$ must be$$\color{blue}{\mathbf P_{\pi} = \mathbf B(\mathbf B^T\mathbf B)^{-1}\mathbf B^T}$$ 此时我们注意到： $\mathbf p = \mathbf B \color{blue}{\lambda}$ $\mathbf p = \color{blue}{\mathbf P_{\pi}}\mathbf x$ 其中，$\lambda$ 是 $\mathbf p$ 在 column space of $\mathbf B$ 中的坐标；而 $\mathbf P_{\pi}$ 是将 $\mathbf x$ 投影到 $\mathbf B$ 上的 Projection matrix。 Remark 1: Comparing the solutions fro projecting onto a one-dimensional subspace and the general case, we see that the general case includes the 1-D cases as a special case: If $dim(\mathbf U)=1$ then $\mathbf B^T \mathbf B \in \mathbb R$ is a scalar and we can rewrite the projection matrix $\mathbf P_{\pi} = \mathbf B(\mathbf B^T \mathbf B)^{-1}\mathbf B^T$ as $\mathbf P_{\pi} = \frac{\mathbf B\mathbf B^T}{\mathbf B^T \mathbf B}$, which is exactly the projection matrix for the one-dimensional cases. Remark 2: The orthogonal projection $\pi_{\mathbf U}(\mathbf x)$ are still vectors in $\mathbb R^n$ although they lie in an $m$-dimensional subspace $\mathbf U \subseteq \mathbb R^n$. However, to represent a projected vector we only need the $m \color{blue}{\text{ coordinates}}$ $\color{blue}{\lambda_1,\lambda_2,…,\lambda_m}$ with respect to the basis vectors $\mathbf b_1,\mathbf b_2,…,\mathbf b_m$ of $\mathbf U$. Remark 3: In vector spaces with general inner products, we have to pay attention when computing angels and distances, which are defined by means of the inner product. $\quad$ Projections allow us to look at situations where we have a linear system $\mathbf {Ax = b}$ without a solution. Recall that this means that $\mathbf b$ does not lie in the span of $\mathbf A$, i.e., the vector $\mathbf b$ does not lie in the subspace spanned by the columns of $\mathbf A$(i.e. column space of $\mathbf A$). Given that the linear equation cannot be solved exactly, we can find an approximate solution. The idea is to find the vector in the subspace spanned by the columns of $\mathbf A$ that is closest to $\mathbf b$, i.e., we compute the orthogonal projection of $\mathbf b$ onto the subspace spanned by columns of $\mathbf A$. This problem arises often in practice, and the solution is called the least square solution (assuming the dot product as the inner product) of an overdetermined system. 投影矩阵的性质$\mathbf P_{\mathbf \pi} = \mathbf P_{\mathbf \pi} ^T$ $\mathbf P_{\mathbf \pi}^2 = \mathbf P_{\mathbf \pi}$ 练习 这里有一个 Projection of vectors in n-dimensional space onto m-dimensional subspace 的全部推导过程，link . 综合练习： link. 编程练习将数据从高维投影到低维 从正交投影的角度理解linear regression. [^1]: In practical applications( e.g., linear regression), we often add a “jitter term” $\epsilon \mathbf I$ to $\mathbf B^T\mathbf B​$ to guarantee increase numerical stability and positive definiteness. This “ridge” can be rigorously derived using Bayesian inference,]]></content>
      <categories>
        <category>课程笔记</category>
        <category>Mathematics for Machine Learning</category>
        <category>PCA</category>
      </categories>
      <tags>
        <tag>Dimensonality Reduction</tag>
        <tag>Orthogonal Projection</tag>
        <tag>Projection Matrix</tag>
        <tag>Orthogonality</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二.内积（重要基础概念）]]></title>
    <url>%2Farchive%2F2018-07-21%2FInner-Product%2F</url>
    <content type="text"><![CDATA[简介 Data can be interpreted as vectors. Vectors allow us to talk about geometric concepts, such as lengths, distances and angles to characterise similarity between vectors. This will become important later in the course when we discuss PCA. In this module, we will introduce and practice the concept of an inner product. Inner products allow us to talk about geometric concepts in vector spaces. More specifically, we will start with the dot product (which we may still know from school) as a special case of an inner product, and then move toward a more general concept of an inner product, which play an integral part in some areas of machine learning, such as kernel machines (this includes support vector machines and Gaussian processes). We have a lot of exercises in this module to practice and understand the concept of inner products. 学习目标 Explain inner products, and compute angles and distances using inner products Write code that computes distances and angles between images Demonstrate an understanding of properties of inner products Discover that orthogonality depends on the inner product Dot Product $\ne$ Inner Product, the former is just one of the later. Inner Product is the generalization of dot product. Inner Product allow us to define and describe some geometric concepts. Dot ProductTwo vectors $\mathbf x, \mathbf y \in \mathbb R^n$. Definition1 $\color{blue}{\mathbf x \cdot \mathbf y = len(\mathbf x)len(\mathbf y)cos(\theta)}$ In this definition, lengths and angles are given! We use them to define the “dot product.” Definition2 $\color{blue}{\mathbf x \cdot \mathbf y = \sum_{i=1}^n x_iy_i = \mathbf x^T \mathbf y}$ In this definition, there is no lengths and angle, we only use vector itself and two operations: scalar multiplication and addition. Or, we also can just definite it in a matrix form: transpose and matrix multiplication. Length of a vector $len(\mathbf x) = \Vert \mathbf x \Vert = \sqrt { \Vert \mathbf x \Vert ^2} $ $ \Vert \mathbf x \Vert ^2 =\mathbf x \cdot \mathbf x =\mathbf x^T \mathbf x$. Note, here we are not defining the length, we just derive this property from the definiton of “dot product”. i.e. $\mathbf x \cdot \mathbf x = len(\mathbf x)len(\mathbf x)cos(\theta) =len^2(\mathbf x)$. Distance between two vectors $\mathbf d = \mathbf x - \mathbf y$ $\Vert \mathbf d \Vert = \sqrt{\mathbf d^T \mathbf d}$ Angle between two vectors. $cos(\theta) = \frac{\mathbf x \cdot \mathbf y}{\Vert \mathbf x\Vert \Vert \mathbf y \Vert} = \frac{\mathbf x^T \mathbf y}{\Vert \mathbf x\Vert \Vert \mathbf y\Vert}$ We are not definiting the angle too, we just derive this property from the definition of “dot product”. Inner ProductDefinitionInner product is a generializeiton of dot product, which can be used to represent the geometric concepts: length, distance and angle, etc. A vector space with a additional structure–inner product is referred to the inner product space. 从这一刻开始，我们首先定义 Inner Product， 然后任何的几何概念、几何操作 都用 Inner Product 来描述。我们熟悉的 dot product-也仅仅是一种 valid Inner Product 而已。 此后，几何概念（e.g. 长度、距离、夹角等待）均由内积来定义和描述。此刻开始的linear algebra，现有内积，后有几何。 Definition Formally, an inner product space is a vector space $V$ over the field $\mathbb R$ together with an inner product which is a map $\color{blue}{\langle \cdot,\cdot \rangle: V\times V \rightarrow \mathbb R}$. $\color{purple}{\text{i.e. it is a function which take two vectors as input and output a scalar.} }$ that satisfies the following three axioms for all vectors $x,y,z \in V$ and scalars $\alpha \in \mathbb R$: Commutativity $\langle \mathbf x, \mathbf y \rangle = \langle \mathbf y,\mathbf z \rangle$ Sometimes, also is called symmteric. Linearity $\langle \alpha \mathbf x, \mathbf y \rangle=\alpha \langle \mathbf x, \mathbf y \rangle$ $\langle \mathbf x+\mathbf y, \mathbf z \rangle=\langle \mathbf x, \mathbf z \rangle + \langle \mathbf y, \mathbf z \rangle$ 实际上，这一条axiom配合第一条axiom，可以很容易地推导出：$\langle x,y+z \rangle = \langle y+z,x\rangle= \langle y,x\rangle + \langle z ,x\rangle$ ,因此这一条linear axiom实际上可以说是bilinearity. Positive-definite $\langle \mathbf x,\mathbf x \rangle &gt; 0$ , $\text{for any}$ $\mathbf x \ne \mathbf 0$. $\langle \mathbf x, \mathbf x \rangle=\mathbf 0$, $\text{if and only if }$ $\mathbf x =\mathbf 0$. Example 1: $\color{blue}{\langle \mathbf x,\mathbf y \rangle=\mathbf x^T \mathbf y}$ Which is dot product, it’s a vaild inner product according to the three axioms. Example 2: $\color{blue}{\langle \mathbf x, \mathbf y\rangle = \mathbf x^T\mathbf A \mathbf y}$, where $\mathbf A$ is a symmetric positive matrix，i.e. $\mathbf A\in \mathbb S^{\dagger}$. 我们现在用三条公理验证一下： $\langle \mathbf y, \mathbf x\rangle = \mathbf y^T\mathbf A \mathbf x = (\mathbf y^T\mathbf A \mathbf x)^T=\mathbf x^T\mathbf A^T \mathbf y= \mathbf x^T\mathbf A \mathbf y = \langle \mathbf x, \mathbf y \rangle$. $\langle \mathbf x+ \mathbf y,\mathbf z\rangle = (\mathbf x+\mathbf y)^T\mathbf A \mathbf z = \mathbf x^T \mathbf A \mathbf z + \mathbf y^T \mathbf A \mathbf z = \langle \mathbf x, \mathbf z \rangle + \langle \mathbf y, \mathbf z \rangle$. $\langle \mathbf x, \mathbf x \rangle= \mathbf x^T \mathbf A \mathbf x &gt; \mathbf 0$, for any $\mathbf x\ne \mathbf0.$ (by the definition of quadratic form.) 经过三公理的检验，我们知道:$\langle \mathbf x, \mathbf y \rangle= \mathbf x\mathbf A^T \mathbf y$ 是一种合理的 Inner Product. 由此可见，dot product is just a special kind of vaild inner products. Length of vectors这里，我们先有了 Inner Product的定义，基于此我们来定义 length of vector. 注意这个顺序与在dot product 中是相反的。 Definition $\color{blue}{len(\mathbf x) = \Vert \mathbf x\Vert = \sqrt{\langle \mathbf x,\mathbf x \rangle}}$. ($norm(\mathbf x)$) 在这里，长度的概念依赖于 Inner Product. Inner Product 不同的具体形式决定了“同一个vector 的长度不是恒定不变的，随着Inner Product 的改变同一个vector的长度可能随之改变” Example 3: $\mathbf x =\begin{bmatrix} 1 \\1\end{bmatrix}$, and then $len(\mathbf x) =?$ $\langle \mathbf x,\mathbf y \rangle = \mathbf x^T\mathbf y$ ： $len(\mathbf x) = \sqrt{\langle \mathbf x, \mathbf x\rangle}= \sqrt{2}$. $\langle \mathbf x, \mathbf y\rangle = \mathbf x^T\begin{bmatrix} 2&amp; -1\\-1 &amp; 2 \end{bmatrix}\mathbf y $ : and then $len(\mathbf x) = \sqrt{\langle \mathbf x, \mathbf x\rangle} = 2$. Properties $\text{ for any }\mathbf x，\mathbf y\in \mathbb R^n$ $\Vert \mathbf x\Vert \ge 0 $. $\Vert \lambda \mathbf x \Vert = \vert \lambda \vert \Vert \mathbf x \Vert$. ( homogeneity ) $\Vert \mathbf x + \mathbf y \Vert \le \Vert \mathbf x \Vert + \Vert \mathbf y \Vert.$ (triangle equality) $\vert \langle \mathbf x , \mathbf y\rangle\vert \le \sqrt{\langle \mathbf x, \mathbf x \rangle}\sqrt{\langle \mathbf y,\mathbf y \rangle} = \Vert \mathbf x \Vert \Vert \mathbf y \Vert $ .( Cauchy-Schwart inequality ) Distance between two vectors同样，也是基于Inner Product 的定义。也就是说，2个向量间的距离取决于 Inner Product 的具体形似；不同的 Inner Product 会导致不同的距离。 $d(\mathbf x, \mathbf y)=\Vert \mathbf{x-y} \Vert = \sqrt{\langle (\mathbf{x-y}),(\mathbf {x -y}) \rangle}$. Example 4: $\mathbf x = \begin{bmatrix}2 \\3 \end{bmatrix}, \mathbf y = \begin{bmatrix}4\\1 \end{bmatrix}$. $\langle \mathbf x, \mathbf y\rangle = \mathbf x^T \mathbf y$ : $d(\mathbf x, \mathbf y) =\sqrt {(\mathbf{x-y})^T(\mathbf {x-y})}=\sqrt{8}$. $\langle \mathbf x, \mathbf y\rangle =\mathbf x^T \mathbf A \mathbf y$ , where $\mathbf A = \begin{bmatrix}2 &amp; -1\\-1&amp;2 \end{bmatrix}$ : $d(\mathbf x, \mathbf y) = \sqrt{(\mathbf {x-y})^T\mathbf A (\mathbf {x-y})} = \sqrt{24}$. 我们可以看出，在不同的 Inner Product 下，同样两个向量的 distance是不一样的。 Angles and orthogonalityAngles between two vectors. $cos(\theta) =\frac{\langle \mathbf x, \mathbf y \rangle}{\sqrt{\langle \mathbf x, \mathbf x\rangle}\sqrt{\langle \mathbf y, \mathbf y\rangle}} = \frac{\langle \mathbf x, \mathbf y \rangle}{\Vert \mathbf x \Vert \Vert \mathbf y \vert}$. 注意这个定义和本节课最初的和角度相关的公式的区别. 之前的角度公式是dot product 的性质；这里我们是用 Inner Product 来定义角度 ； 之前我们是先有了角度，才有dot product，现在我们是先有了 Inner product，然后才定义了角度。 既然angles 由 Inner Product 来定义，那么与之前的length， distance一样：$cos(\theta)$ 取决于 Inner Product的定义，不同的 Inner Product 会导致 angle不同。 Orthogonality The definition of orthogonality depend on Inner product: $ \langle \mathbf x, \mathbf y \rangle = 0 \Leftrightarrow $ $\mathbf x$ and $\mathbf y$ is orthogonal $\Leftrightarrow \theta =\frac{\pi}{2}rad =90^\circ.$ Again, 我们再一次强调，之前我们的结论是：先有两向量垂直，再有 dot product 为0; 现在流程反过来：先有Inner product 为 0， 再有两向量垂直。这可以体现出，哪个concept 是主导概念，哪个是附属概念。这里， Inner Product 显然是主导概念。 Example 5: $\mathbf x = \begin{bmatrix}1\\1 \end{bmatrix}, \mathbf y=\begin{bmatrix} -1\\1 \end{bmatrix}.$ $\langle \mathbf x, \mathbf y\rangle = \mathbf x^T \mathbf y$ : $cos(\theta) = 0 \Rightarrow \theta = 90^\circ$, i.e. $\mathbf x$ and $\mathbf y$ are orthogonal. $\langle \mathbf x, \mathbf y\rangle = \mathbf x^T \begin{bmatrix}2 &amp; 0\\ 0 &amp; 1 \end{bmatrix} \mathbf y = -1\ne 0 $ : i.e. $\mathbf x$ and $\mathbf y$ are not orthogonal. 同样2个向量，基于不同的 inner product 是否 orthogonal 并不一定是一致的。 几何观点： 从几何观点看待两个向量 orthogonal，即：这两个向量最不相似，除了原点之外没有共同之处。 Inner product of functions and random variable思考之前 Inner Product 的定义：其本质是一个函数，输入2个vectors 输出一个 scalar. 如果现在的 vector 是函数，那什么函数符合这个原则？积分！ Inner product for functionDefinition现在，$V$ 是一个function space which means that each vector in this vector space is a function . $\color{blue}{\langle f,g\rangle = \int_a^b f(x)g(x)dx}$ 上面定义的 innder product 符合3哥axioms，如下： Commutativity $\langle f,g\rangle = \int_a^b g(x)f(x)dx = \langle g,f \rangle$. Linearity $\langle f+h,g \rangle = \int_a^b(f(x)+h(x))g(x)dx = \int_a^bf(x)g(x)dx + \int_a^b h(x)g(x)dx = \langle f,g\rangle + \langle h,g \rangle$ $\langle\alpha f,g \rangle = \int_a^b \alpha f(x)g(x) = \alpha \int_a^b f(x)g(x)dx = \alpha \langle f,g \rangle$ Positive-definite $\langle f,f\rangle = \int_a^b f^2(x)dx &gt;0$ for any $f(x)\ne 0$ when $x\in[a,b]$. $\langle f,f\rangle = \int_a^b f^2(x)dx =0$ if and only if $f(x) = 0$ for $x\in [a,b]$. Orthgonal/angle类比我们在传统向量空间中的概念，函数空间中也有类似概念：两个函数的正交性,夹角等。 Example 6: $f(x) = sin(x), g(x) = cos(x), a=-\pi,b = \pi$ ,so $\langle f,g\rangle = \int_{-\pi}^{\pi} sin(x)cos(x)dx$. $\langle sin(x), cos(x)\rangle = 0 \Rightarrow \color{purple}{\text{sin(x) and cos(x) are orthogonal.}}$ Example 7: $\lbrace 1, cosx, cos2x,cos3x,…\rbrace$ Functions in this set Is orthogonal to each other. Norm/length of function$len(f(x))=norm(f(x)) = \Vert f(x) \Vert=\sqrt{\langle f,f\rangle}$. Example 8: innder product is defined as the one in example 6. $len(sin(x)) = \int_{-\pi}^{\pi} sin(x)sin(x)dx = 4$. Inner product for random variableDefinitionIf two random variables are uncorrelated, then $var[X+Y] = var[X] + var[Y]$. If we define inner product is : $\color{blue}{\langle X,Y\rangle =cov[X,Y]}$, 我们现在验证其是否为一个 valid inner product. Commutativity $\langle X,Y\rangle = cov[X,Y] = cov[Y,X] = \langle Y,X\rangle$. Linearity $\langle X+Z,Y\rangle = cov[X+Z,Y]= cov[X,Y] + cov[Z,Y] = \langle X,Y \rangle + \langle Z,Y \rangle$. $\langle \alpha X,Y\rangle = cov[\alpha X,Y]=\alpha cov[X,Y]=\alpha \langle X,Y \rangle$. Positive-definite $\langle X,X \rangle = cov[X,X]=var[X] &gt;0$ for any random variable $\sigma(X) \ne 0$. 所以，covariance 是 valid inner product. Norm/length$len(X)=norm(X) = \sqrt{\langle X,X\rangle} = \sqrt{cov[X,X]} = \sqrt{var[X]}=\sigma(X)$ 随机变量的长度 就是其 standard deviation。 Angle$cos(\theta) = \frac{\langle X,Y\rangle}{\sqrt{\langle X,X \rangle}\sqrt{\langle Y,Y \rangle}}= \frac{cov[X,Y]}{\sqrt{var[X]}\sqrt{var[Y]}}$. which is the just the correlation coefficient. 两个随机变量间的夹角就是他们的相关系数。并且： $\theta = 0 \Leftrightarrow$ $X$ And $Y$ are uncorrelated $\Leftrightarrow$ $X$ and $Y$ are orthogonal. Distance$$\begin{align}\color{blue}{d(X,Y)} &amp;= \Vert\langle X-Y,X-Y\rangle \Vert \\&amp;= \sqrt{\langle X-Y,X-Y\rangle} \\&amp;= \sqrt{cov[X-Y,X-Y]}\\&amp;=\sqrt{var[X-Y]}= \color{blue}{\sigma(X-Y)}\end{align}$$ Basic vectorsHere is a document which describe the basic vector clearly, you can download here. 总结 在这一节课，我们先定义了什么是 Inner Product： 一个函数，输入为两个 vectors, 输出为一个 scalar. 在此基础之上，我们定义了 length（norm）,angle, distance. 也就是说，内积直接决定了这些概念，随着选择不同的内积，这些概念的取值可能发生变化。 我们将上述概念扩展到了 function space and random variable space. 我们重新认识了 Basic vectors. 一份在线练习，用来巩固关键概念并借此实现一个经典分类算法-KNN 分类， link.]]></content>
      <categories>
        <category>课程笔记</category>
        <category>Mathematics for Machine Learning</category>
        <category>PCA</category>
      </categories>
      <tags>
        <tag>Inner Product</tag>
        <tag>Vector Space</tag>
        <tag>Function Space</tag>
        <tag>Random Variable Space</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一.数据集的统计特性]]></title>
    <url>%2Farchive%2F2018-07-17%2FStatistics-of-Datasets%2F</url>
    <content type="text"><![CDATA[简介 Principal Component Analysis (PCA) is one of the most important dimensionality reduction algorithms in machine learning. In this course, we lay the mathematical foundations to derive and understand PCA from a geometric point of view. In this module, we learn how to summarize datasets (e.g., images) using basic statistics, such as the mean and the variance. We also look at properties of the mean and the variance when we shift or scale the original data set. We will provide mathematical intuition as well as the skills to derive the results. We will also implement our results in code (jupyter notebooks), which will allow us to practice our mathematical understand to compute averages of image data sets. 学习目标 Compute basic statistics of data sets Interpret the effects of linear transformations on means and (co)variances Compute means/variances of linearly transformed data sets Write code that represents images as vectors Write code that computes basic statistics of datasets Dataset数据集简介数据集这个概念分为好几个角度和层次，不能一概而论。 从数据处理的流程先后划分， 原始数据 清洗之后的数据 特征转化之后的数据- 特征向量 从数据的 modality 划分， text image speech video（hybrid） Example1: Text Data Raw Data “This is a nice course in edX, I love it so much. Here is the link [URL], no thanks! Haha!” - a natural sentence. Pre-processed Data “a nice course edX, I love it so much. Here link, no thanks! Haha!” 预处理可能包含， 去除stop word 去除URLs 大小写转换 … Example 2: Image data Raw data Transformed data 图片完成了转化，从一个 $3\times 3$ 的matrix 转成 $\in \mathbb R^9$ 的向量。（NN的输入必须是向量） 无论是什么modality的数据，最终都将转化为如下形式之后送给分类器(e.g. NNs) 送给分类器作为输入-特征向量- 这部分最为核心 $X_1$ (feature 1) $X_2$ (feature 2) $\cdots$ $X_n$ (feature n) Label $x_{11}$ $x_{12}$ $\cdots$ $x_{1n}$ $y_1$ $x_{21}$ $x_{22}$ $\cdots$ $x_{2n}$ $y_2$ $\vdots$ $\vdots$ $\cdots$ $\vdots$ $\vdots$ $x_{m1}$ $x_{m2}$ $\cdots$ $x_{mn}$ $y_m$ 上述表格中各个量含义如下： 上面的表可以叫做：data table; data frame; data matrix etc. 其代表数据集。（未必是数据集的原始形态） n 代表 feature 的个数。 $X_i$: feature，我们把它建模为一个随机变量。 Data matrix 中的一行我们称为一个example，是一个 vector $\in \mathbb R^n$，其中第 $j$ 行记为：$\mathbf x_j$. 注意，通常我们把它表示为列向量，虽然其组织的形式上是“行”。实际中，根据context来决定，其本质是等价的。 树立一个观念不同领域对数据集中各个要素的称呼不同，这里做一个简单总结。 数据集中的要素 统计学 机器学习 数据库 行 instance example Record 列 attribute feature attribute 整体 Sample data sets Table 各种叫法会带来一些困惑，但是我们应该树立一个观念，； 数据集中的一条数据, 在数学上是高维空间中的一个point；整个数据集就是分布在空间中的很多points Example： 数据集中的每个example 都有$n$ 个特征，那么个example 就是一个 vector $\in \mathbb R^n$. 概率视角我们把数据的每个特征看作一个随机变量，则其特征集合$\lbrace X_1,X_2,…,X_n \rbrace$ 就构成了一个随机向量，记为：$\mathbf{X}$ or $\mathbf X = \lbrace X_1,X_2,…,X_n\rbrace^T$, column vector. 那么，包含$m$ 个examples 的数据集可以看作是随机向量 $\mathbf X$ 的 m 个具体取值，也同时构成了一个$\in \mathbb R^{m\times n}$ matrix，也记为：$\mathbf X$. 符号$\mathbf X$ 有时候表示随机向量$\lbrace X_1,X_2,…,X_n\rbrace$，有时候用以表示 data matrix。具体情况取决于具体的context。 *一般来讲， 当出现2个indexes的时候表示 data matrix, e.g. $\mathbf X_{ij}$；表示data matrix 中第$i$ 行，第$j$ 列的 entry，当然从概率视角出发，它也同时表示 “随机变量 $X_j$” 的第 $i$ 个具体取值。 当出现一个index 的时候表示随机向量， e.g. $\mathbf X_j$ 即为 $X_j$, 即表示第 $j$ 个 Mean Value区别两个概念 mean 均值，既可出现在统计的context 下，也可出现在 probability 的context下。 expectation 期望，只能与 随机变量/向量 一起使用。也就是说，只有出现随机变量这个概念的时候，expectation 才可以使用。 $\mathcal D = \lbrace \mathbf x_1,\mathbf x_2,…,\mathbf x_m\rbrace$ 是我们的数据集，其中 $\mathbf x_i$ 是一个example，或者叫做 data point，是随机向量$\mathbf X$ 的一组具体取值。 根据上小节的知识，我们用随机向量/变量 $\mathbf X$ 代替 $\mathcal D$. Mean value of $\mathcal D$ is defined as:$$\mathbb E[\mathbf X] = \frac{1}{m}\sum_{i=1}^m \mathbf x_i$$mean value 不一定是数据集中的一个具体的点，它只是表示数据集中所有 examples的一个平均值；从 data space 的角度看，mean 就是 分布在空间中的那些点（来自数据集）的质心。 Variances and covariances Variance 衡量数据的分散程度，从平均意义上衡量数据集中的每个点与均值的差异的大小。 上图中所示的两组points, 拥有相同的均值，但红色point 分散一些；蓝色point集中一些。 1-D data所谓1-D 数据 是指 “该数据只涉及一个特征 ”，也即：只有一个表示特征的随机变量记为，$X$ $X = {x_1,x_2,…,x_m}$, where $x_i \in \mathbb R$ 是随机变量$X$的 $m$ 个具体取值。 variance of $X$ is defined as follows:$$\begin{align}var[X] &amp;= \mathbb E[(X - \mathbb E[X])^2] = \mathbb E[X^2] - (\mathbb E[X])^2 \\&amp;=\frac{1}{m}\sum_{i=1}^m (x_i -\mu)^2 \\&amp; \mu = \mathbb E[X]\end{align}$$Example 3： 有数据集 $\mathcal D = {1,3,6,10}$, i.e. 随机变量 $X$ 取值为：${1,3,6,10}$ $\mathbb E[X] = \frac{1}{4}(1+3+6+10) = 5$ $var[\mathcal D] = \frac{1}{4}[(1-5)^2 + (3-5)^2 + (6-5)^2 + (10-5)^2] =$ 结论： Variance measures the average squared distance of every data point from the mean, which imply the spread of the data points. The less variance is, the more stable. n-D dataOk, 下面我们对多维数据做一个详细的描述。 假设我们现在的数据集中的每个 data point 都是多维向量: n 维. 不像之前在 Example 3 中的数据集，其中的data point 都是1-D，e.g. point1 =1,point2 = 3,point3 = 6,point 4 = 10. 那么，参考之前小节-《概率视角》的知识，并且与data matrix 的形式保持一致，我们可以将数据集写成如下形式： $$\begin{align}&amp;\mathbf X = \lbrace X_1,X_2,…,X_n\rbrace \\&amp;X_j = \begin{bmatrix}x_{1j} \\x_{2j} \\\vdots \\x_{mj}\end{bmatrix}\\&amp;x_{kj},k\in{1,…,m} : \text{the specific values of random variable } X_j (\text{the j-th feature}).\end{align}$$ 那么数据集亦可表示为： $$\begin{align}\mathcal D&amp;= {\mathbf x_1, \mathbf x_2,…,\mathbf x_m},\\\mathbf x_i &amp;= \begin{bmatrix}x_{i1}\\x_{i2}\\\vdots\\x_{in}\end{bmatrix}\in \mathbb R^n,\text{ which is a specific value for random vector } \mathbf X.\end{align}$$ 由于 data point 是$n$ 维 vector space 中的一个点 - a vector with $n$ components, 因此我们说数据集是$n$ 维数据集。 现实中的数据大多是多维的，因此我们总是使用多个特征来描述一个object, 比如人的特征：身高，体重，血型… 然而，当我们的数据point 是在n 维空间的时候，仅仅使用均值和方差来刻画数据集的特性就不足够了。体现在：即使两个多维数据集的均值和方差都相同，这两个数据集本身也可能会有极大的差异。如下图， Positive correlated Negative correlated 上面两幅图的 mean and variance 都相同，但我们可以明显地看出这两个数据集是有很大差异的。 具体来说：图一表明，随机变量(feature) $X$ and $Y$ 是正相关的，即：当 $x$ 变大， $y$ 也会变大； 图二表明，随机变量(feature) $X$ and $Y$ 是负相关的，即：当 $x$ 变大， $y$ 会变小。 这个时候我们需要新的度量标准来帮助我们刻画多维数据集的特性, 这个标准是：Covariance **Covariances随机变量 $X$ and $Y$ 的 covariance 定义为：$$\begin{align}cov[X, Y] &amp;= \mathbb E[(X - \mathbb E[X])(Y - \mathbb E[Y])] \\&amp;= \mathbb E[XY] - \mathbb E[X] \mathbb E [Y] \\&amp;= \mathbb E[XY] - \mu_X \mu_Y\end{align}$$随机变量$X​$ and $Y​$ 的 covariance matrix 记为：$\Sigma_{X,Y}​$， 定义为：$$\begin{align}\Sigma_{X,Y}&amp; = \begin{bmatrix}var[X,X] &amp; cov[X,Y] \\cov[Y,X] &amp; var[Y,Y]\end{bmatrix}\\&amp;=\begin{bmatrix}\sigma_{XX} &amp; \sigma_{XY}\\\sigma_{YX} &amp; \sigma_{YY}\end{bmatrix}\in \mathbb R^{2\times 2}\end{align}$$ 几个重要结论 covariance matrix of two random variables is a $2 \times 2$ matrix, denoted as $\Sigma_{X,Y}$. $\Sigma_{X,Y}$ is symmetric and positive semi-definite. This conclusions could be generalized to co cariance matrix of random vector. $cov[X,Y]$ 的正负表示两个随机变量的相关性， 如果 &gt; 0, 表明 $X$ 和 $Y$ 正相关（positively coorelated），即：$X$ 取值增大的时候，$Y$ 的取值也会变大。 如果&lt; 0, 则表明 $X$ 和 $Y$ 负相关（negatively coorelated），即：$X$ 取值增大的时候，$Y$ 的取值会减小。 Ok, 现在我们将概念推广到 $n$ 个随机变量，即推广到 $n$ 维数据集。 数据集记为: $\mathcal D =\lbrace\mathbf x_1,\mathbf x_2,…,\mathbf x_m\rbrace$, $\mathbf x_i$ 是随机向量 $\mathbf X$ 的第 $i$ 个具体取值，是个vecotr$\in \mathbb R^n$. Random vector $\mathbf X^T = \lbrace X_1,X_2,..,X_n \rbrace$. $\mathcal D$ 和 $\mathbf X$ 概念上是等价的，区别在于我们从不同的角度描述数据。多维数据（随机向量）的convariance也叫做 variance，是一个意思。 其 mean value 定义为：$$\mu_{\mathbf X} = \mathbb E[\mathbf X]=\begin{bmatrix}\mathbb E[X_1]\\\mathbb E[X_2]\\\vdots \\\mathbb E[X_n]\end{bmatrix}=\begin{bmatrix}\mu_1\\\mu_2\\\vdots \\\mu_n\end{bmatrix}\in \mathbb R^n.$$现在考虑 covariance matrix 的定义。 随机向量是随机变量的自然地扩展，我们先从这个思路出发，其定义如下： $$\begin{align} &amp;cov[\mathcal D] =var[\mathcal D]= cov[\mathbf X ] =var[\mathbf X] =\Sigma_\mathbf X \\\ &amp;=\mathbb E[(\mathbf X -\mu_{\mathbf X})(\mathbf X - \mu_{\mathbf X})^T] \\ &amp;=\mathbb E [ \begin{bmatrix} X_1 - \mu_{X_1}\\ X_2 -\mu_{X_2}\\ \vdots \\ X_n - \mu_{X_n} \end{bmatrix} \begin{bmatrix} X_1 - \mu_{X_1} &amp; X_2 - \mu_{X_2} &amp; \cdots &amp; X_n - \mu_{X_n} \end{bmatrix}]\\&amp;= \mathbb E[\begin {bmatrix}(X_1-\mu_{X_1})^2 &amp; (X_1-\mu_{X_1})(X_2-\mu_{X_2}) &amp; \cdots &amp; (X_1-\mu_{X_1})(X_n-\mu_{X_n})\\(X_2-\mu_{X_2})(X_1-\mu_{X_1}) &amp; (X_2-\mu_{X_2})^2 &amp; \cdots &amp;(X_2-\mu_{X_2})(X_n-\mu_{X_n})\\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\(X_n-\mu_{X_n})(X_1-\mu_{X_1}) &amp; (X_n-\mu_{X_n})(X_2-\mu_{X_2}) &amp; \cdots &amp;(X_n-\mu_{X_n})^2\end{bmatrix}]\\&amp;= \color{blue}{\begin{bmatrix}\mathbb E(X_1-\mu_{X_1})^2 &amp; \mathbb E(X_1-\mu_{X_1})(X_2-\mu_{X_2}) &amp; \cdots &amp; \mathbb E(X_1-\mu_{X_1})(X_n-\mu_{X_n}) \\\mathbb E(X_2-\mu_{X_2})(X_1-\mu_{X_1}) &amp; \mathbb E(X_2-\mu_{X_2})^2 &amp; \cdots &amp; \mathbb E(X_2-\mu_{X_2})^2 \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\\mathbb E(X_n-\mu_{X_n})(X_1-\mu_{X_1}) &amp; \mathbb E(X_n-\mu_{X_n})(X_2-\mu_{X_2}) &amp; \cdots &amp; \mathbb E(X_n-\mu_{X_n})^2\end{bmatrix}}\\&amp;=\color{blue}{\begin{bmatrix}var[X_1] &amp; cov[X_1,X_2] &amp;\cdots &amp;cov[X_1,X_n] \\cov[X_2,X_1] &amp; var[X_2] &amp;\cdots &amp;cov[X_2,X_n] \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\cov[X_n,X_1] &amp; cov[X_n,X_2] &amp;\cdots &amp; var[X_n]\end{bmatrix}} \\&amp;=\color{blue}{\begin{bmatrix}\sigma_{11} &amp; \sigma_{12} &amp; \cdots &amp; \sigma_{1n}\\\sigma_{11} &amp; \sigma_{22} &amp; \cdots &amp; \sigma_{2n}\\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\\sigma_{n1} &amp; \sigma_{n2} &amp; \cdots &amp; \sigma_{nn}\end{bmatrix}} \end{align}$$ 我们直接考虑variance or covariance 的含义：计算两个随机变量之间某种关联的程度。那么随机向量其本质也就是若干个随机变量的集合而已，那么当我们需要计算随机向量的variance 的时候，我们自然想到本质就是计算：该随机向量所包含的随机变量之间的covariance。很容易知道，这就像 graph 的 adjacency matrix一样，是个$n \times n$ 的矩阵。我们记为：$var[\mathbf X]$ or $cov[\mathbf X]$, ; 或者从数据集的角度为：$var[\mathcal D]$ or $cov[\mathcal D]$. 以后我们统一写为：$\Sigma_{\mathbf X}$, 简记为 $\Sigma$ , 其中： $$\Sigma_{ij} = cov(X_i, X_j)$$ 上面都是从概念上讲，现在我们给出一个便于计算的随机向量的variance/covariance 的表达式。这个表达式直接来自定义1 （公式第二行），如下： $$\begin{align}\Sigma &amp;= \mathbb E[(\mathbf X - \mathbf \mu)(\mathbf X - \mathbf \mu)^T] \\&amp;=\frac{1}{m}\sum_{i=1}^m (\mathbf x_i -\mu)(\mathbf x_i - \mu)^T\end{align}$$ 很明显，3中定义式是$m$ 个matrix 相加。可以很容易证明（展开硬算），3式确实与1式等价。 注意对比这个公式与 1中公式的异同： 来源于定义1 直接按照期望定义计算 $\mathbf x_i$ 是随机向量 $\mathbf X$ 的一组具体取值，是个 vector $\in \mathbb R^n$. 表现在 data matrix 中就是第$i$ 行。其具体形式为： $$\mathbf x_i = \begin{bmatrix}X^1_i \\X^2_i \\\cdots \\X^n_i\end{bmatrix}=\begin{bmatrix}\mathbf X_{i1} \\\mathbf X_{i2} \\\vdots \\\mathbf X_{in}\end{bmatrix}$$ $\mathbf \mu$ 是 随机向量$\mathbf X$ 的均值，即:$\mu_{\mathbf X}$，其定义为: $$\mathbf \mu = \begin{bmatrix}\mu_{X_1}\\\mu_{X_2}\\\vdots \\\mu_{X_n}\end{bmatrix}\in \mathbb R^n$$ 而其中$$\mu _{X_j} = \frac{1}{m} \sum_{i=1}^m \mathbf X_{ij}$$Note！: 按照之前的分析，$\mathbf x_{ij}$ 和 $\mathbf X_{ij}$ 指的是data matrix 中同一个entry。希望读者不要给符号给混淆了。 另一个covariance matrix 计算式：$$\begin{align}\Sigma_{\mathbf X} &amp;= E[(\mathbf X - \mathbb E[\mathbf X])(\mathbf X - \mathbb E[\mathbf X])^T]\\&amp;= E[(\mathbf X - \mathbb E[\mathbf X])(\mathbf X^T- \mathbb E[\mathbf X]^T)]\\&amp;= E[\mathbf X \mathbf X^T- \mathbf X \mathbb E[\mathbf X]^T - \mathbb E[\mathbf X]\mathbf X^T + \mathbb E[\mathbf X ] \mathbb E[\mathbf X]^T ]\\&amp;= \mathbb E[\mathbf X \mathbf X^T] - \mathbb E[\mathbf X] \mathbb E[\mathbf X^T]\end{align}$$ Linear/Affine transformation of datasets我们经常会对数据做一些线性变换，比如：shit, scale,rotation, etc. 这是数据处理中非常常见的操作。 那么在我们对数据做了线性变换之后，数据的某些统计特征会不会发生变化呢？比如 mean value and variance. Effect on the Mean提前说明 $\mathcal D + b$ = “给数据集中每个data point (which is a vector) 都加 $\mathbf b$ ” = “data matrix 中的每个entry 都加 $b$.” 即：$$\begin{align}&amp;\mathcal D +b \text{ means } \mathcal D+\Beta = \lbrace \mathbf x_1 + \mathbf b , \mathbf x_2 + \mathbf b,\cdots,\mathbf x_m +\mathbf b \rbrace\\&amp;\text{where, } \Beta_{ij} = b ,\\&amp;\text{where, }\mathbf b = \begin{bmatrix}b\\b\\ \vdots \\ b\end{bmatrix}\end{align}$$ $a\mathcal D$ = “给数据集中每个data point 都乘以2 ” = “给 data matrix 中的每个entry 都乘以 $a$.” 即：$$\begin{align}a \mathcal D &amp;= \mathbf A \mathcal D \\\text{where } \mathbf A &amp;= \begin{bmatrix}a &amp;0&amp;\cdots&amp;0 \\0&amp;a&amp;\cdots&amp;0 \\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\0 &amp; 0 &amp; \cdots&amp;a\end{bmatrix}=diag(a).\end{align}$$很多时候为了简化起见，我们很多时候直接写成：$\mathcal D +b$ 和 $a\mathcal D$ 这种形式。 Example 1: shit all the data points in the dataset, i.e. shift the dataset., same meaning. datasets $\mathcal D =\lbrace -1,2,3 \rbrace$. $\mathcal D^\prime = \lbrace 1,4,5\rbrace = \mathcal D + 2$ 数据集是1-D 数据，因此可以看做一个随机变量的取值，此随机变量记为：$X$ $\mathbb E[\mathcal D]=\frac{-1+2+3}{3}=4/3$. $\mathbb E[\mathcal D^\prime]=\frac{1+4+5}{3}=\frac{(-1+2)+(2+2)+(3+2)}{3} = \frac{-1+2+3}{3} +\frac{2+2+2}{3} = \mathbb E[\mathcal D] + 2$ 上述结论就像我们在概率论课程中学习到的一样，对随机变量$X$，我们有这个结论：$\mathbb E [X + c] = \mathbb E[X] +c$. 之所以一致，是因为就像上面讲的一样：1-D 数据集本身就可以看做一个随机变量$X$. Example 2: scale all the data points in the dataset, i.e. stretch the dataset, same meaning. $\mathcal D^{\prime\prime} = \lbrace -2,4,6 \rbrace = 2\mathcal D$. $\mathbb E[\mathcal D^{\prime\prime}]= \frac{(-1)\times2 + 2\times 2 + 3\times 2}{3}=\frac{(-1+2+3)\times2}{3}=\frac{-1+2+3}{3}\times2 =2 \mathbb E[\mathcal D]$. 总结$$\color{blue}{\mathbb E[\alpha\mathcal D +\beta] = \alpha\mathbb E[\mathcal D] + \beta}$$其中, $\alpha,\beta​$ 都是常数(constant number.) Effect on VarianceExample 3 (1-D dataset): shit the dataset. $\mathcal D = \lbrace -1,2,3 \rbrace$, $\mathcal D^\prime = \lbrace 1,4,5\rbrace = \mathcal D + 2$ 现在我们计算 $\mathcal D^\prime$ 的variance：$$\begin{align}&amp;var[\mathcal D^\prime] = \mathbb E[(\mathcal D^\prime - \mu_{\mathcal D^\prime})^2]\\\text{we have known that: } &amp;\mathcal D^\prime = \mathcal D +2 , \mu_{\mathcal D^\prime}= \mu_{\mathcal D} +2 \\\text{So: } &amp;\mathcal D^\prime - \mu_{\mathcal D^\prime} = \mathcal D - \mu_{\mathcal D}\\\Rightarrow &amp;var[\mathcal D^\prime] =\mathbb E[(\mathcal D - \mu_{\mathcal D})^2]= var[\mathcal D]\end{align}$$Example 4 (1-D dataset): scale the dataset. $\mathcal D^{\prime\prime} = 2\mathcal D$.$$\begin{align}var[\mathcal D^{\prime\prime}] &amp;= \mathbb E[(\mathcal D^{\prime\prime} - \mu_{\mathcal D^{\prime\prime}})^2]\\&amp;= \mathbb E[(2\mathcal D - 2\mu_{\mathcal D})^2]\\&amp;= \mathbb E[2^2(\mathcal D - \mu_{\mathcal D})^2]\\&amp;= \mathbb 2^2E[(\mathcal D - \mu_{\mathcal D})^2]\\&amp;= 2^2var[\mathcal D]\end{align}$$总结：$$\color{blue}{var[\alpha\mathcal D+\beta]= \alpha^2 var[\mathcal D]}.$$ Effect on CovarianceCovariance 只是针对多维数据，i.e. 随机向量。 $\mathcal D = \lbrace \mathbf x_1,…,\mathbf x_m \rbrace$, where $\mathbf x_i \in \mathbb R^n$. (回忆 data matrix中的结构，$\mathbf x_i$ 对应第 $i$ 行) $\mathcal D$ 对应随机向量 $\mathbf X$. $\mathcal D$ 既代表dataset，又代表随机向量，具体什么含义取决于context。 结论：$$\begin{align}\color{blue}{\mathbb E[\mathbf A\mathcal D + \mathbf b] = \mathbf A \mathbb E[\mathcal D]+\mathbf b} \\\color{blue}{var[\mathbf A\mathcal D + \mathbf b] =\mathbf A var[\mathcal D]\mathbf A^T}\end{align}$$我们现在证明上述结论： 一些前提结论$$\begin{align}\mathbb E[\mathcal D] = \mathbf \mu = \frac{1}{m}\sum_{i=1}^m \mathbf x_i \in \mathbb R^n\end{align}$$Proof 1$$\begin{align}\color{blue}{\mathbb E[\mathbf A \mathcal D + \mathbf b] } &amp;= \frac{1}{m}\sum_{i=1}^m ( \mathbf A \mathbf x_i +\mathbf b)\\&amp;=\frac{1}{m}\sum_{i=1}^m ( \mathbf A \mathbf x_i ) +\mathbf b \\&amp;=\mathbf A \frac{1}{m}\sum_{i=1}^m \mathbf x_i + \mathbf b\\&amp;=\color{blue}{\mathbf A \mathbb E[\mathcal D] +\mathbf b }\\&amp;=\color{blue}{\mathbf A \mathbf \mu + \mathbf b }\end{align}$$Proof 1-1$$\begin{align}\mathbb E[\mathcal D^T]&amp; = \frac{1}{m}\sum_{i=1}^m \mathbf x_i^T\\&amp;=[( \frac{1}{m}\sum_{i=1}^m \mathbf x_i^T)^T]^T\\&amp;= (\frac{1}{m}\sum_{i=1}^m \mathbf x_i)^T\\&amp;= \mathbb E[\mathcal D]^T\end{align}$$Proof 1-2$$\begin{align}\mathbb E[\mathcal D \mathbf A ] &amp;= \frac{1}{m}\sum_{i=1}^m \mathbf x_i\mathbf A \\&amp;= (\frac{1}{m}\sum_{i=1}^m \mathbf x_i)\mathbf A\\&amp;= \mathbb E[\mathcal D]\mathbf A\end{align}$$Proof 2$$\begin{align}\color{blue}{var[\mathbf A\mathcal D +\mathbf b]} &amp;= \mathbb E[(\mathbf A\mathcal D +\mathbf b-\mathbb E[\mathbf A\mathcal D +\mathbf b])(\mathbf A\mathcal D +\mathbf b - \mathbb E[\mathbf A\mathcal D +\mathbf b])]\\&amp;= \mathbb E[(\mathbf A\mathcal D - \mathbf A\mu)((\mathbf A\mathcal D - \mathbf A\mu)^T]\\&amp;= \mathbb E[\mathbf A(\mathcal D - \mu)(\mathcal D -\mu)^T\mathbf A^T]\\&amp;=\mathbf A \mathbb E[(\mathcal D - \mu)(\mathcal D - \mu)^T] \mathbf A\\&amp;=\color{blue}{\mathbf A var[\mathcal D]\mathbf A^T}\end{align}$$ 半正定Covariance of a random vector is always postitive semi-definite. Proof: Let $\mathbf X \in \mathbb R^n$ is a random vector, for any vector $a \in \mathbb R^{1\times n}$,$$\begin{align}\mathbf a Var[\mathbf X] \mathbf a^T &amp;= Var[\mathbf a \mathbf X] \text{(by the multiplication of constant matrix property.)}\\&amp;\ge 0 \text{(follow the fact that variance is always positive.)}\end{align}$$ 练习一个在线练习用以熟悉本节课所讲的概念，link. Covariance between two linear transformation$a,b$ 是两个常向量 $\in \mathbb R^{1\times n}$, $\mathbf X \in \mathbb R^n $ 是一个随机向量。Then, the covariance between the two linear transformations $a\mathbf X$ and $b \mathbf X$ can be expressed as a function of the covariance matrix:$$\color{blue}{cov[\mathbf a\mathbf X, \mathbf b\mathbf X] = \mathbf a Var[\mathbf X]\mathbf b^T}$$Proof：$$\begin{align}cov[\mathbf a\mathbf X, \mathbf b\mathbf X] &amp;= \color{blue}{\mathbb E[ (\mathbf a \mathbf X - \mathbb E[\mathbf a \mathbf X]) (\mathbf b \mathbf X - \mathbb E[\mathbf b \mathbf X]) ] } \text{(by definition of covariance.)} \\&amp;= \mathbb E[\mathbf a(\mathbf X - \mathbb E[\mathbf X]) \mathbf b(\mathbf X - \mathbb E[\mathbf X]) ]\\&amp;= \mathbb E[\mathbf a(\mathbf X - \mathbb E[\mathbf X]) (\mathbf b(\mathbf X - \mathbb E[\mathbf X]))^T ] \text{(transpose of a scalar is equal to itself.)}\\&amp;= \mathbb E[\mathbf a(\mathbf X - \mathbb E[\mathbf X]) (\mathbf X - \mathbb E[\mathbf X])^T\mathbf b^T ]\\&amp;=\mathbf a \mathbb E[(\mathbf X - \mathbb E[\mathbf X]) (\mathbf X - \mathbb E[\mathbf X])^T]\mathbf b^T \text{(by the linearity of expection.)}\\&amp;= \color{blue}{ \mathbf a Var[\mathbf X]\mathbf b^T}\end{align}$$ 一句话总结 Shit the dataset: 改变mean value, 不改变 variance Scale the dataset: 两者都改变。]]></content>
      <categories>
        <category>课程笔记</category>
        <category>Mathematics for Machine Learning</category>
        <category>PCA</category>
      </categories>
      <tags>
        <tag>Statistics</tag>
        <tag>Mean</tag>
        <tag>Variance</tag>
        <tag>Linear Transformation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[六.回归]]></title>
    <url>%2Farchive%2F2018-07-17%2FRegression%2F</url>
    <content type="text"><![CDATA[简介 In order to optimize the fitting parameters of a fitting function to the best fit for some data, we need a way to define how good our fit is. This goodness of fit is called chi-squared, which we’ll first apply to fitting a straight line - linear regression. Then we’ll look at how to optimise our fitting function using chi-squared in the general case using the gradient descent method. Finally, we’ll look at how to do this easily in Python in just a few lines of code, which will wrap up the course. 学习目标 Describe regression as minimization of errors problem. Distinguish appropriate from inappropriate models for particular data sets Create code to fit a non-linear function to data using gradient descent 线性回归 这节课是从现实中的数据出发，用函数拟合数据。这和我们之前讲到的优化方法有一点不同：在本节课中，模型的参数是我们真正关心的！因为从模型的角度来说，参数还是参数；但我们要通过损失函数优化参数的时候（这个过程才是核心！），损失函数才是是我们的研究对象，而非回归函数。因此，回归函数中的参数如: $\omega , b$ 对于损失函数来说是自变量，是我们要优化的对象。 这一点要切记，很多同学对这一点比较模糊的原因就是没有做好模型参数的 “角色转换”。 数据集 $\mathbf x_1$ $\mathbf x_2$ $\cdots$ $\mathbf x_n$ Label $y_1$ 一个关于线性回归的在线练习，link. 非线性回归通常情况下，我们可能会遇到如下问题： 无法显示地写出梯度的表达式(不管什么原因) 写出了梯度的表达式，却无法用解析方法求解（e.g. 令梯度为0，求解出合适的参数取值。） 此时，就应该采用 gradient descent 算法通过迭代更新自变量的方式，逐步达到目标函数的最小值。 Example： 梯度下降编程练习，link.]]></content>
      <categories>
        <category>课程笔记</category>
        <category>Mathematics for Machine Learning</category>
        <category>Multivariate Calculus</category>
      </categories>
      <tags>
        <tag>Gradient Descent</tag>
        <tag>Linear Regression</tag>
        <tag>Least Squared</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[五.优化基础]]></title>
    <url>%2Farchive%2F2018-07-14%2FIntroduction-to-optimization%2F</url>
    <content type="text"><![CDATA[简介 If we want to find the minimum and maximum points of a function then we can use multivariate calculus to do this, say to optimise the parameters (the space) of a function to fit some data. First we’ll do this in one dimension and use the gradient to give us estimates of where the zero points of that function are, and then iterate in the Newton-Raphson method. Then we’ll extend the idea to multiple dimensions by finding the gradient vector, Grad, which is the vector of the Jacobian. This will then let us find our way to the minima and maxima in what is called the gradient descent method. We’ll then take a moment to use Grad to find the minima and maxima along a constraint in the space, which is the Lagrange multipliers method. 学习目标 掌握优化的原理 用多元微积分实现优化 分析一些算法失效的情况–没有返回最优解的情况 用拉格朗日乘数法解决受约束的 Gradient Descent问题。 Fitting as Optimization Problem Optimization 的目的是：Find the specific variables $\mathbf{x} = \begin{bmatrix}x_1\\x_2\\ \vdots\\x_n \end{bmatrix}$ , let $f(\mathbf{x})=f_{min}$ or $f(\mathbf{x})=f_{max}$. 为了描述统一下面只说 minimize function $f$ 的情况，maximize的情况是完全类似的。 Optimization 的过程是：Starting at a specific point $\mathbf{x}_0$, Iteratively update the variable using the following process, $$\color{blue}{\mathbf x_{i+1} := \mathbf x_{i} +\Delta \mathbf x} \text{ , where } i=0,1,…,n \\\color{blue}{f(\mathbf x_n) = f_{min}}$$ 各种优化算法的区别往往在于如何确定 $\color{blue}{\Delta \mathbf{x}}$，比如对下文即将要讲到的两种优化算法而言： Optimization Algorithm $\Delta \mathbf{x}$ 自变量变化的方向（i.e. $\Delta \mathbf{x}$ 的方向） Newton Method $-\frac{f(x)}{\color{blue}{f^\prime(x)}}$ 导数的反方向 Gradient Descent $- \alpha \color{blue}{\nabla_{\mathbf{x}}f}$ 梯度的反方向 可以看出二者对 $\Delta \mathbf{x}$ 的选取是不一样的。 按道理讲，因为$\Delta \mathbf{x}$ 是向量，因此不同的优化算法选取$\Delta \mathbf{x}$ 的这个所谓“不一样” 应该包括： 方向不一样 大小不一样 （i.e. $\Vert \Delta \mathbf{x}\Vert$ 不一样） 但实际上，我们仔细看上面表格的话会发现：牛顿法和梯度下降法选择的 $\Delta \mathbf{x}$ 仅仅是大小不一样，方向是一样的：都是梯度（导数）的反方向。 这个现象不是偶然的，在下面的内容中会讲到。 Linear Approximation / Newton MethodNewton Method is used to solve the root of function. 问题：存在一个函数 $f(x)$, 已知 $f(x)=0$ 有解，求$x=?$ 核心思想： 我们虽然无法直接找到方程的解 $\hat x$, 但是我们可以先选择一个相对“靠谱”的点 $x_0$, 基于$x_0$ 再选择出一个$x_1$, 不断迭代这个过程，最后我们选择的$x_n$ 就与真正的解 $\hat x$ 比较接近了。 “靠谱”的意思是，$f(x_0)$ 的值与 $f(\hat x)$ 差别不要过大，即：$x_0$ 选在 $\hat x$ 附近。 核心思想之下，可以从两个角度来思考牛顿法的原理。 一. 从 Linear Approximation 的角度 我们在某个点 $x=x_0$ 处 linearize 函数 $f(x)$ 的时候，我们可以问一个问题：$x_0$ 附近（a short distance way）的点 $x_0 + \Delta x$ 处的函数值是多少, i.e. $f(x_0 + \Delta x)=?$ 根据我们之前学习的知识我们知道，在一个点处linear approximation of function 可以通过一阶 Taylor serise解决：$$f(x_0 + \Delta x) \approx f(x_0) + f^\prime(x_0)\Delta x$$ 实际上 linear approximation of function 可以并不依赖于Taylor series 的相关知识，我们可以直接从导数的定义公式推导出来： $f^\prime(x_0) = \lim_{\Delta x \to 0} \frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x} \Rightarrow$ 即在$x_0$ 附近不远的范围有: $f^\prime(x_0) \approx \frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x} \Rightarrow$ $f(x_0 + \Delta x) \approx f(x_0) + f^\prime(x_0)\Delta x$. 与 Taylor series得出的结论是一致的。 现在，我们选择 $x_0$ 作为我们的初始点。根据牛顿法的思想，如果现在我们假设函数在$x_0$ 点附近有0值，我们可以re-arrange to find how far away, i.e. 我们现在转换一下问题：这个使 $f(x)=0$ 的点$\hat x$ 距离$x_0$ 的距离多远？当然，由于我们采用了first order approximation, 我们最多只能得到真实距离的近似值。 为什么将问题转换为寻找$\hat x$ 距 $x_0$ 的距离呢？这是因为，如果我们能找到这个距离，我们把最初选择的点按照这个距离移动不就更加接近真实的 $\hat x$ 了吗？这个距离不就是 这就得到：$f(x_0 + \Delta x)=0$, 进一步：$$\Delta x \approx -\frac{f(x_0)}{f^\prime(x_0)}$$这个 “距离的近似值” 我们得到了，我们就可以得到$x_0 +\Delta x$，记为 $x_1$, 这个点就比 $x_0$ 更接近真实的解 $\hat x$ 了。应为是近似距离，只移动一次是不够的，因此我们要迭代地进行上述过程。 迭代次数$n$可以趋于无穷大，因此在下面的公式中就是用等号而不用约等于了，如下：$$x_{i+1} = x_i + \Delta x =x_i - \frac{f(x_i)}{f^\prime(x_i)}, i = 0,1,…,n$$我们迭代 $n$ 次，当$\vert f(x_n )\vert &lt; \varepsilon$ 的时候（$\varepsilon$ 是最够小的正数），我们就认为 $x_n$ 是 $\hat x$ 的合理近似值。此时，我们称为Newton Methond Converges. 二. 从几何结构 上图中，$x_0$处的蓝色切线与 $x-axis$ 的截距为记为 $x_1$， 夹角为$\theta$. 那么有，根据几何及函数在点处导数的关系可知，$$\tan(\theta) = \frac{f(x_0)}{x_1 - x_0} =f^\prime (x_0)$$即：$$x_1 = x_0 - \frac{f(x_0)}{f^\prime(x_0)}$$我们把这个过程迭代$n$ 次，可得$$x_{i+1} = x_i - \frac{f(x_i)}{f^\prime(x_i)}$$与上面从 Linear Approximation of function 的角度得到的结果是一致的。 此外，Newton Method 虽然和 Linear Approximation of function 有密切关系，但是也有本质区别： Approximation 是在自变量已知的情况下，对函数在该点附近做近似。 Newton Method 刚好反过来，是在函数值已知情况下$f(x) = 0$, 求满足条件的自变量的取值。 Abnormality of Newton Method一. 拐点附近 我们知道$\Delta x \approx -\frac{f(x)}{f^\prime(x)}$ , 在拐点附近$f^\prime(x)$ 会很小， step 会变得很大，会导致收敛变得比较慢；甚至会导致无法收敛，i.e. diverge 二. 震荡 有些 starting point 会导致Newton Methond 既不收敛同时也不发散，而是震荡。此时，迭代过程是一种会陷入一种死循环。如下图， *梯度下降算法 $\color{red}{\text{WoW! This is the most promising algorithm in this course.}}$ Gradient descent 核心思想与Newton Method 有点类似：迭代地移动自变量$\mathbf{x} \rightarrow \mathbf{x + \Delta x}$, 使函数值逐渐变小。区别在于： 只不过gradient descent 对于Multivariate function 也适用；Newton method 只用于单变量函数。 Newton method 是知道函数值求解自变量；Gradient descent 并不知道函数的具体取值，而是不断地通过改变自变量来获得更小的函数值。 Gradient以上图中函数 $f(x,y)=x^2y$ 为例, 函数的梯度定义为如下形式：$$\text{Grad}f=\frac{\partial f}{\partial\mathbf{x}}= \nabla_{\mathbf{x}}f = \begin{bmatrix}\frac{ \partial f}{\partial x} \\ \frac{\partial f}{\partial y} \end{bmatrix} = \frac{\partial f}{\partial x}\begin{bmatrix} 1\\0 \end{bmatrix} + \frac{\partial f}{\partial y}\begin{bmatrix}0\\1 \end{bmatrix} =\frac{\partial f}{\partial x}\vec i + \frac{\partial f}{\partial y}\vec j$$ n 元实值函数的梯度 is a vector with n components. Each component of gradient is a partial derivative of function $f$ with respect to a corresponding variable. In this example, say $x,y$. Each component of a gradient vector(i.e. Grad) 表示函数沿着那个分量方向的变化速度。 Gradient is a collection of the derivatives of function with repsect to all variables. Directional derivative对一个函数, e.g. $f(x,y) = x^2y$, the partial derivative with respect to $ x$ gives the rate of change of f in the $x$ direction, the partial dericate with respect to $y$ gives the rate of change of f in the $y$ direction; Gradient gives a overall indication about the rate of change of $f$. How do we compute the rate of change of f in arbitrary direction? Defition:The rate of change of a multivarite function in the direction $\mathbf{u}$ is called the directional derivative in the direction $\mathbf{u}$. Here $\mathbf{u}$ is assumed to be a unit vector. Assuming that:$$\begin{align}f = f(x,y) \\\mathbf{u}=\begin{bmatrix}u_1\\u_2\end{bmatrix}\end{align}$$ 那么，我们有：$$\color{blue}{D_{\mathbf{u}}f } = \color{orange}{\nabla_{\mathbf{x}}f } \cdot \color{green}{\mathbf{u}} = \color{orange}{\frac{\partial f}{\partial x}} \color{green}{u_1} + \color{orange}{\frac{\partial f}{\partial y}} \color{green}{u_2}$$因此，方向导数是梯度与$\mathbf{u}$ 的点积。 注意： 如果 $\mathbf{u}$ 是$x$轴方向的单位向量，i.e. $\mathbf{u}=\begin{bmatrix}1\\0 \end{bmatrix}$, 那么方向导数就是函数对$x$ 偏导数, i.e. $D_u f = \frac{\partial f}{\partial x}$. 一般情况 一般情况下，the directional derivative is the linear combination of all the partial derivaties. Example: $\mathbf{u} = \begin{bmatrix}2\\1 \end{bmatrix}$, $f(x,y) = x^2y$ . 求函数$f$ 在$(2,3)$ 点, 在 $\mathbf{u}$ 方向的导数？ $D_{\mathbf{u}}f = \nabla_{x}f \cdot \mathbf{u} =\begin{bmatrix}12\\4 \end{bmatrix} \cdot \begin{bmatrix}2\\1 \end{bmatrix}=28$. Directions of Greatest Increase and Decrease根据前面的知识，我们可以把方向导数可以写成下面的形式， $D_{\mathbf{u}}f = \nabla_{\mathbf{x}}f \cdot \mathbf{u} = \Vert \nabla_{\mathbf{x}}f \Vert \Vert \mathbf{u} \Vert cos\theta$ $\theta$ 是gradient vector 与方向向量 $\mathbf{u}$ 之间的夹角。 那么我们有如下结论： 当 $\theta = 0$ ，方向导数 $D_{\mathbf{u}}$ 取得其最大的正值。Hence, the direction of greatest increase $f$ is the same as the direction of gradient vector. 当 $\theta = \pi$ ，方向导数 $D_{\mathbf{u}}$ 取得其最大的负值。Hence, the direction of greatest decrease $f$ is the direction opposite to gradient vector. Ok, 实际上，以上结论也就是 Gradient descent 算法的思想来源和理论基础。 根据上述结论，我们可以给梯度一个描述性的定义， 梯度是这样一个向量：1. 沿着它的方向函数增加最快；沿着它的反方向函数减小最快。2. 梯度的size(norm) 表明了函数增加的速率（快慢）。 偏导、梯度和方向导数对于 n 元实值函数 $f=f(x_1,x_2,…,x_n)$, 概念 记号 含义 表示 Partial derivative $\frac{\partial f}{\partial x_i}$ 函数在 $x_i$ 方向的变化速度；绝对值代表变化率，正、负表示方向 scalar Gradient $\nabla_{\mathbf{x}}f$ , $\frac{\partial f}{\partial \mathbf{x}}$ 是所有偏导数集合而成的向量, 因此它包含函数在各个自变量方向的变化速度；而且gradient 的方向是函数增加最快的方向；norm of gradient 表明函数变化速率（大小） vecotr Directional derivative $D_u f$ 函数在 $\mathbf{u}$ 方向的变化速度，绝对值代表变化率，正、负表示方向 scalar 梯度下降的过程 前面详细地分析了gradient 的定义、计算机器含义，以及函数值变化与gradient 的关系。 现在的问题是：如果我们需要 minimize 函数$f(\mathbf{x})$, 如何做？ Gradient descent：不断让自变量沿着梯度的反方向update ，那么同时函数值自然不断地减小，最终收敛于一个局部最小值；或者最后停在我们认为满意的地方（达到一定迭代次数）。（当然也可能是全局最小）。 Gradient Descent Process of Minimization: 选择一个起始点 $\mathbf x_0$, and compue the gradient of $f(\mathbf x)$ at $\mathbf x_0$: $\nabla_{\mathbf x}f(\mathbf x)|_{\mathbf x_0}$. For $i=0,.., n$ do: $\mathbf x_{i+1}:= \mathbf x_i -\alpha \nabla_{\mathbf x}f(\mathbf x)|_{\mathbf x_i}$ return $\mathbf x_{n+1}$. Then, $f_{min}=f(\mathbf x_{n+1})$. 遗留的问题 初识点如何选取？ 如何选取 learning rate $\alpha$. 过小，收敛太慢 过大，有无法收敛的风险 更加细致的观察 gradient，Does it really works? 观察如下两幅图，对助于对GD更细节地理解。 这里有一个练习，用来巩固对梯度下降算法的理解，link. Lagrange Multipliers之前讲的优化问题没有对自变量做任何约束，因此自变量的取值范围是：整个定义域。但现实中，很多情况下的优化问题是对自变量有约束的条件下，因此需要有方法将约束考虑在内。朗格朗日乘数法就是解决在自变量约束条件下的优化问题。 优化问题只从公式来看不具备几何的直观性，难以从感官上理解优化的过程。Gradient 和 Contour，无论多元函数的优化，亦或是其可视化来讲，都是非常重要的。下面给出两个函数的梯度和等高线，我们从几何上直观地感受一下。 一. $f(x,y)=xe^{-(x^2+y^2)}$ 二.$f(x,y)=x^2y$ Constrained Optimization (simple)General Case: Exist function $f(\mathbf x)$, s.t. $g(\mathbf x) = c$, where $c$ is constant. Optimization: Find the specific $\mathbf {\hat x}$ which make $f(\mathbf {\hat x})=f_{min}$ or $f_{max}$. Example : $f(x,y)=x^2y$, s.t. $x^2+y^2=r^2$ where $r&gt;0$ and is constant. Optimization: Find $(\hat x,\hat y)$ which make $f(\hat x,\hat y)=f_{min}$or $f_{max}$. 分析： $x^2+y^2=r$ 在x-o-y平面内是一个以原点为圆心，半径为$r$ 的圆。我们记：$g(x,y)=x^2+y^2$, 那个半径为 $r$ 的圆就是 a specific contour of the contours of function $g(x,y)$. 受限优化含义就是函数$f(x,y)$ 上所有的点中，有一部分投影到x-o-y 平面时会落在 $x^2+y^2=r$ 这个圆上。在这部分点中（这就是contrained 的意思），寻找$(\hat x,\hat y)$ 使得函数取得最大或最小值。用专业的话说，即：When projection onto x-o-y plane of contours of function $f(x,y)$ tangent to the contour of $g(x,y)$ at $(\hat x,\hat y)$, would maximize or minimize the function $f(x,y)$. Where $f(\hat x,\hat y)=f_{max}$ or $f_{min}$. 如下图所示， 根据上面知识，我们已知： 两个 contour 相切 gradient 与contour 垂直 那么我们就能得出结论： gradient of $f(x,y)$ parallel to gradient of $g(x,y)$, not equal, at least proportional! i.e.:$$\color{blue}{\nabla_{\mathbf x}f(\mathbf x) = \lambda \nabla_{\mathbf x}g(\mathbf x)} \\\text{where } \mathbf x =\begin{bmatrix}x\\y \end{bmatrix} \text{,and }\lambda \in \mathbb R \text{ which is called lagrange multiplier.}$$根据上述结论，我们将 $f(x,y)=x^2y$ $g(x,y)=x^2+y^2$ $g(x,y)=r^2$ 这些已知条件带入，可得：$$\begin{align}\begin{bmatrix} 2xy \\ x^2 \end{bmatrix}&amp;=\lambda \begin{bmatrix}2x\\ 2y \end{bmatrix} \\x^2 + y^2 &amp;= r^2\end{align}$$上述公式有三个未知数$x,y,\lambda$, 有三个方程。解方程即得：$$\begin{cases}\hat x = \pm \frac{2}{\sqrt 3}r \\\hat y = \pm \frac{1}{\sqrt 3}r \\\lambda = \pm \frac{1}{\sqrt 3}r\end{cases}\\\color{purple}{\text{ we don’t care }\lambda \text{ here, } }\\\color{purple}{\text{becasue we just want to find the proper points or point } (\hat x,\hat y) \text{ to optimize out function }f(x,y).}$$因此，我们得到四个点分别为，其中两个会让函数 $f(x,y)$ 取最大值；另外两个点处 $f(x,y)$ 取最小值。 反思本节课后半部分我们主要分析如何用 Lagrange Multipliers 解决 Constrained optimization 的问题。 那么， Lagrange Multipliers的核心思想是什么呢？ 我个人有两种理解方式， 我么找到函数$f(x,y)$ 的梯度: $\nabla_{\mathbf x}f(\mathbf x)$, 也找到constrained function $g(x,y)$ 的梯度 $\nabla_{\mathbf x}g(\mathbf x)$, 根据 两个函数的contour 相切点，是$f(x,y)$最大or最小值点, 因此让：$$\nabla_{\mathbf x}f(\mathbf x) = \lambda \nabla_{\mathbf x} g(\mathbf x)$$从而求得: $\mathbf {\hat x}$. Lagrange 通过 $\lambda$ 将constrained equation 与 original objective function $f(x,y)$ 结合起来形成一个新的 objective function: $\mathcal L (\mathbf x,\lambda)$. 把它当作 unconstrained function 去优化它: $$\begin{align}&amp;\color{blue}{\mathcal L(\mathbf x,\lambda) = f(\mathbf x) - \lambda (g(\mathbf x)-c)} \\&amp;\color{blue}{\text{Let: }\nabla \mathcal L(\mathbf x, \lambda) = 0 } \Rightarrow \begin{bmatrix}\frac{\partial f}{x_1} - \lambda \frac{\partial g}{\partial x_1} \\\frac{\partial f}{x_2} - \lambda \frac{\partial g}{\partial x_2} \\\vdots \\\frac{\partial f}{x_n} - \lambda \frac{\partial g}{\partial x_n} \\\color{purple}{ -g(\mathbf x)}\end{bmatrix}= 0\end{align}$$ 求解上述方程，即得：$\mathbf {\hat x}$. (这里，我们同样不关心 $\lambda$.) 关于Constrained optimization 有几个很好的视频资源：link1, link2. 一个在线练习，link.]]></content>
      <categories>
        <category>课程笔记</category>
        <category>Mathematics for Machine Learning</category>
        <category>Multivariate Calculus</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
        <tag>Gradient Descent</tag>
        <tag>Newton Method</tag>
        <tag>Parameters Space</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[四.泰勒级数和线性化]]></title>
    <url>%2Farchive%2F2018-07-12%2FTaylor-series-and-linearisation%2F</url>
    <content type="text"><![CDATA[学习目标： 掌握幂级数对函数的近似 理解线性化的意义及关联 学会多元函数的近似表示方法 Taylor Series for approximationsTaylor级数的核心思想是： 对于一个复杂函数$f(x)$, 我们希望用若干其他简单函数的组合去近似$f(x)$, 并且我们的目标函数$f(x)$ 是behave good（e.g. 连续的）的时候，这种方法一定是可行的。指导我们approximate $f(x)$ 的原则如下： 选择函数$f(x)$ 的某一点$(x_0,y_0)$ 作为切入，并且要保证在该点处： $g(x_0)=f(x_0)$ $g^\prime (x_0)=f^\prime(x_0)$ $g^{\prime\prime}(x_0) =f^{\prime\prime}(x_0) $ … 换句话说，我用来approximate $f(x)$ 的函数需要在事先选好的某一点处：与$f(x)$ 函数值相同，一阶导数相同，二阶导数相同….n 阶导数相同….. 最终我们的$g(x)$ 可以无限接近$f(x)$。 上面讲到，我们选择$f(x)$ 上的某一点处切入，在满足若干要求之后得到$g(x)$ 可以无限接近$f(x)$. 这就意味着我们通过知道函数上某一点的所有信息（e.g. 函数值，一阶导数，二阶导数,….,n阶导数），就可以获得整个函数$f(x)$ 的全部信息。这是非常令人吃惊的现实，函数$f(x)$ 上的每一个点竟然蕴含了该函数所有的信息，这不能不令人惊叹！这有点像中国有句古语：管中窥豹，可见一斑。 因此，使用Taylor series approximate 函数$f(x)$有另外一种称呼：在一点处将函数$f(x)$ Taylor展开。 简言之，Taylor serise 是用函数的局部来描述整体，而且可以描述的任意接近。 当然，Taylor series 还有一些其他的细节问题，但是上面的内容就是Taylor series 的核心了。 Power series derivationPower series 是Taylor series 的一种特例，其核心思想是： You can use a series of increasing power of x to re-express functions. 如下图所示， 我们下面推导函数的幂级数表示： 幂函数的集合为： ${1=x^0,x,x^2,x^3,…,x^n,…}$, 我们需要找到它们的线性组合权重系数，使得组合的结果接近$f(x)$. 我们令： $g(x) = a_0 + a_1x + a_2x^2 + a_3x^3+….a_nx^n+…$ 我们需要找到这些权重系数：${a_0,a_1,…,a_n }$ 我们选择在$(x=0,f(0))$ 这一点做切入点，那么依照上小节的结论我们有：$$\begin{align}g(0) &amp;= f(0) \Rightarrow a_0 = f(0) \\g^\prime(0) &amp;= f^\prime(0) \Rightarrow a_1 = f^\prime(0) \\g^{\prime\prime}(0) &amp;= f^{\prime\prime}(0) \Rightarrow a_2 = \frac{f^{\prime\prime}(0)}{2} \\g^{(3)}(0) &amp;= f^{(3)}(0) \Rightarrow a_3 = \frac{f^{(3)}(0)}{6} \\\cdots \\g^{(n)}(0) &amp;= f^{(n)}(0) \Rightarrow a_n = \frac{f^{(n)}(0)}{n!}\\\cdots \\\end{align}$$我们得到了所有我们想要的权重系数，并且只用函数在$x = 0$ 这一点处的全部信息。此时，我们就知道了函数$f(x)$ 的全部， 即：$$\begin{align}&amp;\color{blue}{ f(x) = g(x) = f(0) + f^\prime(0)x + \frac{f^{\prime\prime}(0)}{2} x^2 + \cdots + \frac{f^{(n)}(0)}{n!}x^n + \cdots= \sum_{n=0}^{\infty} \frac{f^{(n)}(0)}{n!} x^n }\\\end{align}$$PEND. 在上图中，我们会明显地发现当高阶term不断加入的时候，之前已经有的低阶项会保持不变，高阶项的加入扩大了接近的范围。 上面我们在$x=0$ 点处将函数$f(x)$ Taylor 展开，这种特例叫做：Maclaurin Expansion。 上述Taylor展开是针对一元函数$f(x)$进行的，这种思想可以扩展到n元函数的情况。届时我们就不是approximate 曲线，而是appriximate hyper surfaces。 Example: 根据 Maclaurin series 我们可得：$$e^x = g(x) = 1 + x + \frac{1}{2}x^2 + \frac{1}{3!}x^3 + \cdots + \frac{1}{n!}x^n+\cdots$$我们从这里也可以发现，对$g(x)$ 求导之后：$$g^\prime(x) = g(x) , i.e. (e^x)^\prime = e^x$$即：$e^x$ 的各阶导数等于 $e^x$. PS: 这里有一个可视化的例子用来帮助理解 Taylor Series: link Power series Details在$x=0$ 点处用来reconstruct or re-express $f(x)$ 的series 我们称之为： Maclaurin serise. 实际上，我们可以选择在任何一点处对函数做Taylor 展开- 用Taylor series approximate 函数。 对比图如下， 上半部分是Maclaurin 展开，下半部分是在$x=p$ 处 Taylor展开。 在P点：$(p,f(p))$ 即 $x=p$ 处用 Taylor series approximate 函数$f(x)$ （把函数在P点处Taylor泰勒展开） 通用的 Taylor Series 为：$$\color{blue}{f(x) = g(x) = f(p) + f^\prime(p)(x-p) + \frac{f^{\prime\prime}(p)}{2!}(x-p)^2 \cdots+\frac{f^{(n)}(p)}{n!}(x-p)^n \cdots = \sum_{n=0}^{\infty} \frac{f^{(n)}(p)}{n!}(x-p)^n}$$推导过程与Maclaurin Series类似，这里不展开讲。 区别在于 $x \rightarrow (x-p)$, 实际上相当于在Maclaurin serise 基础上对整个$g(x)$ 沿着 $x$ 轴做了一次平移: 平移量 = $p$. 在$x=0$ 处 我们需要对$\lbrace1=x^0,x^1,x^2,…,x^n,..\rbrace$ 进行组合来approximate $f(x)$. 在$x=p$ 处 我们需要对 $\lbrace1=(x-p)^0,(x-p)^1,(x-p)^2,…, (x-p)^n,… \rbrace$ 进行组合来approximate $f(x)$. 以上两种情况我们都用了统一的原则去求解那些权重系数：$a_0,a_1,…,a_n,…$.且 $a_n = \frac{f^{(n)}(p)}{n!}$. PS: 所谓统一的原则就是第一小节讲到的 “$g(x)$ 与 $f(x)$ 在$x=p$ 处：函数值相等，一阶导数相等，二阶导数相等,…，n阶导数相等，…”. Taylor Series 将函数 $f(x)$ 重构成幂函数的线性组合, 此即所谓的linearisation. ExamplesExample 1: 将 $cos(x)$ 在$x=0$ 点展开为 Maclaurin serise. $cos(x)$ 函数有两个特点保证我们可以这样做： 连续 无穷阶可导 Example 2: 将 $\frac{1}{x}$ 在 $x=1$ 点展开为Taylor serise. $\frac{1}{x}$ 函数并不是一个“nice”的函数： 在$x=0$ 处并不连续 两个展开见下图 有以下几个特点需要注意： 随着不断增加$x$ 的次幂$n$, 近似范围越来越大。但是在近似范围之外，$g(x)$ 与 $f(x)$ 差距很大。因此无论是用Maclaurin series 还是 Taylor series ，都要注意 函数的定义域范围，超出范围的部分并不是approximation $cos(x)$ 的 Maclaurin series 展开式中，含有$x$ 的奇次幂的项都消失了，剩下的都是偶次幂项：$1,x^2,x^4…,$ . 换句话说就是这些 basis functions 也都会偶函数，和它们要approximate 的函数 $cos(x)$ 在这方面保持了一致性。 $\frac{1}{x}$ 在 approximate的过程中，$g(x)$ 直接跨越了渐近线。 $\frac{1}{x}$ 函数的 Taylor series 展开结果，我们发现只在 $x&gt;0$ 的部分 $g(x)$ 试图approximate 函数$\frac{1}{x}$, 而 $g(x)$ 对于 $x&lt;0$ 的部分, 却完全没有考虑在内。 在两个例子中，Taylor series 都表现出$g(x)$ 值 正、负交替出现的情况：这种情况下 $g(x)$ 收敛会很慢。 Multivariable Taylor SeriesLinearisation本节主要解决的问题是：我们想知道当我们用了 Taylor series 对$f(x)$ 做approximation之后，与$f(x)$ 之间的误差-error 是多少. 上图是将 $f(x)$ 在$x=p$ 点处 Taylor 展开，i.e.$$f(x) = g(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(p)}{n!} (x-p)^n$$其中，first order approximation 是绿色直线，i.e.$$\color{green}{g_1(x) = f(p) + f^\prime (p)(x-p)}$$first order approximation其实就是在$x=p$ 附近用直线来近似$f(x)$. p 附近的点$x$ 与其距离为: $x-p$，我们将这里距离记为：$\Delta p$ ,此时$x= p+\Delta p$. 如下图所示， 我们可以将 first order approximation 中的 $x$ 替换为 $p$, 如下图所示 此时一阶近似为，$$\color{green}{g_1 (p + \Delta p) = f(p) + f^\prime (p) \Delta p}$$$p$ 是我们任意选择的一点，我们把上述图和公式中的 $p$ 替换为 $x$. 替换没有改变任何概念上的事实，只是更符合大家常见的形式。如下图， 此时一阶近似为，$$\color{green}{g_1(x+\Delta x) = f(x) + f^\prime (x) \Delta x}$$这个时候，我们按照上述形式可以重写 Taylor series 公式为：$$f(x+ \Delta x) = \sum_{n=0}^\infty \frac{f^{(n)}(x)}{n!} (\Delta x)^n$$上述公式表示：在任意点$x$ 处函数$f(x)$ 的 Taylor series展开。 我们基于此来考察 我们的 first order approximation 与$f(x)$ 的误差情况。 现在我们把 “新的” Taylor series 公式展开看一下，$$f(x + \Delta x) = f(x) + f^\prime(x)\Delta x + \frac{f^{\prime\prime}(x)}{2!} \Delta x^2 + \frac{f^{(3)}(x)}{3!} \Delta x^3 + \cdots$$我们只保留一阶项，忽略二阶及以上的项并将他们看作误差：这一过程称之为 Linearisation. 如下，$$\color{orange}{f(x + \Delta x) = f(x) + f^\prime \Delta x + O(\Delta x^2)}$$换句话说，对$f(x)$ linearise 是要带误差项的， 相比于最初的不带误差项的 first order approximation：$\color{green}{g_1(x+\Delta x) = f(x) + f^\prime (x) \Delta x}$ 而言多了一个 $\color{orange}{\Delta x^2}$. 如下图所示， 通过 “新的” Taylor series 公式，我们可以得到其导数的近似表达式，$$f^\prime(x) = \frac{f(x+ \Delta x) - f(x)}{\Delta x} + O(\Delta x)$$当我们使用数值方法通过计算机计算导数或者函数值的时候，上面讲到的所有近似思想和公式有重要的意义。 这里有一个关于 Taylor series 的练习帮助理解，link Multivariate Taylor之前我们讨论的是用 Taylor serise 对一元函数 $f(x)$ 做approximation。 对于多元函数的情况 Taylor series 同样起作用。 上面的对比图显了2种 Taylor series一项一项的对比，出于简便的目的现采用如下形式的 Taylor series：$$f(x+\Delta x) = f(x) + f^\prime(x)\Delta x + \frac{f^{\prime\prime}(x)}{2}\Delta x^2 +…$$假设有2元函数 $f=f(x,y)$, 我们要回答如下问题$$f(x+\Delta x, y+\Delta y)= \color{red}{\ ?}$$考虑的如下现实原因， 多元函数二阶以上导数是 Tensor，不方便表示和计算 多元函数的二阶 Taylor series 在小范围内已经可以做到很好的approximation。 针对二元函数（or 多元函数）我们只考虑截断到二阶的 Taylor series。 Example： Gaussian function $f(x,y) = e^{-(x^2+y^2)}$ 入下图所示， 0 order approximation $\color{blue}{g_0(x+\Delta x, y+ \Delta y)}$. 如下图所示， 1 order approximation $\color{blue}{g_1(x + \Delta x, y+\Delta y)}$. 如下图所示， 2 order approximation $g_2(x + \Delta x,y+ \Delta y)$. 如下图所示， 现在我们给出 2元 Taylor series（二阶）如下图， 综上，我们有一个简洁的针对多元函数$f(\mathbf{x})$ 的二阶泰勒级数：$$\color{blue}{f(\mathbf{x+\Delta x}) = f(\mathbf{x}) + \mathbf{J}_f \mathbf{\Delta x} + \frac{1}{2} \mathbf{\Delta x}^T \mathbf{H}_f \mathbf{\Delta x} + \cdots }$$其中， $\mathbf{x}\in \mathbb{R}^n$ is vector $\mathbf{\Delta x}$ means $[\Delta x_1,\Delta x_2,\cdots,\Delta x_n]^T$. 这里有一个2个练习帮助理解： 2-D Taylor series link. 整个Taylor series 概念的考察 link.]]></content>
      <categories>
        <category>课程笔记</category>
        <category>Mathematics for Machine Learning</category>
        <category>Multivariate Calculus</category>
      </categories>
      <tags>
        <tag>Taylor Series</tag>
        <tag>Linearisation</tag>
        <tag>Approximation</tag>
        <tag>泰勒级数</tag>
        <tag>线性化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三.多元链式法则及其应用]]></title>
    <url>%2Farchive%2F2018-07-09%2FMultivariate-chain-rule-and-its-applications%2F</url>
    <content type="text"><![CDATA[学习目标 应用多元链式法则对复合函数求梯度 解释神经网络的结构和其代表的函数 解释神经网络的结构和其代表的函数 编程实现反向传播算法 多元函数链式法则（Chain Rule）上节课讲过基于 Chain rule 我们可以求出3元复合函数的全导数(参考第二课，《多元函数的导数》)。现在我们将这概念拓展到 n元函数： 存在 n元实值函数 $f(\mathbf{x})=f(x_1,x_2,…,x_n)$，而且每个自变量$x_i(i=1,…,n)$都是另外一个变量 t 的函数，即： $x_i = x_i(t)$. 现在，我们需要计算 $\frac{df}{dt}=$ ? $$\text{Gradient }=\frac{\partial f}{\partial \mathbf{x}} = \begin{bmatrix}\frac{\partial f}{\partial x_1}\\ \frac{\partial f}{\partial x_2}\\ \vdots\\ \frac{\partial f}{\partial x_n} \end{bmatrix},\text{derivatives of each component of } \mathbf{x}\text{ with respect to t is }\frac{d\mathbf{x}}{dt}=\begin{bmatrix}\frac{dx_1}{dt} \\\frac{dx_2}{dt}\\\vdots \\\frac{dx_n}{dt}\end{bmatrix}$$ 回想之前我们采用链式法则求解 total derivatives的方式，我们有： $$\begin{align}\frac{df}{dt} &amp;= \frac{\partial f}{\partial x_1}\frac{dx_1}{dt} + \frac{\partial f}{\partial x_2}\frac{dx_2}{dt} +\cdots +\frac{\partial f}{\partial x_n} \frac{dx_n}{dt} \\&amp;= \color{blue}{\frac{\partial f}{\partial \mathbf{x}}} \cdot \color{green}{\frac{d\mathbf{x}}{dt}}(\nabla_{\mathbf{x}}f(\mathbf{x})\cdot\frac{d{\mathbf{x}}}{dt})\\&amp;= \color{blue}{\mathbf{J}_f(\mathbf{x})} \color{green}{\mathbf{J_x}(t)}\end{align}$$ 说明： 对于n元实值函数来说，gradient 都是column vector！因此上式中可以使用 “点积”：$\cdot$ 对于n元实值函数来说，gradient 是 Jacobian的转置，这毋庸置疑。使用Jacobian的时候，采用的是矩阵乘法来表示计算。 可对于向量值函数来说，“gradient” 是无法定义的，这个时候只能使用Jacobian来解决。因此上式中$\frac{d\mathbf{x}}{dt}$ 其实说的是$\mathbf{J}_{\mathbf{x}}(t)$: 即 Jacobian of $\mathbf{x=x}(t)$, $\mathbf{x}$ 是向量，实际上， $$\mathbf{x=x}(t) \Leftrightarrow\begin{cases}x_1=x_1(t) \\x_2=x_2(t)\\\vdots \\x_n = x_n(t)\end{cases}$$ 上面公式中$\frac{d\mathbf{x}}{dt}$ 是处于方便理解的目的而产生的表达式，是不严格的，其语义就是：向量 $\mathbf{x}$ 的每个 component 对 $t$ 求导，然后将结果组成一个列向量；这也就是 Jacobian本身的含义。 More complex-链式法则依然有效之前我们介绍的Chain Rule 都是针对2层嵌套函数的，实际上链式法则对多层嵌套也是适用的- Chain Rule fit more than two links. e.g. there is 3 links, 3 means multiple. Example 1: 函数$f(x)=5x$, 而且 $x(u)=1-u$, $u(t) = t^2$. 这个复合函数的links 为：$f\rightarrow x \rightarrow u \rightarrow t$, 有3层links（嵌套） 且$f,x,u$ 都为实值函数。 此时我们依照之前2层嵌套的思路有如下结果： $$\frac{df}{dt}= \frac{d f}{dx}\cdot \frac{dx}{du} \cdot \frac{du}{dt} =5 \times (-1)\times 2t = -10t$$ Example 2: 存在函数 $f(\mathbf{x}(\mathbf{u}(t)))$, 其中： $f(\mathbf{x}) = f(x_1,x_2)$ $\mathbf{x}(\mathbf{u})=\begin{bmatrix}x_1(\mathbf{u}) \\ x_2(\mathbf{u}) \end{bmatrix}=\begin{bmatrix}x_1(u_1,u_2)\\ x_2(u_1,u_2) \end{bmatrix}$ $\mathbf{u}(t) = \begin{bmatrix}u_1(t) \\ u_2(t) \end{bmatrix}$ 这个函数的特点如下： 有3层links 函数$f$ 是实值函数: $\mathbf{x},\mathbf{u}$ 是vector valued function-向量值函数 这些特点如下图所示，$$f \rightarrow \begin{cases} x_1 \rightarrow \begin{cases}u_1 \rightarrow t \\ u_2 \rightarrow t \end{cases} \\ x_2 \rightarrow \begin{cases}u_1\rightarrow t \\ u_2 \rightarrow t \end{cases} \end{cases} \Leftrightarrowf \rightarrow \mathbf{x} \rightarrow \mathbf{u} \rightarrow t$$解决思路一致：$$\frac{df}{dt} =\color{blue} {\frac{\partial f}{\partial \mathbf{x}}} \color{green}{\frac{\partial \mathbf{x}}{\partial \mathbf{u}}} \color{purple}{\frac {d \mathbf{u}}{dt}}=\color{blue}{\begin{bmatrix} \frac{\partial f}{\partial x_1} &amp; \frac{\partial f}{\partial x_2}\end{bmatrix}}\color{green}{\begin{bmatrix} \frac{\partial x_1}{\partial u_1} &amp; \frac{\partial x_1}{\partial u_2}\\ \frac{\partial x_2}{\partial u_1} &amp; \frac{\partial x_2}{\partial u_2} \end{bmatrix}}\color{purple}{\begin{bmatrix}\frac{du_1}{dt} \\ \frac{du_2}{dt} \end{bmatrix}}$$遗留问题： 上式中$\frac{\partial f}{\partial \mathbf{x}}$ 怎么写成行的形式，变成 Jacobian了？ 如果用Jacobian 为什么要用 gradient 的记号？ 概念和记号上，时而用gradient时而用 Jacobian,为什么？ 原因： 用 gradient的记号是因为从视觉角度便于理解整个过程。 对于n元函数（无论 实值函数 or 向量值函数），Jacobian 都是兼容 gradient，而且更加通用。 Jacobian 相比于 gradient 表示更一般的情况- vector valued function。 原则上：“如果 gradient 够用就用gradient；如果gradient不够用则用 Jacobian” 从上述分析过程我们可以总结此类问题下链式法则通用计算方法为 ：$$\frac{df}{dt} = \color{blue}{\mathbf{J}_f(\mathbf{x})} \color{green}{\mathbf{J_x(u)}} \color{purple}{\mathbf{J_u}(t)}$$其中，乘法的意思就是 “矩阵乘法”。 总结： 对于使用Chain Rule对嵌套函数求导的时候，只要嵌套的函数中有一个是 vector-valued 的函数，那么使用Jacobian 来描述几乎是唯一的选择，因为此时我们不用考虑乘法是“数乘”，“dot product” or “matrix multiplication”, 此刻都统一为矩阵乘法；否则会陷入各种“乘法不兼容的麻烦”中去。 同时也由此可见，Jacobian 确实是比 gradient 更加通用的概念，用来表示计算过程更加方便。 神经网络（Neural Networks）先介绍一个简单的神经网络 Two Layer NN 如上图所示，这是一个简单的2层神经网络，没有中间层- hidden layer. 输入为：向量 $\mathbf{a}^{(0)}=\begin{bmatrix}a_0^{(0)}\\ a_1^{(0)} \\ a_2^{(0)} \end{bmatrix}$. 输出为：向量 $\mathbf{a}^{(1)} = \begin{bmatrix} a^{(1)} _0 \\ a^{(1)} _1 \end{bmatrix}$ Weights: 矩阵 $\mathbf{W^{(1)}}= \begin{bmatrix}\mathbf{w_0}^T\\ \mathbf{w_1}^T \end{bmatrix}\in \mathbb{R^{2\times 3}}$ Bias: 向量 $\mathbf{b^{(1)}}=\begin{bmatrix} b_0 \\ b_1 \end{bmatrix}$ 为了简化起见，我们采用Matrix-vector 的形式描述这个simple neural network:$$\mathbf{a^{(1)}} = \sigma (\mathbf{W^{(1)}} \mathbf{a^{(0)}}+ \mathbf{b^{(1)}})$$ Details of 2-layer NN上面用简洁的 matrix-vector 的形式介绍了一个简单的2层神经网络,现在补充一些细节。 一个2层的NN，其输入：n scalars 输出：m scalars，见下图， 3 Layers NN-1 hidden layer Fully Connected Feedforward NN Training NN用NN工作的基本流程如下： 设计网络结构，e.g. FCNNs, RNNs, CNNs etc. 初始化参数（weights and biases） 设计 Cost function and Optimization: update all the weights and biases to minimize the Cost function to best match our training data with the ground truth-labels. 用 Backpropagation 计算 gradient. Gradient descent 更新所有参数($\mathbf{W,b}$) to minimize the Cost function. 得到最后最优参数， i.e. 使得 Cost function 取最小值的参数 $(\mathbf{W^,b^})$ 这个阶段主要做 Training NN，因此也可以叫做 Training（从人的角度） or Learning（从算法/机器的角度）。 Make Prediction 在新的测试数据上做预测 这个阶段又叫做 Test. 下图可以看出Training NN 的大致过程， 整个Training Process中最为核心的部分就是： 通过Backpropagation（本质就是multivarite chain rule） 计算gradient；然后使用 Gradient descent 来寻找使得 Cost function 最小的参数。 We need to build the Jacobian by gathering together the partial derivatives of the cost function with respect to all of the revelant variables - weights and biases. 结合之前的知识我们简单地对上图做个分析， 我们通过迭代更新parameters 来使得 Cost function 的值不断变小，最终达到最小值，此时的 weights 和 biases 就是我们最终想要的。 所谓更新parameters，就是指 利用gradient descent算法使Cost function 的gradient不断下降（计算gradient处的那个点 对应的Cost function的值），不断调整 parameters的大小 gradient 和 参数有什么关系？参数 weights and biases 才是 Cost function的变量，神经网络的输入以及label是常量。因此，gradient is matrices and vectors of partial derivatives of the Cost funtion with resprect to weights and biases. Example 3： 我们现在以一个最简单的NN为例：一个 input neuron，一个 output neuron. 如图， 简要分析： Cost function 是 square loss，衡量模型输出 $a^{(1)}$ 与训练数据的真实标签 $y$之间的平方差。我们的目的是最小化这个函数使得：模型的输出与真实的结果越接近越好。 我们有两个 Chain expressions，均有三个links. 通过他们我们可以计算得到 gradient(Jacobian) of Cost function with repsect to weights and biases. 从而使得我们可以使用梯度下降算法来找到使 Cost function取最小值时的参数 $w^,b^$. $C = (a^{(1)}-y)^2$ 是针对一个 specific training example而言的，如果有多个examples 应该将它们的损失函数加起来作为整个模型的损失韩式，即： $$C = \frac{1}{N} \sum_k^N C_k$$ 其中 $C_k$ 是第$k$ 个training example 的 Cost function；$N$ 是 number of examples in training data; $C$ is the total Cost function of the training data. Example 4: 当我们增加更多的layers 和neurons的时候，事情会变得稍微复杂一些，见下图： 但是基本的过程依然如同我们training 上面那个最简单的NN一样。 可以参考一个关于 Training neural network的练习题link 关于梯度下降所谓”Gradient descent” ，可以参考下图： 其包含以下这些方面： Cost function 上用以计算 gradient 的点的位置在下降，其实也就是说该点处的函数值在减小。 对于n元实值函数来说，其Gradient是一个vector。gradient vector 的长度或者叫范数，是衡量函数在某一点处变化的快慢；gradient 的方向指明了函数值增加/上升最快的方向，那么其反方向就指明了函数值减小/下降 最快的方向。 gradient 是函数对各个自变量偏导数的向量表示，所以它是自变量空间或者叫parameter space（对神经网络来说）中的一个向量，这个向量的方向即为函数值上升最快的方向。 综上所述，所谓gradient descent的本质含义就是：在与gradient相反的方向上改变自变量的取值，会使得函数值变小。 下面稍做解释： 要梯度相反的方向，对gradient 直接取负就好，得到 $-\nabla_{\mathbf{x}}f(\mathbf{x})$. $$-\nabla_{\mathbf{x}}f(\mathbf{x}) = \begin{bmatrix}-\frac{\partial f}{\partial x_1} \\ -\frac{\partial f}{\partial x_2}\\\vdots \\-\frac{\partial f}{\partial x_n}\end{bmatrix}$$ 注意：我们根据偏导数的概念知道$\frac{\partial f}{\partial x_i}$ 是指在$x_i$ 这个自变量的方向上，函数值变化的快慢（沿着$x_i$ 轴方向）。那么$-\frac{\partial f}{\partial x_i}$ 就是沿着$-x_i$ 轴方向函数值变化的快慢。 根据梯度的定义: 我们知道沿着负梯度函数值减小。函数值的变换本质上是由于自变量取值的变化，我们也知道自变量变化可以表示为：$x\rightarrow x+\Delta x$. 对n元函数来说，是按照如下方式： $$\begin{align}x_1 := x_1 + \Delta x_1\\x_2 := x_2 + \Delta x_2\\\vdots \\x_n := x_n + \Delta x_n\end{align}$$ 我们只要使得每个 $\Delta x_i$ 的方向是沿着$-x_i$ 轴的方向（即与 $-\frac{\partial f}{\partial x_i}$ 同向），那么我们就可以保证所有自变量都沿着负梯度方向变化，此时：函数值降低。根据上面内容，我们只要保证$\Delta x_i$ 是 -$\frac{\partial f}{\partial x_i}$ 或者是它的正数倍就可以达到我们的目的。换句话说即，$$x_1 := x_1 +\alpha(-\frac{\partial f}{\partial x_1})\\x_2 := x_2 + \alpha(-\frac{\partial f}{\partial x_2})\\\vdots\\x_n := x_n + \alpha (-\frac{\partial f}{\partial x_n})\\\text{Where } \alpha &gt; 0$$将上述公式写成向量形式，$$\begin{align}&amp;\color{blue}{\mathbf{x} := \mathbf{x} - \alpha \nabla_\mathbf{x}f(\mathbf{x})}\\&amp;\text{or another equal expression} \\&amp;\color{blue}{\mathbf{x} := \mathbf{x} - \alpha \frac{\partial f}{\partial \mathbf{x}}} \\\end{align}$$上式就是我们常见的 gradient descent算法中参数的更新公式。对于神经网络来说，即：$$\begin{align}&amp;\text{For weights}\\&amp;\color{blue}{\mathbf{W} := \mathbf{W} - \alpha \nabla_\mathbf{W}C(\mathbf{W})} \Leftrightarrow\color{blue}{\mathbf{W} := \mathbf{W} - \alpha \frac{\partial C}{\partial \mathbf{W}}}\\&amp;\text{and for biases} \\&amp;\color{blue}{\mathbf{b} := \mathbf{b} - \alpha \nabla_\mathbf{b}C(\mathbf{b})} \Leftrightarrow\color{blue}{\mathbf{b} := \mathbf{b} - \alpha \frac{\partial C}{\partial \mathbf{b}}} \\\end{align}$$计算梯度$\nabla C$ 的算法就是基于multivariate链式法则的 Backpropagation，它是NN context 下Gradient descent 的核心。 这里有一个很好的关于梯度下降的讲解视频, 值得认真学习、反复钻研 link Backpropagation我们现在以包含一个hidden layer 的3层神经网络为例分析backpropagation算法是如何计算梯度的。Example 4:神经网络如下图 输入：向量 $\mathbf{a}^{(0)}\in \mathbb{R}^4$ 隐层：向量 $\mathbf{a}^{(1)}\in \mathbb{R}^3$ 输出：向量 $\mathbf{a}^{(2)}\in \mathbb{R}^2$ Layer 1的参数：$\mathbf{W}^{(1)}\in \mathbb{R}^{3\times 4},\mathbf{b}^{(1)}\in \mathbb{R}^3$ Layer 2的参数：$\mathbf{W}^{(2)}\in \mathbb{R}^{2\times 3} ,\mathbf{b}^{(2)}\in \mathbb{R}^2$ For a specific training example, Vector form of Cost function: $C_k = \Vert \mathbf {a}^{(2)} - \mathbf{y} \Vert^2$. 如果我们想计算Cost function对于最后一层(layer 2)的 weights 和 bias的梯度（或者不严谨地叫做偏导数），与之前流程一样：$$\begin{align} \frac{\partial C_k}{\partial \mathbf{W}^{(2)}} &amp;= \frac{\partial C_k}{\partial \mathbf{a}^{(2)}} \frac{\partial \mathbf{a}^{(2)}}{\partial \mathbf{z}^{(2)}} \frac{\partial \mathbf{z}^{(2)}}{\partial \mathbf{W}^{(2)}} \\\frac{\partial C_k}{\partial \mathbf{b}^{(2)}} &amp;= \frac{\partial C_k}{\partial \mathbf{a}^{(2)}} \frac{\partial \mathbf{a}^{(2)}}{\partial \mathbf{z}^{(2)}} \frac{\partial \mathbf{z}^{(2)}}{\partial \mathbf{b}^{(2)}}\end{align}$$如果我们要计算 Cost function 对于前一层(layer 1) 参数的梯度，我们可以使用下面表达式，$$\begin{align} \frac{\partial C_k}{\partial \mathbf{W}^{(1)}} &amp;= \frac{\partial C_k}{\partial \mathbf{a}^{(2)}} \frac{\partial \mathbf{a}^{(2)}}{\partial \mathbf{a}^{(1)}} \frac{\partial \mathbf{a}^{(1)}}{\partial \mathbf{z}^{(1)}} \frac{\partial \mathbf{z}^{(1)}}{\partial \mathbf{W}^{(1)}} \\\frac{\partial C_k}{\partial \mathbf{b}^{(2)}} &amp;= \frac{\partial C_k}{\partial \mathbf{a}^{(2)}} \frac{\partial \mathbf{a}^{(2)}}{\partial \mathbf{a}^{(1)}} \frac{\partial \mathbf{a}^{(1)}}{\partial \mathbf{z}^{(1)}} \frac{\partial \mathbf{z}^{(1)}}{\partial \mathbf{b}^{(1)}}\end{align}$$其中 $\frac{\partial \mathbf{a}^{(2)}}{\partial \mathbf{a}^{(1)}}$ 可以被展开为:$$\frac{\partial \mathbf{a}^{(2)}}{\partial \mathbf{a}^{(1)}}=\frac{\partial \mathbf{a}^{(2)}}{\partial \mathbf{z}^{(2)}} \frac{\partial \mathbf{z}^{(2)}}{\partial \mathbf{a}^{(1)}}$$ 我们可以将上述过程推广到求损失函数对任何层梯度的情况，$$\frac{\partial C_k}{\partial \mathbf{W}^{(i)}} = \frac{\partial C_k}{\partial \mathbf{a}^{(L)}} \color{blue} {\underbrace{\frac{\partial \mathbf{a}^{(L)}}{\partial \mathbf{a}^{(L-1)}} \frac{\partial \mathbf{a}^{(L-1)}}{\partial \mathbf{a}^{(L-2)}} \cdots \frac{\partial \mathbf{a}^{(i+1)}}{\partial \mathbf{a}^{(i)}}}_{\text{from layer L to layer i}} }\frac{\partial \mathbf{a}^{(i)}}{\partial \mathbf{z}^{(i)}} \frac{\partial \mathbf{z}^{(i)}}{\partial \mathbf{W}^{(i)}} \\\frac{\partial C_k}{\partial \mathbf{b}^{(i)}} = \frac{\partial C_k}{\partial \mathbf{a}^{(L)}} \color{blue}{\underbrace{\frac{\partial \mathbf{a}^{(L)}}{\partial \mathbf{a}^{(L-1)}} \frac{\partial \mathbf{a}^{(L-1)}}{\partial \mathbf{a}^{(L-2)}} \cdots \frac{\partial \mathbf{a}^{(i+1)}}{\partial \mathbf{a}^{(i)}}}_{\text{from layer L to layer i}}} \frac{\partial \mathbf{a}^{(i)}}{\partial \mathbf{z}^{(i)}} \frac{\partial \mathbf{z}^{(i)}}{\partial \mathbf{b}^{(i)}}$$ 其中$\frac{\partial \mathbf{a}^{(j)}}{\partial \mathbf{a}^{(j-1)}}$ 可以被展开为：$$\frac{\partial \mathbf{a}^{(j)}}{\partial \mathbf{a}^{(j-1)}} = \frac{\partial \mathbf{a}^{(j)}}{\partial \mathbf{z}^{(j)}} \frac{\partial \mathbf{z}^{(j)}}{\partial \mathbf{a}^{(j-1)}}$$ 以上，就是Backpropagation的全过程。需要注意的几点是： 上面公式中我们都用了偏导数的符号$\frac{\partial}{\partial}$, 但根据本节课之前的内容我们应该明白这些偏导数符号表达的是 Jacobian。 乘法应该是matrix multiplication 因为只有基于上述两点，以上整个过程涉及的公式才能将： $f:\mathbb{R}^n \rightarrow \mathbb{R}$, i.e. $\nabla_\mathbf{x}f(\mathbf{x})=(\mathbf{J}_f(\mathbf{x}))^T \in \mathbb{R}^{n\times 1} $ $f:\mathbb{R}^{m\times n} \rightarrow \mathbb{R}$, i.e. $\nabla_{\mathbf{x}}f(\mathbf{x}) \in \mathbb{R}^{m\times n}$ $f:\mathbb{R}^n \rightarrow \mathbb{R}^m$,i.e. $\mathbf{J_f(x)} \in \mathbb{R}^{m\times n}$ $f:\mathbb{R} \rightarrow \mathbb{R}^m$,i.e. $\mathbf{J_f}(x) \in \mathbb{R}^{m\times 1}$ 这四种函数值与自变量的组合情况统一在一组公式之下，因为：Jacobian 完全兼容 gradient，更加具有通用性；缺点就是Jacobian不如gradient的表示式那么直观。 动手练习这是一个练习 Backpropagation的notebook-link. Learning by doing! Learning by teaching!]]></content>
      <categories>
        <category>课程笔记</category>
        <category>Mathematics for Machine Learning</category>
        <category>Multivariate Calculus</category>
      </categories>
      <tags>
        <tag>Gradient Descent</tag>
        <tag>Chain Rule</tag>
        <tag>Neural Network</tag>
        <tag>Joacbian</tag>
        <tag>Backpropagation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二.多元微积分]]></title>
    <url>%2Farchive%2F2018-07-07%2FMultivariate-calculus%2F</url>
    <content type="text"><![CDATA[学习目标 1.学习将微积分应用到多变量的情况 2.掌握向量、矩阵在多元微积分中的作用 3.Jacobian方法在二元问题中的应用 多元的含义记得我们上节课说到”变量这个概念是context-dependent 的”，意思就是说：如果在一个函数中出现多个符号，哪些是variable，哪些不是，那个是function，这取决于你要研究的对象是什么，而不是有固定模式帮助我们认定函数、变量、常量等待。这些概念都是相对的。 所谓多变量是指这样一种情况：我们有一个函数表达式 $f(x,y,z)$, 这个表达式的值依赖于$x,y,z$ ，意思是说逻辑上是先有了$x,y,z$的取值，我们才能确定$f(x,y,z)$的值，因此我们称$f(x,y,z)$ 是自变量$x,y,z$ 的函数。 如果一个函数的值取决于多个变量共同的取值，我们称这个函数是一个多元函数（多变量函数） 我们应该具体问题具体分析: variables, constants, context 多元函数求导偏导数（parietal derivative） 是指多元函数对某一个单独变量的导数。单独地衡量该变量对函数值变化的贡献，这时候其他变量被看作常数。 存在一个函数$f(x,y,z)$，其对$x,y,z$的偏导数分别记为：$\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y},\frac{\partial f}{\partial z}$. 全导数（total derivative） 存在一个函数$f(x,y,z)$， 而且$x=x(t),y=y(t),z=z(t)$ . 也就是说除了三个自变量 $x,y,z$之外还有一个变量$t$, 它与之前三个自变量都有关系。那么如此看来，$f$ 直接depend on $x,y,z$；但是间接地depend on $t$. 那么所谓的全导数是指：$\frac{d f}{dt}$. 在求解全导数的过程中和我们在求偏导数的时候有一点不一样：在不能将其他变量看作是常数而忽略, 计算公式如下，$$\frac{d f(\color{blue}{x},\color{green}{y},\color{purple}{z})}{d t}=\frac{\partial f}{\partial \color{blue}{x}}\frac{d \color{blue}{x}}{dt} + \frac{\partial f}{\partial \color{green}{y}} \frac{d\color{green}{y}}{dt}+\frac{\partial f}{\partial \color{purple}{z}} \frac{d\color{purple}{z}}{dt}$$Example: TO_DO… Vector of derivatives一.多元实值函数的Jacobian Jacobian 本质上一个是多变量的函数，其表现形式往往为一个向量（更泛化地说，是一个 matrix）。 Jacobaian is just a single function of many variables 存在一个n元实值函数$f(x_1,x_2,…,x_n)$ ，$f:\mathbf{R}^n\rightarrow \mathbf{R}$，其 Jacobian 如下：$$\mathbf{J}_f = \begin{bmatrix}\frac{\partial f}{\partial x_1}&amp; \frac{\partial f}{\partial x_2} &amp; \cdots &amp;\frac{\partial f}{\partial x_n}\end{bmatrix}$$Jacobian 写成行和列本质上是一样的，习惯上是写成行，记住：仅仅是习惯上。 但他俩都是 vector of derivates. 显而易见，这种情况下的 Jacobian 就是我们常见的多元函数的梯度。 Example： $f(x,y,z)=x^2y+3z$ $\mathbf{J}_f=\begin{bmatrix} 2xy &amp; x^2 &amp; 3 \end{bmatrix}$ Jacobian 能告诉我们什么信息呢？ 如果给我们一组确定的$(x,y,z)$坐标，Jacobian 代表的那个向量的方向指向了函数$f(x,y,z)$ 斜率最大的方向即函数变化最快的方向，Jacobian 向量的magnitude 代表了斜率的大小，即函数在那个确定点处变化的大小。总之，这种情况下的 Jacobian是函数$f(x,y,z)$的梯度，它衡量的函数的变化：大小和方向。 二. 多元向量值函数的Jacobian存在两个函数$u(x,y)=x-2y,v(x,y)=3y-2x$ 如下图，我们把$u,v$ 看作一个向量的components，因此我们可以把它们各自的Jacobian 按行堆叠起来组成一个Jacobian矩阵，过程如下，$$\begin{align}u(x,y) &amp;= x-2y\\v(x,y) &amp;= 3y-2x\\J_u&amp;=\begin{bmatrix}\frac{\partial u}{\partial x}\ \ \frac{\partial u}{\partial y}\end{bmatrix}\\J_v&amp;=\begin{bmatrix}\frac{\partial v}{\partial x}\ \ \frac{\partial v}{\partial y}\end{bmatrix}\\J &amp;= \begin{bmatrix}\frac{\partial u}{\partial x}\ \ \frac{\partial u}{\partial y}\\\frac{\partial v}{\partial x}\ \ \frac{\partial v}{\partial y}\end{bmatrix}= \begin{bmatrix} 1 &amp; -2\\-2&amp;3 \end{bmatrix} \\\end{align}$$ 其中，$J_u,J_v​$ 的意思是 $J(u(x,y)), J(v(x,y))​$.从上述结果可以看出，Jacobian矩阵的entries 都是常数，那是因为$u,v$ 均为$x,y$ 的线性函数，所以梯度处处都为常数。 如果细心的人会发现，如果将上面最初的$u,v$ 函数的表达式看作方程，就会得到：$$\begin{bmatrix}1 &amp; -2 \\-2 &amp; 3\end{bmatrix}\begin{bmatrix}x\\y\end{bmatrix}=\begin{bmatrix}u\\v\end{bmatrix}$$ 此即为线性代数中最常见的线性方程的表达式：$\mathbf{A}\vec x = \vec b$我们还发现，上面的Jacobian $\mathbf{J=A}$. 这不是偶然的，因为根据这里Jacobian的定义和线性方程的线性特性：线性变换$\mathbf{A}$ 就是 $\mathbf{J}$. 换句话说, 在u,v 都是 x,y 的线性函数的情况下， $\mathbf{J}$ 是一个从$x,y$ 空间到$u,v$ 空间的一个线性变换；且就是$\mathbf{A}$。 上面介绍了两种多元函数对应的 Jacobian，现在给出一个统一的定义： Jacobian的统一定义: 存在一个函数 $\mathbf{f}:\mathbf{R^n}\rightarrow \mathbf{R^m}$ (如果m=1，其为实值函数；如果m&gt;1,其为向量值函数)，它的Jacobian定义为：$$\begin{align}\mathbf{J_f} = \begin{bmatrix}&amp;\frac{\partial f_1}{\partial x_1} &amp;\frac{\partial f_1}{\partial x_2} \dots &amp;\frac{\partial f_1}{\partial x_n}\\&amp;\frac{\partial f_2}{\partial x_1} &amp;\frac{\partial f_2}{\partial x_2}\dots &amp;\frac{\partial f_2}{\partial x_n}\\&amp;\vdots &amp;\dots &amp;\vdots \\&amp;\frac{\partial f_m}{\partial x_1} &amp;\frac{\partial f_m}{\partial x_2} \dots &amp;\frac{\partial f_m}{\partial x_n}\end{bmatrix}\in \mathbf{R^{m \times n}}\end{align}$$我们来分析一下这个统一定义： 这个定义要比上面提到的 “向量值函数的Jacobian” 的情况更加泛化，如果$\mathbf{f}$ 是线性变换，那么和上面情况一样，$\mathbf{J}$ 就是表示线性变换的那个矩阵；如果 $\mathbf{f}$ 不是线性变换，那么就不同于之前提到的情况。因此这个统一定义是有着更加泛化的意义的。 如果 m = 1，$f$ 的Jacobian 就是其 gradient(一个行，一个列；严格来说是转置关系。) 前面分析讲到了, 当函数是多元实值函数的时候，其Gradient 就是是其 Jacobian，两个概念是一样的; 根据Jacobian的统一定义，当函数$f$ 是向量值函数时，Gradient 和 Jacobian 就没有这样的关系了；因为一个向量值函数是没有所谓的Gradient概念的。 向量值函数$f$ 本质上是由m 个 实值函数组成的向量，否则是无法计算 gradient的，当然也就无法计算Jacobian. 因此，向量值函数的Jacobian计算过程就是分别计算这 m 个实值函数的gradients ,然后按行堆叠起来组成一个$m\times n$ 的矩阵即可。其中，n 是函数自变量的个数；m 是组成最后$f$ 向量值的实值函数的个数。 下面我们给出Gradient的统一定义： 存在一个实值函数 $f:\mathbf{R^{m \times n} \rightarrow \mathbf{R}}$，其自变量是一个$\in \mathbf{R^{m\times n}}$ 的矩阵，记为：$\mathbf{A}$ 函数$f$关于$\mathbf{A}$ 的gradient 为一个由偏导数组成的矩阵：$$\nabla_{A}f(A) \in \mathbf{R^{m \times n}} = \begin{bmatrix}\frac{\partial f(A)}{\partial A_{11}} &amp; \frac{\partial f(A)}{\partial A_{12}} &amp;\dots &amp;\frac{\partial f(A)}{\partial A_{1n}}\\\frac{\partial f(A)}{\partial A_{21}} &amp; \frac{\partial f(A)}{\partial A_{22}} &amp;\dots &amp;\frac{\partial f(A)}{\partial A_{2n}}\\\vdots &amp;\dots &amp;\dots &amp;\vdots \\\frac{\partial f(A)}{\partial A_{m1}} &amp; \frac{\partial f(A)}{\partial A_{m2}} &amp; \dots &amp; \frac{\partial f(A)}{\partial A_{mn}}\end{bmatrix}$$注意： 函数$f$ 的 gradient 总是与其自变量矩阵$\mathbf{A}$ 同型的。 与 Jacobian统一定义的不同之处在于： gradient 一定是针对实值函数的；而 Jacobian在向量值函数上也是有定义的。 对实值n元函数$f:\mathbb{R^n}\rightarrow \mathbb{R}$ 而言， $\frac{\partial f}{\partial \mathbf{x}} =\mathbf{J^T}$; Gradient向量的每一个components 表明了该处自变量对于函数变化速度的贡献。 如果特殊情况下，$\mathbf{A}$ 仅仅是个向量 $\mathbf{x} \in \mathbf{R^n}$ 而非矩阵，即：$f: \mathbf{R^n} \rightarrow \mathbf{R}$那么:$$\nabla_\mathbf{x}f(\mathbf{x}) =\frac{\partial f}{\partial \mathbf{x}} = \begin{bmatrix}\frac{\partial f}{\partial x_1}\\\frac{\partial f}{\partial x_2}\\\vdots\\\frac{\partial f}{\partial x_n}\end{bmatrix}$$此时 gradient 确实等于其 Jacobian, i.e. $\nabla_\mathbf{x} f(\mathbf{x}) = \mathbf{J}f$ or 等于 Jacobian的转置: $\nabla{\mathbf{x}}f(\mathbf{x})=(\mathbf{J}_f)^T$. Jacobian 的应用（细节可以参考这个 Jupyter book ） 海森矩阵-Hessian多元函数的 Hession 可以看作是gradient 概念的延伸，存在一个n元实值函数 $f:\mathbf{R^n} \rightarrow \mathbf{R}$, Gradient 把函数对各个自变量的一阶偏导数集合起来组成一个向量$\in \mathbf{R^n}$. Hessian 把函数对各个自变量的二阶偏导数集合起来组成一个矩阵$\in \mathbf{R^{n\times n}}$. $$\text{Hessian}(f)=\mathbf{H}= \begin{bmatrix}\frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1\partial x_2} &amp; \dots &amp;\frac{\partial^2 f}{\partial x_1\partial x_n} \\\frac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2 f}{\partial x_2^2} &amp; \dots &amp;\frac{\partial^2 f}{\partial x_2\partial x_n}\\\vdots &amp; \dots &amp;\dots &amp; \vdots \\\frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \frac{\partial^2 f}{\partial x_n \partial x_2} &amp; \dots &amp; \frac{\partial^2 f}{\partial x_n^2}\end{bmatrix}$$ PS: 当然针对一个多元实值函数来说，Hessian 也可以看作是 Jacobian的延伸，因为这种情况下的Jacobian即是gradient。 针对Hessian矩阵的分析： Hessian 是对称矩阵。 既然对称，一定可以对角化。且可以选出一组正交的eigen-vectors来对角化：$\mathbf{H=Q\Lambda Q^T}$, 其中$\mathbf{Q}$ 是正交矩阵，其每一列为$\mathbf{H}$ 的特征向量，$\Lambda$ 是由$\mathbf{H}$ 的特征值构成的对角矩阵。 优化在一元函数中，我们有如下结论：$$\begin{cases}\frac{\partial f}{\partial x}=0 , \frac{\partial^2 f}{\partial x^2}&gt;0 \Rightarrow f_{min} \\\frac{\partial f}{\partial x}=0 , \frac{\partial^2 f}{\partial x^2}&lt;0 \Rightarrow f_{max}\end{cases}$$在多元函数中，我们有类似结论：$$\begin{cases}\mathbf{J}=\vec 0, \mathbf{H} \text{ is Positive Definite} \Rightarrow f_{min} \\\mathbf{J}=\vec 0, \mathbf{H} \text{ is Negative Definite} \Rightarrow f_{max}\end{cases}$$ 什么时候 $\mathbf{H}$ is Positive Definitive？其充要条件为：1. $\vec x^T \mathbf{H}\vec x &gt;0$ for any $\vec x \ne 0$.(正定矩阵的定义)2. 所有特征值&gt;03. 所有Pivots &gt; 04. 所有subdeterminants &gt; 0 Example1: $f(x,y)=x^2+y^ 2$, 其图像如下图 我们利用上述结论求其最小值： Jocabian = $\begin{bmatrix}2x\\ 2y \end{bmatrix}$ ,令其gradient为0，i.e. $J=0 \Rightarrow \begin{bmatrix}0\\0\end{bmatrix}$ 点是一个critical point(因为gradient为0)。 现在求出函数$f$ 的Hessian，并判断其正定情况： $\mathbf{H}= \begin{bmatrix}\frac{\partial^2 f}{\partial x^2} &amp; \frac{\partial^2f}{\partial x \partial y} \\ \frac{\partial^2 f}{\partial y\partial x} &amp; \frac{\partial^2 f}{\partial y^2} \end{bmatrix} = \begin{bmatrix}2 &amp; 0\\ 0 &amp; 2 \end{bmatrix}$ 根据上面关于正定矩阵的充要条件，我们知道 $\mathbf{H}$ 是正定的，因此函数$f$ 在 （0，0）处取的最小值。 Example2: $f(x,y) = x^2-y^2$，其图像如下 $J = \begin{bmatrix} 2x \\ -2y \end{bmatrix}$, 令其gradient=0 $\Rightarrow x=0,y=0$ ,所有(0,0) 是一个critical point. $\mathbf{H} = \begin{bmatrix}2 &amp; 0 \\0 &amp; -2 \end{bmatrix}$, 根据Hessian正定的充要条件，$\mathbf{H}$ 既不是正定，也不是负定$\Rightarrow \mathbf{H}$ 是不定的，说明(0,0)点是函数 $f$ 的一个 saddle point $\Rightarrow$ 在某个方向是最大值，在某个方向是最小值 $\Rightarrow$ 全局来说，既非最大也非最小！ 现实是艰难的不连续性 很多跳变点 鞍点 … 待补充]]></content>
      <categories>
        <category>课程笔记</category>
        <category>Mathematics for Machine Learning</category>
        <category>Multivariate Calculus</category>
      </categories>
      <tags>
        <tag>Calculus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一. 什么是微积分]]></title>
    <url>%2Farchive%2F2018-07-07%2FWhat-is-calculus%2F</url>
    <content type="text"><![CDATA[学习目标 1.了解微积分的概念 2.介绍导数的形式化定义 3.三大求导法则：加法、乘法、链式法则 导数的定义导数（derivative）, 是用来衡量函数随着自变量变换而变换的速度的量。 $f(x)$ 是一个以$x$ 为自变量的函数（或者这样说，$f(x)$ 是一个作用于自变量$x$的函数），其关于$x$的导数 记为：$f^\prime(x)$ 或者 $\frac{df}{dx}$, 定义为：$$f^\prime (x)=\lim_{\Delta x\to 0} \frac{f(x+\Delta x) - f(x)}{\Delta x}$$当然，函数$f(x)$ 对于$x$ 的导数$f^\prime(x)$ 也是一个关于自变量$x$ 的函数；只不过$f^\prime(x)$ 是用来刻画函数$f(x)$ 变换的函数，这一点我们很容易想象：对于自变量$x$的每个具体取之，在该点处 $f(x)$ 的变化可能不一样，又快又慢、变大变小。 在几何上的表现是某点切线的斜率。 PS: 变量这个概念非常依赖于 context，识别什么是自变量、什么是函数一定要结合具体的context. 三个求导法则Sum Rule$$\frac{d(f(x)+g(x))}{dx} = \frac{df}{dx} + \frac{dg}{dx}=f^\prime(x)+g^\prime(x)$$Product Rule$$\begin{align}\frac{d(f(x)g(x))}{dx}&amp;= \frac{df}{dx}g(x)+f(x)\frac{dg}{dx} \\&amp;= f^\prime(x)g(x)+f(x)g^\prime(x)\end{align}$$Division Rule$$(\frac{f(x)}{g(x)})^\prime = \frac{f^\prime(x)g(x)-f(x)g^\prime(x)}{g^2(x)}$$Chain Rule$$\begin{align}f(g) ,g(h),h(x) \\f^\prime(x)=\frac{df}{dx}&amp;= \frac{df}{dg}\cdot \frac{dg}{dh}\cdot \frac{dh}{dx} \\&amp;= f^\prime(g) g^\prime(h)h^\prime(x)\end{align}$$ 特殊函数的导数 $(e^x)^\prime = e^x$ $sin^\prime(x) = cos (x)$ $cos^\prime(x)=-sin(x)$ $(x^n)^\prime = nx^{n-1}$]]></content>
      <categories>
        <category>课程笔记</category>
        <category>Mathematics for Machine Learning</category>
        <category>Multivariate Calculus</category>
      </categories>
      <tags>
        <tag>Calculus</tag>
        <tag>Jacobian</tag>
        <tag>Gradient</tag>
        <tag>Hessian</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[五.特征向量与特征值]]></title>
    <url>%2Farchive%2F2018-07-01%2FEigenvalues-and-Eigenvectors%2F</url>
    <content type="text"><![CDATA[学习目标 认识什么是特征向量、特征值？ 能够应用数学公式在实际情况中 构建对高维度特征系统的认知 什么是Eigen-Things?特征值、或者特征向量等概念是依赖于线性变换这个基础概念而存在，并非可以独立存在的概念。 在线性代数的领域中，一般讲到线性变换也就等同于说一个matrix。因此，我们会经常听到这种说法： 矩阵的特征值是… 这个线性变换的特征向量…. 这个变换矩阵的特征向量是… 这个线性映射的特征值…. 以上几种说法在 线性代数这个大的语境下，表达的是一个意思。 一个矩阵本身就是一个线性变换，我们常常考察的情景是：一个线性变换$\mathbf{A}$ 如何对一个向量$\vec v$ 进行变换；有时我们也会考察这个线性变换对整个空间的向量如何进行变换，这体现在我们具体考察线性变换对一个空间的基向量如何做变换：$\mathbf{A}\begin{bmatrix}v_x\\v_y\end{bmatrix} = \mathbf{A}(v_x \begin{bmatrix} 1\\0\end{bmatrix}+v_y\begin{bmatrix}0\\1\end{bmatrix})=v_x(\mathbf{A}\begin{bmatrix}1\\0\end{bmatrix})+v_y(\mathbf{A}\begin{bmatrix} 0\\1\end{bmatrix})$ 其中，$\mathbf{A}\begin{bmatrix}1\\0\end{bmatrix}$ 就是线性变换$\mathbf{A}$ 对原space的第一个基向量做的变换，其变换结果为：矩阵$\mathbf{A}$ 的第一列，对另一个基向量的变换同理。 通常我们是这样认为的：一个线性变换对与一个空间的基向量做了变换之后，那么就等于对整个空间中所有的向量做了变换。至少理论上是这样的，因为该空间的任何一个向量都可以由基向量线性组合而成。但是，如果有些向量被线性变换之后，是下面2种结果： 没有发生变换（大小、方向都没变） 没有发生很大的变换（大小变了，但是方向没变） 那么对一个线性变换 $\mathbf{A}$ 来讲，这些向量就显得比空间中其他向量要特殊一些。所谓的“特征”这个概念，就是用来研究这些特殊的向量。 好了，那什么的Eigen-things？ 在一个线性变换的作用下，空间中有些向量保持他们的span不变，那么这些向量就称为这个线性变换/矩阵的 特征向量；线性变换对这些向量做尺度变换（拉伸、压扁、反向）的系数，称为在该线性变换下，该特征向量对应的特征值。 简而言之，就是在线性变换中保持某种程度不变的量。 PS：其实eigen这个词是从德语系中借用过来的，其本身就是 ”characteristic“ 的意思：本质、本证、本性的意思。这也符合上述我们所讲的：”在变换中那些保持某种程度不变“ 这一思想。 了解特征问题的细节我们给出形式化的定义： 给定一个线性变换（矩阵）$\mathbf{A} \in \mathbf{R}^{n\times n}$, 如果有$\vec x \in \mathbf{R}^n$ 使得 $\mathbf{A}\vec x=\lambda \vec x$成立，我们称$\vec x$ 为线性变换 $\mathbf{A}$ 的特征向量，$\lambda$ 称为特征向量$\vec x$ 在线性变换$\mathbf{A}$ 之下对应的特征值。 关于这个定义我们简要地分析一下： 特征值、特征向量一定是针对某个线性变换$\mathbf{A}$而言的！！！Eigen-things 不是一个可以单独存在的概念。 定义中的 $\lambda$ 指的是线性变换对向量 $\vec x$ 的变换系数，其绝对值表示放大、缩小的倍数；其正、负表示与原来方向相同还是相反。 从上述定义我们可知，如果一个向量$\vec x$ 是线性变换$\mathbf{A}$ 的一个特征向量，那么该向量的乘以任何一个scalar也是线性变换$\mathbf{A}$的一个特征向量，即：特征向量的个数是无穷的。 我们只关心那些线性无关的特征向量以及他们对应的特征值。 我们不考虑 $\vec 0$ 作为我们的特征向量（那没有任何意义！） 所谓在线性变换的作用下某些向量不改变它的span，体现在公式中即：$\mathbf{A}\vec x $ 与 $\lambda \vec x$ 在一个方向（同一个直线上） 特征基在什么时候有用？我们知道一个空间的基可以有很多个，其中有一些比较“特殊”，或者说他们比较“好”一点。比如在前面课程讲到过的“标准正交基”，就具有很多优良的性质：比如 标准正交基为列组成的矩阵叫做正交矩阵。$\mathbf{UU^T=U^TU=I}$ 标准正交基可以使用投影做基变换，而不必使用一个 transformation matrix（像普通的矩阵所代表的线性变换那样） 面对一组普通的基，我么采用Gram-Schmit 正交化过程将其转换成正交基，这一过程我们也称之为正交化。 我们知道，除了标准正交基之外，还有其他很多种类的基，比如：如果用一个线性变换（矩阵）的eigen-vectors 作为一组基，我们称之为 特征基（eigen-basis）。这会引出一个我们称之为矩阵对角化的过程。 其来源于这样一种现实问题，如下图 问题描述：现在有一个线性变换$\mathbf{A}$, 我们需要做n次同样的变换，即：$\mathbf{A} \cdots(\mathbf{A}(\mathbf{A}\vec x))=\mathbf{A^n}\vec x$ 我们需要计算 $\mathbf{A^n}$ . 途径一：通过changing of basis，我们希望在特征基代表的空间中做n 次$\mathbf{A}$ 变换，即：计算$\mathbf{A}^n$. $\mathbf{S}=\begin{bmatrix}\vec x_1\ \vec x_2\ \cdots \vec x_n\end{bmatrix}$ 是以矩阵$\mathbf{A}$ 的特征向量为列构成的矩阵，这个矩阵代表：“特征空间中的基向量在我们坐标系下的表示”，根据第四课的知识，我们知道 $\mathbf{S^{-1}}$ 代表了我们的基在特征空间坐标系下的表示。 现在我们需要思考，在特征空间中是如何看待$\mathbf{A}$的–得到线性变换$\mathbf{A}$ 在eigen-space坐标系下的表示。 根据$\mathbf{A}\vec x=\lambda \vec x$, 我们知道在特征空间中，线性变换$\mathbf{A}$只是对向量做了scale，系数为 $\lambda$. 即：$$\begin{align}\text{Linear transformation in my world }\mathbf{A} \rightarrow\Lambda = \begin{bmatrix}&amp;\lambda_1 &amp;\cdots&amp;\cdots\\&amp;\vdots &amp;\ddots &amp;\vdots \\&amp;\cdots &amp;\cdots &amp;\lambda_n\end{bmatrix}: \mathbf{A}\text{ in eigen-space.}\end{align}$$那么整个变换过程如下：$$\mathbf{A}=\mathbf{S} \Lambda \mathbf{S^{-1}}$$其中， $\mathbf{S^{-1}}$ : 我们的基在特征空间坐标系下的表示，进入到特征空间。 $\Lambda \mathbf{S^{-1}}$ : 在特征空间中进行线性变换$\Lambda$ . $\mathbf{S}$ ：特征基在我们空间坐标系下的表示。 $\mathbf{S\Lambda S^{-1}}$ : 将计算结果转换回我们的空间。 那么最终，$\mathbf{A^n}=\mathbf{S\Lambda S^{-1}}\cdots \mathbf{S\Lambda S^{-1}}=\mathbf{S\Lambda^n S^{-1}}$ 其实在changing of basis的角度下，还有另外的思考方式。之前我们说过，我们希望知道在特征空间中是如何看待我们空间中的线性变换$\mathbf{A}$ 的，其变换如下（参考第四课，《在改变后的基上做线性变换》小节）, $\mathbf{S^{-1}AS}$ 此即我们空间中的线性变换$\mathbf{A}$ 在特征空间中的表示。已经达到了我们的初步目的，现在我们只需要计算出它的具体形式即可。$\mathbf{S^{-1}AS=S^{-1}A}\begin{bmatrix}\vec x_1,\dots,\vec x_n\end{bmatrix}=\mathbf{S^{-1}}\begin{bmatrix} \lambda_1\vec x_1,\dots, \lambda_n \vec x_n\end{bmatrix}=\begin{bmatrix}\lambda_1 \mathbf{S^{-1}}\vec x_1,\dots,\lambda_n \mathbf{S^{-1}}\vec x_n \end{bmatrix}=\Lambda$ 可见，在我们空间中进行n 次线性变换$\mathbf{A}$， 其实就等于在特征空间中进行了 n 次线性变换 $\Lambda$。变换结束之后我们只需要把结果再次转换回我们的空间即可，即：$\mathbf{A}=\mathbf{S}\Lambda\mathbf{S^{-1}}\Rightarrow \mathbf{A^n}=\mathbf{S \Lambda^n}\mathbf{S^{-1}}$ 途径二：从矩阵分解的角度直接计算-虽然它本质上当然是属于 changing of basis的情形，但我们换了个角度思考。 $\mathbf{AS}=\mathbf{A}\begin{bmatrix}\vec x_1,\dots,\vec x_n\end{bmatrix}=\begin{bmatrix} \lambda_1\vec x_1,\dots, \lambda_n \vec x_n\end{bmatrix}=\begin{bmatrix}\vec x_1,\dots,\vec x_n\end{bmatrix}\Lambda=\mathbf{S}\Lambda$ 我们可直接得到： $\mathbf{A}=\mathbf{S\Lambda S^{-1}}$， 进而我们也可以得到 $\mathbf{A^n=S\Lambda^n S^{-1}}$. 结果与Changing of basis 的角度是一致的，分析过程也是一致的（从右到左）：先进入特征空间，在其中做变换，再转换回我们空间。 下图展示了这一过程， Ok, 实际上我们上面的整个分析过程（changing of basis），从另一个矩阵的角度引出一个概念：特征分解。 我们把矩阵$\mathbf{A}$ 分解为三个矩阵的乘积： $\mathbf{S\Lambda S^{-1}}$ ，矩阵$\mathbf{S}$ 是由特征向量组成的，中间的$\Lambda$ 是特征值组成的对角矩阵，因此叫做特征分解，也叫做矩阵的对角化。（作为对比：矩阵的QR分解or正交分解，也叫做矩阵的正交化） 相似矩阵第四课的changing of basis中，讲到了表示从 my world -&gt; new world 的 $\mathbf{B^{-1}RB}$; 还是表示从 new world -&gt; my world $\mathbf{BRB^{-1}}$， 本节课讲到的 changing of eigen-basis, $\mathbf{A=S\Lambda S^{-1}}$，区别仅仅在于changing of basis的过程中选择的basis vectors不同。线性代数中的“矩阵相似”的概念可以对基变换的这种情况做一个归纳： 对于任何可逆矩阵 $\mathbf{M}$, 那么 $\mathbf{B=M^{-1}AM}$ 与 $\mathbf{A}$ 相似。 基于上述定义可见， $\mathbf{B}$ 与 $\mathbf{A}$ 相似，那么$\mathbf{A}$ 与 $\mathbf{B}$ 也相似，因为根据定义我们可以推导出：$\mathbf{A=MBM^{-1}}$. 如果我们选择 Eigen-vectors 做我们的basis vectors，即：由特征向量组成矩阵$\mathbf{M}$, 上述过程即等价于：$\Lambda = \mathbf{S^{-1}AS}$, 我们找到了一个与$\mathbf{A}$相似的对角矩阵$\Lambda$ 。从这个意义上说，矩阵对角化就是特征基变换！ 根据相似矩阵的定义形式，我们知道其本质上就是进行了 changing of basis, 告诉我们一个空间中的变换$\mathbf{B}$ ，等同于在另一个空间中的变换 $\mathbf{A}$ ，这个转换过程是通过2组基向量完成的：$\mathbf{M,M^{-1}}$. 因此，当我们说两个矩阵$\mathbf{A,B}$相似时，我们其实再说：存在由2组不同的basis vectors 所张成的空间Space1和Space2，在Space1中矩阵$\mathbf{A}$ 所代表的线性变换，在Space2中表现为由矩阵$\mathbf{B}$ 所表示的线性变换。 同样根据相似矩阵定义和矩阵对角化的公式来看，我们知道如果一个矩阵可以对角化，那么该矩阵与此对角矩阵是相似的：$\mathbf{A}\sim \Lambda$。 既然两个矩阵（线性变换）相似，直觉上他们应该共享一些本质性的东西：特征值！证明过程如下， $$\begin{align}\mathbf{A}\vec x &amp;=\lambda \vec x \\\mathbf{AMM^{-1}}\vec x &amp;=\lambda \vec x \\\mathbf{M^{-1}AMM^{-1}} \vec x &amp;= \lambda \mathbf{M^{-1}}\vec x \\(\mathbf{M^{-1}AM})(\mathbf{M^{-1}}\vec x) &amp;= \lambda (\mathbf{M^{-1}}\vec x)\\\mathbf{B}(\mathbf{M^{-1}}\vec x) &amp;= \lambda (\mathbf{M^{-1}}\vec x)\end{align}$$ 也就是说，如果B 与 A相似，那么他们不仅仅是拥有相同的特征值，而且同一特征值对应的特征向量之间存在这样的关系：$\vec x_B =\mathbf{M^{-1}}\vec x_A $ . 这里可以这样理解：$\vec x_A$ 是在线性变换A所在空间的一个向量，通过 $\mathbf{M^{-1}}$ (space1 的基向量在space2坐标系下的表示)转换成为线性变换B所在空间的向量$\vec x_B$ . 我们知道相似矩阵其本质就是：changing of basis. 基于此，在space1 中由矩阵A 代表的线性变换 相当于 space2中由矩阵B代表的线性变换；那么容易理解，在space1中的特征向量$\vec x_A$ ，在space2中表现形式为 $\vec x_B$. 根据行列式的性质我们可知：$\det(A)=\det(B)$. 数学上我们很容易证明这个结论，但是几何上我们如何理解？ 其实，根据行列式的几何意义：the factor by which a linear transformation change any area(2-D) or volume(3-D) is called -determinant of that transformation. 我们就可以知道，由于 A 和 B 只是统一线性变换在不同空间中的表现形式，既然是同一个线性变换，那么他对空间中一个面积或者体积的改变系数是固定的，所谓万变不离其宗。因此虽然在 space 1中与 space2代表该线性变换的矩阵不一样，但该线性变换的行列式是不变的，换句话说：线性变换在space1中的“幻象”A 与其在space2中的“幻象”B 的行列式是相同的。 什么样的矩阵可以对角化有了前面的知识铺垫，我们这里项探讨一下什么样的矩阵可以对角化这个问题。 首先，根据“矩阵对角化就是特征基变换”这个结论来说，就有， 如果一个n*n的矩阵可以找到n个可以因此按大小排列的特征值$\lambda_1,\lambda_2,…,\lambda_n$ ，那么这个矩阵可以对角化。 其次，根据相似矩阵的定义和矩阵对角化的公式，我们可以知道 如果一个矩阵存在一个与之相似的对角矩阵，那么该矩阵可以对角化, i.e. $\mathbf{A}\sim \Lambda \Rightarrow \mathbf{A=S\Lambda S^{-1}}$. 最后，从特征向量的角度来说就是， 如果一个n*n矩阵存在n 个线性无关的eigen-vectors，那么它可以对角化。 这一点也可以从相似矩阵的定义和矩阵对角化公式得到启示：因此相似性定义要求特征向量矩阵 $\mathbf{S}$ 是可逆的，而$\mathbf{S}$ 的列就是特征向量，所以知道：矩阵$\mathbf{A}$ 有n个线性无关的特征向量，那么它就可以被对角化。 矩阵的正交化、对角化 有非常多的应用场景，关于这一点一定要时时复习、深入钻研！而且它们也是后续 SVD分解，QH分解的基础！ 简化版PageRank算法建模Page LinkPageRank算法是基于图（Graph）的邻接矩阵的一种算法。 如下图所示为一个图，图中每个节点代表一个网页，箭头为指向另一个网页的链接：即我们将网页之间的link关系建模为一个有向无环图（DAG）， 然后我们基于这个图（Graph or Network）构建它对应的归一化邻接矩阵，如下 注意上面的矩阵是按照列排列的：如第一列代表page A link to other pages 的情况；再用该节点所连接到其他节点的个数进行归一化，即每列components之和为1。与此同时，矩阵$\mathbf{L}$ 的行代表了该行对应的点被其他节点link to 的情况，而且这个数值也是经过连接到该节点的那些节点的out-link个数归一化的。 PageRank算法的思想是这样的： 每个网页（节点）都有一个对应的权威值（rank值） 一个网页如果被其他权威网页链接，那么它也应该权威。换句话说，一个网页的rank值是链接到它的其他网页的rank值的加权和，权重等于链接到该网页的其他网页自己的out-links数目的倒数。 PageRank与查询项无关，只是离线计算出所有web pages的权威值；为后续查询相关检索做铺垫。 现在根据前面的分析推导PageRank的公式： 我们设每个节点的rank值分别为：$r_A, r_B,r_C,r_D$, 我们以其中一个为例，$$\begin{cases}r_A &amp;= \sum_{j=1}^n L_{A,j}r_j\\r_B &amp;= \sum_{j=1}^n L_{B,j}r_j\\r_C &amp;= \sum_{j=1}^n L_{C,j}r_j\\r_D&amp;=\sum_{j=1}^n L_{D,j}r_j\end{cases}$$ 我们将上述公式写成向量形式即为，$$\begin{align}\vec r &amp;=\mathbf{L}\vec r\\\text{where, }\vec r &amp;=\begin{bmatrix}r_A\\r_B\\r_C\\r_D\end{bmatrix}\end{align}$$ 现在有两种思路来解决我们得到的上述等式： 一. 迭代法 我们现在并不知道$\vec r$ 的确定的值，我们需要给其赋予一组初始值。然后递归地计算$\vec r$,$$\vec r^{(k+1)}=\mathbf{L} \vec r^{(k)}$$直到达到我们设定的迭代次数或者停止条件（e.g. $\Vert \vec r^{(k+1)}- \vec r^{(k)} \Vert\le 0.001$）, 上述迭代过程停止计算。 二. 特征求解 我们观察公式$\vec r=\mathbf{L}\vec r$ ，符合特征值与特征向量的定义公式。其实我们要求的$\vec r$ 就是一个线性变换$\mathbf{L}$ 等于1的特征值对应的特征向量。 我们可以直接通过特征方程，求解当特征值 $\lambda=1$时其对应的特征向量 $\vec r$。这样做法的缺点是：如果矩阵很大或者矩阵的Markov 条件满足的”不太好“-e.g. 每列只有一个数为1起于全为0, 这样会带来计算过程不稳定的问题。 OK，现在最最初级的PageRank算法已经介绍完毕。 还有什么没解决的问题？ 一点改进上面介绍的PageRank算法还有很多未解决的细节问题，这里只说一个：如果一个网页只有in-links 而没有out-links 怎么办？也就是graph中有一个点，有入度，但其出度=0。 如果按照上述建模过程，直觉上我们知道：如果我们以一定概率点击这个只有in-links 而没有out-links的网页，我们就出不去了，因为这个页面内根本没有出去的超链接！ 换句话说，link matrix $\mathbf{L}$ 有一列全为0！根据我们上面提到的2种算法进行计算的时候会带来2方面的问题： 对于迭代法，会产生非常不可信的结果：这个网页的权威值会非常高，哪怕只有一个网页链接到它！！ 对于特征求解法，会增加计算不稳定的问题，e.g.（待补充！） 这个时候我们需要提供一种机制：使得我们一旦进入这个孤悬的网页之后，还要能有一定概率跳出来。$$\vec r^{(k+1)} = \lgroup p\times \mathbf{L} + (1-p)\times \frac{1}{n} \mathbf{I}\rgroup \vec r^{(k)}$$上述公式表面，我们以一定概率p遵循：“浏览一个网页时，我们下一个点击的网页只可能是这个网页提供的对外超链接”；同时我们还可能以(1-p) 的概率：“随机在浏览器中输入互联网中的任何一个网页的地址，输入每个网址的概率相等，均为：1/n.” Ok， 有了这一点改进，我们基本就有一个“靠谱一点点”的pagerank算法了。还有非常多的细节在这里不进行介绍和说明，那已经超出Linear Algebra 课程的范围了。]]></content>
      <categories>
        <category>课程笔记</category>
        <category>Mathematics for Machine Learning</category>
        <category>Linear Algebra</category>
      </categories>
      <tags>
        <tag>特征值</tag>
        <tag>特征向量</tag>
        <tag>Matrix Transformation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[四.矩阵做线性变换]]></title>
    <url>%2Farchive%2F2018-06-28%2FMatrices-make-linear-mappings%2F</url>
    <content type="text"><![CDATA[学习目标 把矩阵看成是一种操作 把做变换的矩阵与其另外一组基联系起来 基于这些transformation matrices来写代码实现这些映射 写代码找到标准正交基 矩阵是一种映射To-DO…(这部分恐怕得参考 Gilbert strang的教材和我的笔记去完善。。) 矩阵变换-&gt; 将向量映射到新的基Matrices changing basis前面讲过， columns of transformation matrix are the axis of the new basis vector of the mapping in the old coordinate. 我们现在研究的重点换一下：如何把一个向量从一个 set of basis vectors 变换到另一个 set of basis vectors。 换句话说，就是研究一个向量在不同的坐标系间如何转换？如下图， 现在做一些说明：$$\begin{align}&amp;\text{one world} \leftrightarrow \text{one set of basis vectors} \leftrightarrow \text{one coordinate}\\&amp;\text{My world}: \text{the set of basis vectors is } {\vec e_1=\begin{bmatrix}1\\ 0 \end{bmatrix}, \vec e_2=\begin{bmatrix}0\\ 1 \end{bmatrix}}.\\&amp;\text{Bear’s world}: \text{the set of basis vectors is }: {\vec e_{1}^{\prime}=\begin{bmatrix}3\\ 1\end{bmatrix}, \vec e_2^\prime=\begin{bmatrix}1\\ 1\end{bmatrix}}.\text{(in my coordinate)}\end{align}$$上面的话说的意思是：在我的眼里，看bear’s basis vectors 是$\begin{bmatrix}3\\1\end{bmatrix}, \begin{bmatrix}1\\1\end{bmatrix}$, 但是在bear 的眼里看它自己的basis vectors 是$\begin{bmatrix}1\\0\end{bmatrix},\begin{bmatrix}0\\1\end{bmatrix}$. 这很好理解，因为所谓基张成整个space的意思就是，基是整个space 的基础度量。空间中任何向量（点）都是由基向量线性组合而成的，其大小、方向 都是相对于basis vectors来讲的，因此 basis vector 对于同一空间中的其他向量来说就是相当于”unit vector”. 我们的问题： 在 Bear’s world 有一个向量 $\color{blue}{\vec v_b=\frac{1}{2}\begin{bmatrix}3\\1\end{bmatrix}}$, 求他在My world 中的坐标（表示）$\color{red}{\vec v_{my}}$是什么？ 在 My world 中的向量 $\color{red}{\vec v_{my}}$ 在 Bear’s world 中是什么？ 根据之前的说明，在My world 的坐标系下，Bear‘s transformation matrix 是 $\mathbf{B}=\begin{bmatrix}3 &amp; 1\\1&amp;1\end{bmatrix}$. 因此 $\begin{bmatrix}3&amp;1\\1&amp;1\end{bmatrix}\color{blue}{\vec v_b}=\begin{bmatrix}3&amp;1\\1&amp;1\end{bmatrix} \color{blue}{\begin{bmatrix}\frac{3}{2}\\ \frac{1}{2} \end{bmatrix}}=\color{red}{\begin{bmatrix}5\\2\end{bmatrix}}=\color{red}{\vec v_{my}}$ 上式把Bear’s world 的向量变换到 My world. 我们现在考虑反方向的变换：如何把一个My world 的向量变换到Bear’s world? 与之前类似（Bear‘s world的基向量在我们坐标系的表示$\mathbf{B}$），对应地就需要My world 的基向量在Bear’s world的表示（坐标））. 我们之前使用$\mathbf{B}$ 把一个 Bear’s world 的向量变换到 My world中，我们现在要进行一个逆过程，自然是使用$\mathbf{B}^{-1}$. $\mathbf{B}^{-1}\color{red}{\vec v_{my}}=\mathbf{B}^{-1}\mathbf{B}\color{blue}{\vec v_b}=\color{blue}{\vec v_{b}}$ 即： $\frac{1}{2}\begin{bmatrix}1&amp;-1\\ -1&amp;3 \end{bmatrix}\color{red}{\begin{bmatrix}5\\2\end{bmatrix}}=\color{blue}{\begin{bmatrix}\frac{3}{2}\\ \frac{1}{2}\end{bmatrix}}$ 其实，我们只是回答了如何将我们coordinate 中的一个vector 如何变换到 Bear’s coordinate，并没有回答上面的$\mathbf{B}^{-1}$ 其本质是什么？ 根据我们之前讲过的内容，我们知道 $\mathbf{B}^{-1}$ 是一个transformation matrix, 它的列$\begin{bmatrix}\frac{1}{2}\\ -\frac{1}{2}\end{bmatrix}，\begin{bmatrix}-\frac{1}{2}\\ \frac{3}{2}\end{bmatrix}$ 是basis vectors，我们只是不知道它是谁的basis vectors 在什么coordinate 下？ 其实，$\color{red}{\vec v_{my}}$ 是我们坐标系下的向量，最终的$\color{blue}{\vec v_b}$ 是Bear‘s coordinate 下的向量，由此可见$\mathbf{B}^{-1}$ 是 My basis in Bear’s world! 可总结为如下图所示的关系: Changing of basis matrix changes bear’s basis into my coordinate, Then you inverse the changing of basis matrix, this changes my basis into bear’s coordinate(columns of this inverse of changing of basis matrix are my basis vectors in bear’s coordinate), 一种特殊的基变换 如果 basis of my world 以及 basis of bear’s world 都是正交的，我们可以使用projection 来做 changing of basis. ⚠️， 上图中的basis vectors 都是 unit vector。在实际计算中记得做 normalization。（参考：二.向量是空间中移动的对象一课中 基变换-&gt;坐标系统 一节的内容） 在改变后的基上做线性变换这节要回答的问题是：在 My world 中做一个linear transformation such as rotation, 在 Bear‘s world 中是如何看待这个变换的？过程如下图， 现在对上述问题具体化: 已知 Bear’s basis in My world is: $\mathbf{B}=\begin{bmatrix}3&amp;1\\1&amp;1\end{bmatrix}$. 在 Bear’s world中的一个向量 $\vec v_b = \begin{bmatrix}x\\y\end{bmatrix}$. 我们不知道这个transformation matrix 在 Bear’s world中是什么样子，但是我们知道在 My world 中的这个变换是$\mathbf{R}，$ e.g. for rotation，$\mathbf{B}=\begin{bmatrix}\end{bmatrix}$ 问题：这个transformation matrix 在 Bear’s world 中是什么样子？或者等价问题，The vector in bear’s world is transformed by the transformation(described in my world), what does it look like in bear’s world? 解决方案如下， 首先$\mathbf{B}$ 将在 Bear’s world 中的向量 $\color{blue}{\vec v_b=\begin{bmatrix}x\\y\end{bmatrix}}$ 转换成 My world中的向量$\color{red}{\vec v_{my}}$。这是前面讲过的内容。 然后用transformation matrix in My world $\mathbf{R}$ 来对$\color{red}{\vec v_{my}}$ 做 transform，得到 transformed version of $\color{red}{\vec v_{my}}$, 记为：$\color{red}{\vec v_{my}^{transformed}}$. 再用 My basis in Bear’s world- Inverse of Bear’s transformation matrix $\mathbf{B}$ , i.e. $\mathbf{B}^{-1}$，to transform the $\color{red}{\vec v_{my}^{transformed}}$ back to Bear’s world, get $\color{blue}{\vec v_{b}^{transformed}}$. 上述三个过程可总结为(从右向左看下述公式)，$$\color{blue}{\vec v_{b}^{transformed}}=\mathbf{B}^{-1}\color{red}{\mathbf{R}}\mathbf{B}\color{blue}{\vec v_b}\\\color{red}{\text{Analogy: Considering the same tranformation/matrix, }\mathbf{R} \text{ for My world is in analogy with }\\ \mathbf{B^{-1}RB } \text{ for Bear’s world, means :}} \\\mathbf{R}\vec v_{my} =\vec v_{my}^{transformed}\\(\mathbf{B^{-1}RB})\vec v_b=\vec v_b^{transformed}\\$$所以，the same transformation in Bear’s world is: $\mathbf{B^{-1}RB}​$. 下图总结这个过程总结的很好，采用拿来主义: 更加形象、精彩的解释在这里（需要翻墙）。 总结： $\mathbf{B^{-1}RB}$ do the transformation translation from my world into the world of the new basis system. 它就是我们空间中的变换 $\mathbf{B}$ 在新空间中的存在形式。 怎么确定基变换的方向？我们知道 $\mathbf{B^{-1}RB}$ 使得我们空间中的变换$\mathbf{R}$ 可以在另一个空间体现，它的作用是：对另一个空间中的向量（在该空间坐标系下的表示），通过$\mathbf{B}$（-另一个空间的基在我们坐标系下的表示）将其转换成我们空间坐标系下的表示，然后施以我们空间中的变换$\mathbf{R}$ ，最后通过$\mathbf{B^{-1}}$(我们空间的基在另一个空间坐标系下的表示)转换回另一个空间。简而言之上述公式回答了： 我们空间有一个变换$\mathbf{R}$, 其在另一个空间中是何种存在。 可见，上述对于公式起到了一个“翻译”作用，是:my world -&gt; a new world 那如果我们需要从: new world-&gt; my world 的“翻译”过程呢？ 使用的公式为：$\mathbf{BRB^{-1}}$ （注意，两个公式中各个字母的含义根据不同描述可能有所变化！） 具体问题是这样的：现有一个我们空间的向量, 在我们坐标系下的表示为 $\vec v_{my}$, 现在我们像对其做一个变换-比如reflection。 但是我们发现直接计算，复杂度太大。我们发现在其他空间下这个变换的形式很简单，为$\mathbf{R}$，便于计算。因此我们需要将向量$\vec v_{my}$ 变换到另一个空间，然后使该空间的变换 $\mathbf{R}$ 来作用于它，然后将结果再次变换到我们坐标系下，从而达到了我们最初的目的。 我们把另一个空间代称为:Bear’s world, 计算过程如下$$\begin{align}&amp; \mathbf{B^{-1}}\vec v_{my} =\vec v_{b}\\&amp; \mathbf{R}\vec v_b = \vec v_b^{transformed} \\&amp; \mathbf{B}\vec v_b^{transformed} = \vec v_{my}^{transformed}\end{align}$$ 总结起来即为：$$\color{red}{\vec v_{my}^{transformed}}=\mathbf{B}\color{blue}{\mathbf{R}}\mathbf{B^{-1}}\color{red}{\vec v_{my}}$$ 现在问题来了？什么时候我们需要: my world-&gt; new world, 什么时候需要 new world-&gt; my world? 决定的标准在于计算的便利性。哪个路径方便于我们计算，我们就采用相应的基变换的方式，在那个空间下计算，然后将计算结果再变换回来即可。 变换是否可逆基-&gt;正交基 Matrix: 矩阵的列是“一组新的基向量” 在我们坐标系中的表示。如果矩阵的列两两正交，那么由他的列所构成的基就是正交基，如果这些列还是单位向量，那么这个矩阵的列就构成了一组标准正交基。这个矩阵叫做正交矩阵。 正交矩阵一定可逆！ $\mathbf{U}$ is a orthogonal matrix, then $\mathbf{UU^{T}=U^TU=I}$, 也就意味着 $\mathbf{U^{-1}=U^T}$ $\det(\mathbf{U})=\pm 1$ , 因为正交矩阵每一列都是 unit vector, 没有改变 the volume/area of the space.(但是可能改变了方向，因此行列式可能是-1) 正交矩阵还有一个可贵的性质：正交变换不改变向量的2-范数, i.e. $\Vert \mathbf{U}\vec x\Vert =\Vert \vec x \Vert$.$$\begin{align}\Vert \mathbf{U}\vec x\Vert ^2 &amp;= (\mathbf{U}\vec x)^T(\mathbf{U}\vec x)\\&amp;=\vec x^T \mathbf{U}^T \mathbf{U}\vec x\\&amp;=\vec x^T\vec x \\&amp;=\Vert \vec x \Vert^2\end{align}$$ 正交化Gram-schmit Process 标准正交基有很多优良的性质： 我们可以方便的用projection做线性变换 以这些基为列构成的矩阵-正交矩阵，有很多好的性质比如：$\det(\mathbf{U})=1,\mathbf{U^{-1}=U^T}$ 因此，当我们面对一组仅仅是线性无关的vectors（可以作为某个space 的基）时，我们希望能够将这组linear independent vectors 转换成 orthogonal vectors. 其流程如下， 线性无关 ${v_1,v_2,…,v_n}$ -&gt; 正交 ${v_1^\prime,v_2^\prime …,v_n^\prime}$ -&gt; 标准正交 ${\frac{v_1^\prime}{\Vert v_1^\prime \Vert},…,\frac{v_n^\prime}{\Vert v_n^\prime \Vert} }$, 表示为${e_1,e_2,..,e_n}$ 正交化是核心。 Gram-schmit Process:$$\begin{align}&amp;v_1^\prime = v_1\\&amp;v_2^\prime = v_2 - \text{Proj}_{v_1^\prime} v_2 = v_2 - \frac{v_2\cdot v_1^\prime}{\Vert v_1^\prime\Vert ^2}v_1^\prime \\&amp;v_3^\prime = v_3 - \text{Proj}_{v_1^\prime}v_3 - \text{Proj}_{v_2^\prime}v_3= v_3 - \frac{v_3\cdot v_1^\prime}{\Vert v_1^\prime \Vert ^2} v_1^\prime - \frac{v_3\cdot v_2^\prime}{\Vert v_2^\prime \Vert ^2} v_2^\prime \\&amp;…\end{align}$$ Then we normalize the set of orthogonal vectors:$$\begin{align}&amp;e_1 = \frac{v_1^\prime}{\Vert v_1^\prime \Vert} \\&amp;e_2 = \frac{v_2^\prime}{\Vert v_2^\prime \Vert}\\&amp;… \\&amp;e_n = \frac{v_n^\prime}{\Vert v_n^\prime \Vert}\end{align}$$ 最后得到的向量集: ${e_1,e_2,…,e_n}$ 就是标准正交基。 小插曲 我们观察(向量)投影公式： $\text{Proj}_{\vec v}\vec u=\frac{\vec u \cdot \vec v}{\Vert \vec v\Vert ^2}\vec v$ 分母中有一个 $\Vert \vec v \Vert ^2 = \vec v^T \vec v$ ,分子中的$\vec u \cdot \vec v = \vec v ^T\vec u$ 那么$$\begin{align}\text{Proj}_{\vec v} \vec u &amp;= \frac{\vec v ^T\vec u}{\vec v ^T \vec v} \vec v \\&amp;=\vec v \frac{\vec v ^T\vec u}{\vec v ^T \vec v} \text{ (scalar multiplication)}\\&amp;=\frac{\vec v \vec v ^T}{\vec v ^T \vec v}\vec u \text{ (associativeness of matrix multiplication)}\end{align}$$我们把上面公式最后一项中的单独看，$$\mathbf{P}=\color{blue}{\frac{\vec v \vec v ^T}{\vec v ^T \vec v}}$$$\mathbf{P}$ 称为将 $\vec u$ 向 $\vec v$ 方向投影的 Projection Matrix，这就将投影从几何概念提升到了线性变换的层次。也即是说，在线性变换的概念中，一个向量的投影是什么呢？就是经过某个 Projection Matrix 变换之后的结果。（当然，这是针对将向量向另一个向量投影的情况，此外还有将向量向一个平面或超平面投影的情况，那个时候projection matrix 的概念是类似的，不过将公式中的向量 $\vec v$ 换成了一个矩阵$\mathbf{A}$, 那是的$\mathbf{P=A(A^TA)^{-1}A^T}$, 其作用是将向量投影到矩阵$\mathbf{A}$ 的列空间） 实际上，我们通过Gram-Schmit Process 把一个矩阵正交化，等同于对矩阵做分解:$$\mathbf{A=QR}$$其中$\mathbf{Q}$ 是一个正交矩阵。]]></content>
      <categories>
        <category>课程笔记</category>
        <category>Mathematics for Machine Learning</category>
        <category>Linear Algebra</category>
      </categories>
      <tags>
        <tag>Linear Algebra</tag>
        <tag>Matrix</tag>
        <tag>Linear Mapping</tag>
        <tag>Transformation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三.线性代数中的矩阵]]></title>
    <url>%2Farchive%2F2018-06-24%2FMatrices-in-Linear-Algebra%2F</url>
    <content type="text"><![CDATA[学习目标 理解什么是矩阵以及矩阵如何对应一个变换 解释并计算矩阵的逆和行列式 判断矩阵是否可逆 矩阵：对向量的操作矩阵、向量及线性方程组考虑下面方程组，$$\begin{align}2x_1+3x_2=8\\10x_1+x_2 = 13\end{align}$$我们可以将上述方程写成矩阵的形式，$$\begin{bmatrix}2 &amp;3 \\10 &amp; 1\end{bmatrix}\begin{bmatrix}x_1\\x_2\end{bmatrix}=\begin{bmatrix}8 \\13\end{bmatrix}$$我们泛化上述公式，$$\color{red}{\mathbf{Ax=b}}$$矩阵乘以向量可以从两种角度看待, 向量作用于矩阵：是矩阵列向量的 linear combination, with the components of $x$ as the corresponding weight. The result of linear combination of the columns of $\mathbf{A}$ is $\mathbf{b}$. 矩阵作用于向量：matrix $\mathbf{A}$ transform the vector $\mathbf{x}$ to output $\mathbf{b}$. Say, $\mathbf{A}$ maps $\mathbf{x}$ to $\mathbf{b}$ or $\mathbf{x}$ is transformed by $\mathbf{A}$ to be $\mathbf{b}$. 我们现在侧重第二种角度。 回到之前方程组对应的矩阵形式，我们要问:$$\text{what vector }\begin{bmatrix}x_1\\x_2\end{bmatrix}\text{could be transformed by}\begin{bmatrix}2 &amp; 3 \\10 &amp; 1\end{bmatrix}\text{to output }\begin{bmatrix}8\\13\end{bmatrix}$$我们先看看矩阵对标准向量做了变换会得到什么，$$\begin{align}\mathbf{Ae_1}=\begin{bmatrix}2 &amp; 3\\10 &amp; 1\end{bmatrix}\begin{bmatrix}1\\0\end{bmatrix}&amp;=\begin{bmatrix}2\\10\end{bmatrix}=\mathbf{e_1}’ ,\\\mathbf{Ae_2}=\begin{bmatrix}2 &amp; 3\\10 &amp; 1\end{bmatrix}\begin{bmatrix}0\\1\end{bmatrix}&amp;=\begin{bmatrix}3\\1\end{bmatrix}=\mathbf{e_2}’\end{align}$$ 上图展示了变换的结果，矩阵$\mathbf{A}$ 对标准正交基做了变换： $\mathbf{A}$ map ${\mathbf{e_1,e_2}}$ to ${\mathbf{e_1^\prime,e_2^\prime}}$. Matrix transform the basis vectors, so it changes the space. 我们可以从这样的视角看待矩阵： Matrix is a function which acts on input vectors and generate other output vectors. 矩阵转换向量，向量被矩阵转换。 实际上所谓线性代数，不过是对空间中向量进行操作的一个系统。 矩阵如何对空间进行变换上小节内容讲明了一个矩阵如何对标准正交基做变换- each axis is transformed by the corresponding column of matrix. why? tips: Basis is orthonormal， hence the there is 0 in the basis vectors. 根据矩阵乘法的定义，只有矩阵对应列才会最终影响到transformed basis 对应的axis（component）。 任何向量都可以由基的线性组合表示，因此矩阵对一个向量变换的结果也同样是基向量的线性组合。这意味着， 我们空间的格子线保持平行和等间隔 它们可能被拉升，但原点位置没变，空间没有被弯曲 上述结论是向量加法和数乘操作的结果。 下面我们给出一些矩阵乘法的性质，并结合这些性质来展示矩阵是如何对空间做变换的：$$\begin{align}\mathbf{A}\vec r &amp;= \vec r ^\prime \\\mathbf{A}(n\vec r )&amp;= n\vec r ^\prime\\\mathbf{A}(\vec r+\vec s) &amp;= \vec r ^\prime +\vec s^\prime \\\color{red}{\mathbf{A}(n\vec e_1 +m\vec e_2)}&amp;\color{red}{= n\vec e_1 ^\prime + m\vec e_2^\prime }\end{align}$$从上述公式有如下结论 basis vecotrs is transformed：$e_1,e_2$ move to $e_1^\prime,e_2^\prime$. Vector $\vec r$ is transformed to be $\vec r^\prime$. $\vec r$ is a linear combination of $e_1,e_2$ with some weights, $\vec r^\prime$ is a also linear combination of $e_1^\prime,e_2^\prime$ with some weights. But,the weights in original space is the same as the weights in the transformed space.(see last equation), and the weight is the components of vector $\vec r$ : $n,m$. $\color{blue}{\text{First column of }\mathbf{A} =\vec e_1^\prime, \text{Second column of }\mathbf{A}=\vec e_2^\prime}$ 看一个具体的例子 $\mathbf{A}\vec r =\vec r^\prime$at the same time as $\mathbf{A}{\vec e_1,\vec e_2}={\vec e_1^\prime,\vec e_2^\prime}$ ， $$\begin{align}\begin{bmatrix}2 &amp; 3\\10 &amp; 1\end{bmatrix}\begin{bmatrix}3\\2\end{bmatrix}=\begin{bmatrix}12\\32\end{bmatrix}&amp;\Rightarrow \\\begin{bmatrix}2 &amp; 3\\10 &amp; 1\end{bmatrix}(3 \begin{bmatrix}1\\0\end{bmatrix} + 2\begin{bmatrix}0\\1\end{bmatrix})&amp;= 3 (\begin{bmatrix}2 &amp; 3\\10 &amp; 1\end{bmatrix}\begin{bmatrix}1\\0\end{bmatrix})+2(\begin{bmatrix}2 &amp; 3\\10 &amp; 1\end{bmatrix}\begin{bmatrix}0\\1\end{bmatrix})\\&amp;=3\begin{bmatrix}2\\10\end{bmatrix}+2\begin{bmatrix}3\\1\end{bmatrix}\\&amp;=\begin{bmatrix}12\\32\end{bmatrix}\end{align}$$ 验证了我们之前的结论，矩阵乘法的结果可以看作是new basis vector i.e. transformed basis vector 的加权和，矩阵对向量的变换过程改变了basis vectors，但保持权重不变。 $$\color{green}{\begin{align}&amp;\text{Means: Any vector } \vec r\ or\ \vec s \text{ could be represented as }{n\vec e_1+m\vec e_2}, \text{after the vector is transformed by matrix }\mathbf{A}\\&amp;\text{it could be represented as } {ne_1^\prime+me_2^\prime}\end{align}}$$之前我们说过矩阵乘向量可以从两种角度来看，殊途同归, 我们以 $\mathbf{Ax=b}$为例 ：$$\begin{align}&amp;\text{Linear combination of columns of matrix } \mathbf{A}:\\&amp;\mathbf{Ax}=\mathbf{A}\begin{bmatrix}x_1 \\x_2\end{bmatrix}=x_1\text{(1st column of }\mathbf{A})+x_2(\text{2nd column of }\mathbf{A})\\&amp;\text{Matrix operation on vectors } \mathbf{A}:\\&amp;\mathbf{Ax} =\mathbf{A}\begin{bmatrix}x_1 \\x_2\end{bmatrix}= \mathbf{A}(x_1\begin{bmatrix}1\\0\end{bmatrix}+x_2\begin{bmatrix}0\\1 \end{bmatrix} )=x_1(\mathbf{A}\begin{bmatrix} 1\\0 \end{bmatrix})+x_2(\mathbf{A}\begin{bmatrix}0\\1\end{bmatrix} )\\&amp;= x_1(\color{blue}{\mathbf{A}\vec e_1}) + x_2(\color{blue}{\mathbf{A}\vec e_2})\longrightarrow \color{blue}{\text{Matrix operation on vectors: basis transformation}} \\&amp;= x_1\color{blue}{\vec e_1^\prime} + x_2\color{blue}{\vec e_2^\prime}\\&amp;= x_1(\color{red}{\text{1st column of matrix }\mathbf{A}})+x_2(\color{red}{\text{2nd column of matrix }\mathbf{A}}) \longrightarrow \color{red}{\text{Linear combination}}\end{align}$$ 矩阵变换的类型 transform basis $\Leftrightarrow$ transform all the vectors $\Leftrightarrow$ transform the space. 介绍一些特殊的矩阵，看看他们能对basis-向量-space做什么变换。 Identifity Matrix $\begin{bmatrix}1&amp;0 \\ 0&amp;1 \end{bmatrix}\begin{bmatrix}x\\y \end{bmatrix}=\begin{bmatrix}x \\ y \end{bmatrix}$ 没有对向量作出任何改变，也没有改变basis vector Diagonal Matrix $\begin{bmatrix}3&amp;0 \\ 0&amp;2 \end{bmatrix}\begin{bmatrix}x\\y \end{bmatrix}=\begin{bmatrix}3x \\ 2y \end{bmatrix}=x\begin{bmatrix}3\\0 \end{bmatrix}+y\begin{bmatrix}0\\2 \end{bmatrix}$ 改变了basis vectors：拉长了basis vectors，当然也拉长了空间所有的vector，拉长了空间。如下图所示， $\begin{bmatrix}-1&amp;0 \\ 0&amp;2 \end{bmatrix}\begin{bmatrix}x\\y \end{bmatrix}=\begin{bmatrix}-x \\ 2y \end{bmatrix}=x\begin{bmatrix}-1\\0 \end{bmatrix}+y\begin{bmatrix}0\\2 \end{bmatrix}$ 改变了basis vectors: flip the first basis vector,and strech the second basis vector, 当然其作用于全部向量（作用于the space，反转了第一个axis，拉升了第二个axis）。 $\begin{bmatrix}-1&amp;0 \\ 0&amp;-1 \end{bmatrix}\begin{bmatrix}x\\y \end{bmatrix}=\begin{bmatrix}-x \\ -y \end{bmatrix}=x\begin{bmatrix}-1\\0 \end{bmatrix}+y\begin{bmatrix}0\\-1 \end{bmatrix}$ 将两个axis 旋转$180^\circ$ 且没有scale（保持空间中所有向量norm/长度不变），叫做“inversion”。新旧space关于原点对称。 以上2种情况如下图所示， Mirror Matrix $\begin{bmatrix}0&amp;1 \\ 1&amp;0 \end{bmatrix}\begin{bmatrix}x\\y \end{bmatrix}=\begin{bmatrix}y \\ x \end{bmatrix}=x\begin{bmatrix}0\\1 \end{bmatrix}+y\begin{bmatrix}1\\0 \end{bmatrix}$ 将basis vectors沿着$45^\circ$ 直线对折，将第一个axis逆时针旋转$45^\circ$, 第二个axis顺时针旋转$45^\circ$ . $\begin{bmatrix}0&amp;-1 \\ -1&amp;0 \end{bmatrix}\begin{bmatrix}x\\y \end{bmatrix}=\begin{bmatrix}-y \\ -x \end{bmatrix}=x\begin{bmatrix}0\\-1 \end{bmatrix}+y\begin{bmatrix}-1\\0 \end{bmatrix}$ 将第一个axis顺时针旋转$90^\circ$,将第二个axis 逆时针旋转$90^\circ$ ，将 basis vectors 沿$135^\circ$ 线对折，新旧space沿着$135^\circ$ 线堆成。 $\begin{bmatrix}-1&amp;0 \\ 0&amp;1 \end{bmatrix}\begin{bmatrix}x\\y \end{bmatrix}=\begin{bmatrix}-x \\ y\end{bmatrix}=x\begin{bmatrix}-1\\0 \end{bmatrix}+y\begin{bmatrix}0\\1 \end{bmatrix}$ 将第一个axis inverse，第二个axis不变，将第一个basis vector inverse，第二个basis不变。变换后的space 与之前的space 沿着第二个axis对称（关于y轴对称）。 $\begin{bmatrix}1&amp;0 \\ 0&amp;-1 \end{bmatrix}\begin{bmatrix}x\\y \end{bmatrix}=\begin{bmatrix}x \\ -y \end{bmatrix}=x\begin{bmatrix}1\\0 \end{bmatrix}+y\begin{bmatrix}0\\-1 \end{bmatrix}$ 保持第一个axis不变，将第二个axis旋转$180^\circ$. 新旧space 沿着第一个axis对称（关于x轴对称）。 以上几种沿轴对称的情况如下图， Shear Matrix $\begin{bmatrix}1&amp;1 \\ 0&amp;1 \end{bmatrix}\begin{bmatrix}x\\y \end{bmatrix}=\begin{bmatrix}x+y \\ y \end{bmatrix}=(x+y)\begin{bmatrix}1\\0 \end{bmatrix}+y\begin{bmatrix}0\\1 \end{bmatrix}$ 没改变第一个axis, 第二个axis顺时针旋转$45^\circ$. 如下图， Rotation Matrix $\begin{bmatrix}0&amp;-1 \\ 1&amp;0 \end{bmatrix}\begin{bmatrix}x\\y \end{bmatrix}=\begin{bmatrix}-y \\ x \end{bmatrix}=x\begin{bmatrix}0\\1 \end{bmatrix}+y\begin{bmatrix}-1\\0 \end{bmatrix}$ 空间逆时间旋转$90^\circ$, 如图， 矩阵变换的组合上小节讲了通过矩阵进行 stretch、mirror、shear、rotation 等变换，本节考虑将这些对空间的变换进行组合时的情况，e.g. 先对向量 rotation，然后shear, etc. 初始量： $\mathbf{A_2}\mathbf{A_1}\vec r=\mathbf{A_2}(\mathbf{A_1}\vec r)=(\mathbf{A_2}\mathbf{A_1})\vec r$ Let $\mathbf{A_1}=\begin{bmatrix}0&amp;1\\-1&amp;0\end{bmatrix}$,$\mathbf{A_2}=\begin{bmatrix}1&amp;1\\0&amp;1\end{bmatrix}$ Basis vectors：$\vec e_1=\begin{bmatrix} 1\\0\end{bmatrix}$, $\vec e_2 = \begin{bmatrix}0\\1 \end{bmatrix}$ $\vec r=\begin{bmatrix}n\\m \end{bmatrix}=n\vec e_1+m\vec e_2=n\vec e_1^\prime+m\vec e_2^\prime=n\vec e_1^{\prime\prime}+m\vec e_2^{\prime\prime}$ 变换过程 $$\begin{align}&amp;\mathbf{A_1}\vec e_1=\vec e_1^\prime=\begin{bmatrix}0\\-1\end{bmatrix},&amp;&amp;\mathbf{A_1}\vec e_2=\vec e_2^\prime = \begin{bmatrix} 1\\0\end{bmatrix}\\&amp;\mathbf{A_2}(\mathbf{A_1}\vec e_1)=\mathbf{A_2}\vec e_1^\prime=\vec e_1^{\prime\prime}=\begin{bmatrix}-1\\-1 \end{bmatrix}, &amp;&amp;\mathbf{A_2}(\mathbf{A_1}\vec e_2)=\mathbf{A_2}\vec e_2^{\prime}=\vec e_2^{\prime\prime}=\begin{bmatrix}1\\0 \end{bmatrix}\end{align}$$ 从上面过程就可以得到，$\mathbf{A_1A_2}=\begin{bmatrix}-1&amp;1\\-1&amp;0 \end{bmatrix}$，其过程图示如下， Note！： $\mathbf{A_1A_2}\vec r \ne \mathbf{A_2A_1}\vec r$, 矩阵变换的顺序是不能变的！ Summary: matrix is linear transformation matrix composition means combination of lienar transformation linear transformation is also function so, matrix composition is also function composition which is much related with calculus. matrix multiplication is in fact is combination of linear transformation. 逆矩阵高斯消元解决线性方程组：$\mathbf{Ax=b}$ Gaussian-Jordan Elimination$$[\mathbf{A|b}] \stackrel{\text{Gaussian Elimination}}\longrightarrow [\mathbf{U\vert c}] \stackrel{\text{Jordan Elimination}}\longrightarrow [\mathbf{I|x}]$$ 从高斯消元到逆矩阵逆矩阵$$\text{Given a square matrix }\mathbf{A}, \text{if and only if there exist a matrix }\mathbf{A}^{-1} \text{satisfy}:\\\mathbf{AA^{-1}}=\mathbf{A^{-1}A}=\mathbf{I}\\\text{The matrix }\mathbf{A^{-1}} \text{is refered as the Inverse of }\mathbf{A}.$$高斯消元–解方程—求逆矩阵 我们知道高斯消元是用来解方程的，其实，他也同样是求解逆矩阵的过程，理由如下， 对于矩阵乘法有，$$\mathbf{AB=A[b1\ \ b2\ \ b3]}=[\mathbf{Ab_1\ \ Ab_2\ \ Ab_3}]=\mathbf{[(AB)_1\ \ (AB)_2\ \ (AB)_3]}$$将上述结论带入逆矩阵的定义我们有，$$\mathbf{AA^{-1}=A[c_1\ \ c_2\ \ c_3]=[Ac_1\ \ Ac_2\ \ Ac_3]=I}$$因此$$\mathbf{[Ac_1]}=\begin{bmatrix}1\\0\\0\end{bmatrix}, \mathbf{[Ac_2]}=\begin{bmatrix}0\\1\\0\end{bmatrix},\mathbf{[Ac_3]}=\begin{bmatrix}0\\0\\1\end{bmatrix},$$其实上面是3个 $\mathbf{Ax=b}$ 形式的方程，根据高斯消元我们可以求得：$\mathbf{c_1,c_2,c_3}$。也就得到$\mathbf{A^{-1}=[c_1\ \ c_2\ \ c_3]}$. 实际上我们可以合并解三个方程组的过程，一次得到 $\mathbf{A^{-1}}$,$$\mathbf{[A|I]}\stackrel{Gauss Elimination} \longrightarrow \mathbf{[I|A^{-1}]}$$只有方阵才有逆矩阵的概念 用逆矩阵解决线性方程$\mathbf{Ax=b}\Rightarrow \mathbf{x=A^{-1}b}$ 用逆矩阵解方程的前提是：系数矩阵$\mathbf{A}$ 的逆矩阵得存在。 行列式什么是行列式？ 根据前面的内容知道： Linear transformation change the space. 线性变换改变了空间，改变了多少？这是由determinant 回答的。 几何上直观理解什么是行列式：The factor by which a linear transformation change any area(2-D) or volume(3-D) is called -determinant of that transformation.一般的二阶行列式$\begin{vmatrix}a &amp; b\\c &amp; d\end{vmatrix}=ab-cd$其几何意义如下图，前面讲到matrix composition，也即：transformation combination，其是就是复合函数的概念。 前面讲的行列式，针对单个变换，是刻画 transformation/matrix 对空间改变的程度的量。那么行列式对于组合变换呢？即：$\mathbf{M_1M_2}$ 这个组合变换的determinant 是什么呢？其几何意义是什么？$\det(\mathbf{M_2M_1})=\det (\mathbf{M_2})\det (\mathbf{M_1})$其可以看作是在$\mathbf{M_1}$ 对空间变换的基础上，用 $\mathbf{M_2}$ 对中间态空间再做一次变换。其中第一次变换的结果对第二次变换来讲，可以看作 “unit square”. 行列式和逆前面讲到，行列式衡量了一个线性变换对 area or volume of a region in the space 改变的程度（系数），那如果这个系数为0 呢？ 如下图， 形式化定义TO-DO… 总结 行列式只针对方阵，是线性变换相伴随的量。No Linear transformation, No determinant. 行列式是线性变换对volum of space 改变的系数, 衡量改变大小。 行列式是一个scalar, 有正负，体现线性变换对space改变的oritiention。 行列式为0的意思是：the corresponding linear transformation squash the volume or are of any region of the space into 0,say a plane, a line or extreme case, a single point. 因此，矩阵是singular, 当然不可逆。]]></content>
      <categories>
        <category>课程笔记</category>
        <category>Mathematics for Machine Learning</category>
        <category>Linear Algebra</category>
      </categories>
      <tags>
        <tag>Linear Algebra</tag>
        <tag>Matrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二.Learning to Answer Yes or No]]></title>
    <url>%2Farchive%2F2018-06-23%2FLearning-to-answer-yes-or-no%2F</url>
    <content type="text"><![CDATA[Perceptron Hypothesis SetReview of previous lecture上节课程讲过，机器学习的整个架构可以用下图表示 核心部分是：Learning algorithm $A$ take $\mathcal{D}$ and $\mathcal{H}$ as input, to generate a concrete hypothesis $g$ which could make prediction on $\mathbf{x}_{new}$ . 换句话说，学习算法(A)的输入有两个： Hypothesis set: $\mathcal{H}$ Training Data: $\mathcal{D}$ 学习算法的目的就是：在hypothesis set 中诸多的 hypothesis（funcations）中选择出来一个最好的作为我们最终想要的结果，这个选择的过程依赖于对于数据集 $\mathcal{D}$ 的利用。数据集是事先准备好的，无需多虑。 那么现在剩下的问题是：1. 如何确定 Hypothesis set 的具体形式（Hypothesis set 到底长什么样子？）2. 我们怎么在其中选出“最好”的 $g: \mathcal{X}\rightarrow\mathcal{Y}$ 这也是本节课的内容重点. A Simple Hypothesis Set: Perceptron这个世界上有很多种类 Hypothesis Set，我们需要从这么多种类中选择其中一类，来作为 Learning Algorithm 的基础。选择结果依赖于： 人类的先验知识（我偏执地想用线性模型，自然不会选择那些非线形的 Hypothesis set） e.g. NNs, 所谓非线性的 Hypothesis Set 是指：这个set里的每个function 都是非线性函数。 问题的特性 Sequential Labelling Vs. Multi-class ，我认为前者难度较高，我需要一个复杂的函数才能够fit data well, 因此我就会选择一个复杂的 Hypothesis Set. 数据的特质和大小 如果数据量比较小，那么我只需要一个简单的函数，因此我选择一个simple 的hypothesis set, 这样可以避免过拟合；如果数据十分大，这就使得我们有可能训练比较复杂的Hypothesis Set，从而得到一个非常powerful 的 function。 计算资源的限制 e.g. 数据流极大并且计算资源丰富，我需要非常复杂但是十分强大的function，因此我可能会选择NNs作为我的 Hypothesis Set. 本节课先介绍一个简单的（诸多种类中的一种）Hypothesis Set：Perceptron-一种2元-线性分类器。 Algorithm:$\text{For } \mathbf{x}=\lbrace x_1,x_2,…,x_d \rbrace(\text{feature vector}),\text{compute a }\color{red}{ weighted \ score} \text{ and}$$$\text{let } y=+1, \text{ if }\sum_{i=1}^d w_ix_i &gt; threshold \\\text{let } y=-1, \text{ if }\sum_{i=1}^d w_ix_i &lt; threshold\\$$ $y\in {+1(\text{good}),-1(\text{bad})}-$ linear formular $h\in \mathcal{H}$ is:$$ \color{red}{h(\mathbf{x})=sign(\sum_{i=1}^d w_ix_i - \text{threshold})}$$ $h(\mathbf{x})$ 称为 “perceptron hypothesis”. 一个小问题：根据上述公式，我们的$h(\mathbf{x})$ 是由什么决定的？ 答案是：$w_i(i=1,…,d)$ 和 $\text{threshold}$ Vectorization of Perceptron Hypothesis在机器学习中，我们往往将计算（无论是模型部分还是其他部分，能向量化就像量化！）向量化，目的有2: 提高运算速度 表达简洁紧凑 $$\begin{align}h(\mathbf{x})&amp;=sign((\sum_{i=1}^d w_ix_i) - \text{threshold})\\&amp;= sign((\sum_{\color{red}{i=1}}^d w_ix_i) + (\text{-threshold})\cdot(+1))\\&amp;=sign(\sum_{\color{red}{i=0}}^d w_i x_i)\\&amp;=sign(\mathbf{w}^T\mathbf{x})\end{align}$$ 其中，$\mathbf{w},\mathbf{x}\in\mathcal{R}^{d+1} \text{and with } \mathbf{w_{[0]}=\text{threshold},\mathbf{x}_{[0]}=+1} $. 现在，我们的 $h(\mathbf{x})$ 最终由一个量决定：$\mathbf{w},i.e.\text{parameters of perceptron.}$ . Perceptron in $\mathbb{R}^2$Perceotron 的公式已经给出，那么它到底是个什么样子呢？ 我们假设每example只有两个feature的情况下($\mathbf{w},\mathbf{x}\in\mathbb{R}^2$)，其如下图所示 上图中的直线称之为 Decision boundary: 它一边的点（代表training data 中的一个example）属于正例，另一边的点属于负例。 这条直线是怎么来的？ 根据之前的内容不难判断：$$\text{when }h(\mathbf{x})=0\Rightarrow w_0+w_1x_1+w_2x_2=0: \text{a line on the plane.}$$Perceptron的目的就是根据数据学习出这条直线(or hyperplane in high dimension space) 也就是说在Perceptron中： $\mathbf{w}^T\mathbf{x}=0$ 确定了decision boundary learning algorithm 可以学习出 $\mathbf{w}$, 即得到$h\mathbf{(x)}$. 当有新的数据 $\mathbf{x}_{new}$ 时, 将其带入:$ h(\mathbf{x}) $ 即可判断其类别是属于 +1（$h(\mathbf{x})&gt;0$） or -1（$h(\mathbf{x})&lt;0$） 。 Perceptron Learning AlgorithmTO-DO Guarantee of PLANon-Separable DataTO-DO… $\mathbf{x}_{new}=x$ $h(\mathbf{x}_{new})$]]></content>
      <categories>
        <category>课程笔记</category>
        <category>Machine Learning Foundations</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Classification</tag>
        <tag>二分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二.向量是空间中移动的对象]]></title>
    <url>%2Farchive%2F2018-06-22%2FVectors-are-objects-that-move-around-space%2F</url>
    <content type="text"><![CDATA[向量的模、夹角和投影模与内积模（Modulus）从几何视角看待向量，它就是一个有方向、有大小的线段(如下图)。 这个关于向量的简单描述（仅仅从几何视角，没有引入任何分解析方法）有如下含义： 有方向就是指箭头所指那个方向 有大小是指这个线段是有长度的(length, norm, modulus, magnitude等概念都从这里衍生出来) 是线段说明它长度是个有限值。 这个视角下，向量也是可以参与计算的 - 满足平行四边形运算法则。 然而，这远远不够。 不引入解析的方法来单独地看待几何视角下的向量，最大优点就是：直观。同时这也是其最大缺点。想要更加灵活、方便地利用向量这个概念进行表示、计算等操作，你需要回答如下2个核心问题， 有长度，具体多长？你总不能拿尺子量，用欧氏几何中的各种几何定理计算长度吧？ 有方向，具体哪个方向？我们知道所谓的方向，是有相对性的。比如，有三个人 A, B, C。B 对 A来说是在右边，对C 来说是在左边。向量的方向同理，当你需要定量、确定地描述一个向量方向的时候，你必须首先确定一个基准，在此基础之上才能够准确描述。 在几何视角下，是没法回答的。这就需要我们引入解析方法。具体来说，首先我们得为向量空间选定一组基，这个Basis就是我们的一个参照系（Reference Frame），之后这个参照系各个维度都可以用一个数量进行索引，以便度量向量在该维度的值/强度/magnitude，这就得到了坐标系统（coordinate system）。以下的内容我们假设都在vector space 的 笛卡尔坐标系 的语境下展开。 向量就变成了下图这个样子， 在坐标系下定义向量的长度 $\vec r = a\vec i + b \vec j$，其中 $\vec i, \vec j$ 是标准正交基。 $\lVert \vec r \rVert=\sqrt{a^2+b^2}$$$\vec r=\begin{bmatrix}x_1\\x_2\\x_3\\\vdots\\x_n\end{bmatrix},\lVert\vec r\rVert = \sqrt{x_1^2+x_2^2+…+x_n^2}$$其中 $\lVert \vec r\rVert$ 是向量 $\vec r$ 的长度 or 模 or 2范数。 内积（Inner Product）内积是诸多的向量乘法之一，其定义如下：$$\vec r=\begin{bmatrix}r_1\\\vdots\\r_n\end{bmatrix},\vec s = \begin{bmatrix}s_1\\\vdots \\s_n\end{bmatrix}$$$\vec r\cdot\vec s = \sum_{i=1}^n r_is_i=r_1s_1+r_2s_2+…r_ns_n$ 如果我们仔细观察，会发现如果$\vec r= \vec s$ 是同一个向量，那么内积的结果就是其长度的平方， $\vec r\cdot\vec r =\lVert \vec r\rVert ^2$ 结论就是：一个向量与自己内积，结果是其模的平方 内积有以下性质： 交换律 结合律 对加法的分配律 Cosine和点积上一小节解决了向量长度的问题，现在我们思考其方向的问题。 我们用向量的夹角来衡量其方向，正如之前提到的一样-方向是相对而言的，而夹角就具有相对性。 （实际上点积与内积是可以不一样的，但在实数域上定义的vector space中2者是相同的。因此这里不做区分，可以认为点积=内积。） 向量的夹角见下图， 其中, $\vec c=\vec a -\vec b$ . 现在根据余弦定理对夹角 $cos\theta$ ,$$\begin{align}\lVert \vec c \rVert ^2 =\lVert \vec a-\vec b \rVert^2 &amp;=(\vec a-\vec b)\cdot (\vec a-\vec b)\\&amp;=\vec a \cdot\vec a - \vec a \cdot \vec b - \vec b \cdot \vec a +\vec b \cdot \vec b \\&amp;=\lVert \vec a \rVert ^2 - 2\vec a \cdot \vec b + \lVert \vec b \rVert^2\end{align}$$ 由余弦定理可知， $$\lVert\vec c\rVert^2 = \lVert \vec a\rVert^2 + \lVert \vec b\rVert^2- 2\lVert\vec a\rVert \lVert \vec b\rVert cos\theta \\$$ 带入可得$$\begin{align}\vec a \cdot \vec b &amp;= \lVert \vec a\rVert \lVert \vec b\rVert cos\theta \\ cos\theta &amp;= \frac{\vec a \cdot \vec b}{\lVert \vec a\rVert \lVert \vec b\rVert}\end{align}$$上面2个公式把点积和夹角余弦联系了起来。我们可由此得知，如果2个向量夹角为$90^\circ​$（正交，或者说是垂直） , 则其2者内积为0，反之亦然。 投影 本节课讲的 Projection 都是指 Orthogonal Projection，即：difference(is a vector) of vector x and its projection is orthogonal to this projection. 以后不做明确说明的地方，投影均为正交投影。 投影-Projection 可分为 向量对另一个向量的 projection 向量对一个空间的projection 本文仅限于第一种情况。其又分为两种情况， Scalar projection-数量投影 vector projection-向量投影 下面先讲数量投影。 投影的几何视角如下图， 上图中绿线为向量$\vec u$ 对向量 $\vec v$ 的投影，记作：$\text{Proj}_{\vec v}\vec u$ 上图的直角三角形中由余弦的定义我们可知，$$cos\theta =\frac{\text{绿线长度}}{\text{红线长度}}=\frac{\text{Proj}_{\vec v}\vec u}{\lVert \vec u \rVert}$$ 而根据之前我们得到的余弦与点积的关系，我们有，$$cos\theta = \frac{\vec u \cdot \vec v}{\lVert \vec u\rVert \lVert \vec v\rVert}$$ 结合上述公式，我们得到，$$\begin{align}\text{Proj}_{\vec v}\vec u = \frac{\vec u \cdot \vec v}{\lVert \vec v\rVert}=\lVert \vec u\rVert cos\theta\end{align}$$ 上面的结论就是数量投影，从公式中我们很容易知道 $\text{Proj}_{\vec v}\vec u​$ 是个 scalar，只有大小没有方向。 那什么是向量投影呢？ 保持 scalar projection大小不变的同时让其方向与$\vec v$ 一致，因此向量投影为，$$\begin{align}\text{Proj}_{\vec v}\vec u &amp;= \frac{\vec u \cdot \vec v}{\lVert \vec v\rVert} \frac{\vec v}{\lVert \vec v\rVert}\\&amp;=\frac{\vec u \cdot \vec v}{\lVert \vec v\rVert ^2}\vec v\end{align}$$ 其中, $\frac{\vec v}{\Vert\vec v\rVert}$ 是与向量 $\vec v$ 同一方向的单位向量（unit vector）。 改变参照系（Reference Frame）基变换-&gt;坐标系统基是参照系的基础，在此之上才能够定义一个具体的坐标系统（Coordinate System）。下面给出一个完整的逻辑关系，$$\begin{cases}\text{vector}\\\text{向量加法}\\\text{数乘}\end{cases}\\rightarrow\ \ \text{Space}\\rightarrow\text{Basis, dim}\ \rightarrow \text{Coordinate System}\rightarrow\begin{cases}\text{axes }\leftrightarrow \text{dim} \\\text{coordinates(index)}\end{cases}$$左边概念是右边概念的基础。 我们有如上图所示的一个向量$\vec r$, 为向量空间取一组标准正交基 ${\vec e_1,\vec e_2}$ , 还有另外一组正交向量 ${\vec b_1,\vec b_2}$ . 选定了基，那么就有坐标系也就有了坐标，其中，$$\begin{align}&amp;\vec e_1=1\cdot \vec e_1 +0\cdot \vec e_2=\begin{bmatrix}1\\0\end{bmatrix},&amp;&amp;\vec e_2 = 0\cdot \vec e_1 + 1\cdot \vec e_2=\begin{bmatrix}0\\1\end{bmatrix}\\&amp;b_{1,e}=2\cdot \vec e_1+1\cdot \vec e_2=\begin{bmatrix}2\\1\end{bmatrix},&amp;&amp;b_{2,e}=-2\cdot \vec e_1 + 4\cdot \vec e_2=\begin{bmatrix}-2\\4\end{bmatrix}\end{align}$$ 且已经得知$$\begin{equation}\vec r_e= 3\vec e_1 + 4\vec e_2=\begin{bmatrix}3\\4\end{bmatrix}\end{equation}$$ 我们现在的问题是：求 $\vec r$ 在以 ${\vec b_1,\vec b_2}$ 为基的情况下，其坐标是什么？即：$$\vec r_b=\begin{bmatrix}?\\?\end{bmatrix}$$其中$?$ 是scalar. 首先，从在空间中移动的几何角度来看 我们现在泛化这个问题：在给定标准正交基(e.g. ${\vec e_1, \vec e_2}$ )的情况下，如何确定一个向量的坐标？其本质思想就是：从原点出发，沿着 $\vec e_1$ 的方向走 $x$ steps, 然后沿着 $\vec e_2$ 的方向走 $ y$ steps, 那么 $\vec r$ 的坐标就是：$$\vec r_e=\begin{bmatrix}x_e\\y_e\end{bmatrix}$$ 沿着基向量的方向走了多少步，换句话就是问向量 $\vec r$ 在这两个方向上的数量投影是多少？ 于是，这个本质思想我们可以进一步转化为： 将 $\vec r$ 向 $\vec e_1$ 方向投影得到 $\text{Proj}_{\vec e_1}\vec r$, 向 $\vec e_2$ 方向投影得到 $\text{Proj}_{\vec e_2}\vec r$ , 则：$$\begin{cases}x_e=\text{Proj}_{\vec e_1}\vec r \\y_e=\text{Proj}_{\vec e_2}\vec r\end{cases}$$沿着两个basis vector 的方向做投影，也叫做正交分解。注意：这种思路适用于标准正交基的情况，如果基不正交也不是单位基，那么上面的思想就是错误的。但切记，向量加法依然满足平行四边形法则，那么就依然可以利用包含 ? 的那个等式。 现在还有一个问题，如果给定的基只正交但不标准（比如我们现在要解决的问题，$\vec b_1,\vec b_2$ 虽然正交但不是单位向量）的时候如何得到一个向量的坐标呢？其实，我们只需要修改上面思想的一些细节，就可以得到答案。 给定一个正交基（无论标准与否）${\vec b_1,\vec b_2}$，和一个向量 $\vec r$。现在要求向量 $\vec r$ 在这个基下的坐标 $\vec r_b$. 其本质思想为：从原点出发，沿着 $\vec b_1$的方向走 $x_b$ steps， 沿着$\vec b_2$ 方向走 $y_b$ steps, 那么 $\vec r$ 的坐标应该为$$\vec r_b =\begin{bmatrix}x_b\\y_b\end{bmatrix}$$且其值为：$$\begin{cases}x_b = \frac{\text{Proj}_{\vec b_1} \vec r }{\lVert \vec b_1\rVert}=\frac{\vec r \cdot \vec b_1}{\lVert \vec b_1\rVert^2}\\y_b = \frac{\text{Proj}_{\vec b_2} \vec r }{\lVert \vec b_2\rVert}=\frac{\vec r \cdot \vec b_2}{\lVert \vec b_2\rVert^2}\end{cases}$$ 为什么当给定的基只正交而不标准的时候，相比正交基而言坐标的分母会多出一个模？这是因为我们之前说过，坐标系统是相对而言的： 一个向量自己无所谓什么角度，必须是它相对于什么方向的角度；从原点出发在空间中沿着一个方向走几步， 这个steps 是相对哪个刻度而言的？这个刻度就是$\lVert \text{basis vector}\rVert$ . 对于标准正交基而言其长度为1，从原点出发沿着基向量的方向依次走的 steps 是意味着 “我在各个方向，相对那个方向的基向量我走了几步，这个几步是由 $\frac{\text{走的长度}}{\text{该方向基的长度}}$ 来衡量的”。由于基向量长度为一，因此向量$\vec r$ 在基向量方向的数量投影就是坐标。 对于正交但不标准的基而言，由于“该方向的长度” 不是1，因此在上面公式中我们求解坐标（步长-steps） $x_b,y_b$ 的时候会多出来一个分母-基向量的模。 从代数结合四边形法则的角度， 本问题中，我们最后要得到的是$$\vec r_b = \begin{bmatrix}?\\?\end{bmatrix}$$由于在给定基 $ {\vec e_1,\vec e_2}$ 的情况下，$\vec r, \vec b_1, \vec b_2$ 都是有其坐标的，其坐标如下：$$\vec r_e = \begin{bmatrix}3\\4\end{bmatrix},\vec b_{1,e}= \begin{bmatrix}2\\1\end{bmatrix},\vec b_{2,e} = \begin{bmatrix}-2\\4\end{bmatrix}$$ 我们就是要找到上述3者如下的关系：$$\vec r_e = ?\cdot \vec b_{1,e} + ?\cdot \vec b_{2,e}$$那么就能得到 $\vec r_b$ 了。 上面的式子右边是 两个向量之和，且这两个向量一个在 $\vec b_{1,e}$ 方向，一个在 $\vec b_{2,e}$ 方向，根据四边形法则我们自然想到等式右边的两个向量就是 $\vec r_e$ 在$\vec b_{1,e}, \vec b_{2,e}$ 方向上的分量，又因为$\vec b_{1,e},\vec b_{2,e}$ 正交，那么正交分解即：投影！ $?\cdot \vec b_{1,e}$ 和 $?\cdot \vec b_{2,e}$ 就是向量 $\vec r$ 在对应方向上的向量投影 根据向量投影公式 ，$$\text{Proj}_{\vec v}\vec u = \frac{\vec u \cdot \vec v}{\lVert \vec v\rVert^2}\vec v$$ 我们可知，$\vec r_b​$ 为$$\vec r_b = \begin{bmatrix}?\\?\end{bmatrix}=\begin{bmatrix}\frac{\vec r_e \cdot \vec b_{1,e}}{\lVert \vec b_{1,e}\rVert^2} \\\frac{\vec r_e \cdot \vec b_{2,e}}{\lVert b_{2,e}\rVert^2}\end{bmatrix}=\begin{bmatrix}2\\\frac{1}{2}\end{bmatrix}$$ 从matrix与线性组合的角度看， $$\vec r_e = ?\cdot \vec b_{1,e} +?\cdot \vec b_{2,e}$$ 等式左侧是个向量，右侧是2个向量的线性组合，可将其转化为：$$[\vec b_{1,e}, \vec b_{2,e}]\begin{bmatrix}?\\?\end{bmatrix}=\vec r_e$$将坐标带入，解此线性方程组即可得 $\vec r_b$. 向量空间、基与线性无关向量空间 满足以下两个条件的$\mathbb{S}$ 称为向量空间： $\mathbb{S}$ 是一个向量的集合 $\mathbb{S}$ 对 向量加法 和 数乘 这两个操作是封闭的。 Basis 是n个向量的集合，且满足， 线性无关-linear independent（are not linear combinations of each other.） Span 整个空间 然后， 这个空间就是一个n-dim的向量空间。 标准正交基的特性 不同的basis 对应不同的坐标系统。当我们从一组标准正交基 变换到另一组标准正交基的时候，空间中向量对应的坐标会发生变化，但是 依然保持对向量加法和数乘的封闭性。 空间依然是均匀间隔的。 因为不同的标准正交基之间的linear transformation 不会扭曲、折叠 vector space。线性代数中的线性，在几何意义上就是指这个意思。空间可能会被拉升、旋转、反向，but everything remains evenly spaced and linear combinations still work. 当基向量不正交的时候 此时我们就不能仅仅使用 dot product 来进行坐标变换了，我们需要使用矩阵进行：transformation of axes。 基变换的应用这部分内容需要回头看，需要结合Gilbert的书反复思考！ 入门级的机器学习是一个Linear regression 的任务：Fitting a Line! 如下图， 比如将上图中的点从 xoy 空间中映射到那条直线上，去衡量”error”的大小（我们希望越小越好！error 有时候也叫 “resudial”,”noise”） 比如神经网络做人脸识别，某种程度上或多或少地是将像素点映射到另一个空间，然后抽取出那个空间的一组基向量来表示更高级的feature：鼻子的形状、眼睛的距离等等。 现实世界中向量的应用总结 向量是描述我们在空间中位置的一个可以移动的对象 物理空间、data space、参数空间（parameters of a function），etc. 向量加法 ， 数乘 是最基本的2个操作 向量基于上述两个操作，向量可以在空间中移动 定义了space 的基 线性无关 线性相关(由linear combination来定义相关性) 空间的维度=其基向量的个数 用projection 来考虑当从一个正交基变换到另一个正交基时，坐标系统的变化（空间中向量坐标的变化）。]]></content>
      <categories>
        <category>课程笔记</category>
        <category>Mathematics for Machine Learning</category>
        <category>Linear Algebra</category>
      </categories>
      <tags>
        <tag>Linear Algebra</tag>
        <tag>Vector</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2Farchive%2F2018-06-21%2FPopoblue-s-Learning-List%2F</url>
    <content type="text"><![CDATA[Fundamental Knowledge for Deep Learning &amp; Natural Language ProcessingThis list just includes the core of related knowledge in deep learning and natural language processing. The all the courses listed here is personally tested by myself. I believ they are the best until now from and only from my personal persepctive. This list is updating as time goes on... Natural Language Processing * Book Dan Jurafsky( 3rd) Manning Natural Language Processing-Jacob Eisenstein-Nov 13, 2018 Course CS 124: From Languages to Information (First course in NLP!) CSE490U: Natural Language Processing - Winter 2017 (for undergraduates) - Noah Smith CSEP 517: Natural Language Processing - Spring 2017 (for professional master students) - Noah Smith University of WashingtonInstructor: Noah Smith Syllabus and Vedio Lectures CSE 517: NLP (for Ph.D. students)-Spring 2018 - Noah Smith CSE 599D1: Advanced NLP (for Ph.D. students)-Spring 2016 - Noah Smith 601.465 - Natural Language Processing - Fall 2017 - Jason Eisner COMP 790.139: Natural Language Processing (Fall 2017)- Mohit Bansal CS4650 - Natural Language Understanding (Spring 2017) - Jacob Einsten CMU NLP Courses (Theory+Practice) CMU CS-11-711: Algorithms for NLP, Fall 2017 CMU CS 11-747: Neural Networks for NLP, Spring 2019 CIS 700: Advanced Machine Learning for Natural Language Processing-Dan Roth Fall 2016 – CS6501 Natural Language Processing – Kw Chang CS269 Seminar: Machine Learning in Natural Language Processing - Kw Chang CMU-CS-11-731: Machine Learning and Sequence to Sequence Models -Graham Neubig Machine LearningIntroduction Level Book A Course in Machine Learning(2017 2ed.超级经典！) - by DumIII ​ General Notations and Introduction Neural Networks Demystified (非常好的神经网络入门讲解资源- from youtube) Course CMU-10601- Introduction to machine learning Spring 2015(with all meterials) Spring 2018(only slides) CS229- Fall 2017 - Andrew Ng. * Machine Learning - Fall 2017 - 李宏毅 * DS-GA 1003 Spring 2017- Machine Learning and Computational Statistics OXFORD - Machine Learning( 2014~2015) - Nando de Freitas CPSC340 - Machine Learning and Data Mining - (Undergraduate Level)-2012 - Nando de Freitas CPSC540 - Machine Learning - Mastter Level -2013-Nando de Freitas CS446 - Machine Learning - Dan Roth More Advanced Alex Smola (very important and systematic)-Ph.D. Level* CMU-10701-Fall 2015: Introduction to Machine Learning CMU-10715-Fall 2015: Advanced Introduction to Machine Learning Spring 2017 – CS6501 Advanced Machine Learning – Kw Chang OXFORD 2016-2017–Advanced Machine Learning CMU-10-725-Fall 2016: Convex Optimization CMU-10-702-Spring 2017: Statistical Machine Learning Assuming sutdent have taken: 10-715 and 36-705 NYU-DS-GA-1005 - Inference and Representation Deep Learning Book Deep Learning Book - Ian Goodfellow etc. -Dive into Deep Learning- Li Mu and Alex Smola（非常重要） 中文版 英文版 General and Introduction 深层神经网络涉及理念（vedio） Deep Learning 公开课-Yann LeCun General Deep Learning Course UC Berkely(适合入门，hands on！) Berkeley STAT-157: Introduction to Deep Learning (Mxnet based，十分重要可作为第一门课程) 上述课程李沐早期版本（中文） Stanford CS 230- Deep Learning(Andrew Ng) Neural Networks for Machine Learning (Hinton) Machine Learning and Having It Deep and Structured - Spring 2018-李宏毅(持续更新中) MLDS - Spring 2017 - 李宏毅 EPFL-IDIAP-EE-559-Deep Learning MIT 6.S191: Introduction to Deep Learning CMU-Deep Learning CMU-11-785: Introduction to Deep Learning CMU-10-707: Deep Learning DS-GA-1008 Deep Learning, Spring 2017- Yann LeCun fast.ai course For Natural language processing * CMU-CS 11-747- Neural Networks for NLP-Spring 2019 OXFORD- Deep Learning for Natural Language Processing: 2016-2017 (Phil Blunsom) Home page Github-lecture resources Stanford-CS224N: Natural Language Processing with Deep Learning-2018 Home page, 2017 page Video lecture (Small Lesson) - Embeddings and Deep Learning - Hinrich Schutze DS-GA 1011 Fall 2017- Natural Language processing with Representation Learning ( Bowan and Cho ) Lab Git Repository For Computer Vision Course Stanford CS 231N Learning to see (很好的讲CV入门的课程，from youtube) Reforcement Learning ( with deep or not) Introduction and General notations Introduction to RL- by [Joelle Pineau] Course CS294-UC Berkeley-Deep Reinforcement Learning, Spring 2017 CS234 - Stanford CMU-10370- Deep Reinforcement Learning and Control Spring 2017 David Sliver’s Reinforcement Learning Course-2015 Introduction to Reinforcement Learning- Udacity MathematicsCalculus Single Variable Calculus (MIT 18.01) Auroux - Maltivariable Calculus -2017-Fall Lectures(mit 18.02) Linear / Abstract Algebra Book Introduction to linear algebra 5-th edition Coding the Matrix: Linear Algebra through Applications to Computer Science. 1st It’s corresponding course: The Matrix in Computer Science. Course MIT 18.06- Strang (Should be the FIRST CHOICE!) * 18.06 on OCW with vedio lectures 18.06-SC (Optional) MIT 18.06 -Fall 2017-new by other people. Essence of Linear algebra(3Blue1Brown) Coding the Matrix Probabaility, Stats and Informaton theory BOOK Probability and Statistics 4th - DeGroot &amp; Schervish Course MITx-6.041Probability HARVARD - Statistics 110: Probability (Should be the first one!) * CS109: Probability for Computer Scientists (Or this is the 1st) * DS-GA 1001 Introduction to Data Science (2017) Jupyter Versions DS-GA 1002: Statistical and Mathematical methods(2015) DS-GA 1002: Probability and Statistics for Data Science(2017) DUKE-STA663: Computational Statistics and Statistical Computing ( Should be the first one! ) CMU-10-705: Intermediate Statistics - Larry Wasserman ECE598: Information-theoretic methods in high-dimensional statistics STAT364/664: Information theory Math for Machine Learning Book Video Math for CS and EE EE103/CME103: Introduction to Matrix Methods Computational Science and Engineering- Gilbert Strang I: MIT18.085 II: MIT18.086 Stanford-CS103 - Mathematical Foundations of Computing Mathematics for Computer Science Hands-ON AND APPLICAITON ORIENTEDTensorflow Stanford CS20- Tensorflow for deep learninng research TensorFlow_Course Tensorflow for deep learning research(youtube) Deep Learning with Tensorflow(EdX) TensorFlow 101 Pytorch x y MxNet ​ ​ DyNet Homepage Data ScienceFirst of all, a curriculum about data science created by David Venturi is here. It’s serious and important for someone want to engage into data science. I promise! Book Python Data Science HandBook Course Duke-BIOS 821: Data Science Foundations with Python]]></content>
      <categories>
        <category>资源</category>
      </categories>
      <tags>
        <tag>课程</tag>
        <tag>Course</tag>
        <tag>Learning List</tag>
        <tag>学习列表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一.线性代数和数学在机器学习中的介绍]]></title>
    <url>%2Farchive%2F2018-06-20%2FIntro-to-Linear-Algebra-for-Machine-Learning%2F</url>
    <content type="text"><![CDATA[学习目标 Recall how machine learning, vectors and matrices are related. Interprest how changes in the model parameters affect the quality of the fit to the training data. Recognize that variations in the model parameters are vectors on the response surface - that vectors are a generic concept not limited to a physical real space. Use substitution/ elimination to solve a fairly easy linear algebra problem. Understand how to add vectors and multiply by a scalar number. 如何在这门课获得成功 Read the syllabus: all the important information can be found here Make a concrete and reasonable study plan. Log on to the class at least 3 times a week. Ask questions!! Make connections with your fellow learners. 机器学习和线性代数的关系Linear algebra is defined to be study the vectors, vector spaces and a mapping between vector spaces. 为什么需要线性代数 Problem of solving the simultaneous equations.$$\begin{equation}\begin{cases}3x + 2y &amp;=&amp; 5\\x+4y &amp;=&amp; 4\\end{cases}\end{equation}$$ The optimization problem of fitting some data with an equation with some fitting parameters在整个机器学习中，线性代数主要用来解决以上两个问题。 向量操作Vector 的不同表现形式 math space 中个一个点 一组有序数对：$(x_1x_2,…,x_3)$ 一个有方向、有大小的线段(几何视角) 物理 三位空间中的一个点 四维空间（增加时间维度）中的一个点 计算机科学/统计学/机器学习 一个数组/list（programming language） 样本的若干属性（attributes）的一组值 一个example的feature values 名称不一样，但本质大同小异。本课程中我们采用：Point which can move around in a space 这一说法。 向量的基石 向量加法-按照四边形法则$$\vec r+\vec u = \vec v$$向量加法的意思是：从原点开始沿着$\vec r$ ，接着沿$\vec u$ 方向走。 数乘$$\vec r^\prime =a\vec r$$数乘的意思是：对向量$\vec r$ 进行缩放（scale）,如果$a&gt;0$, 沿着$\vec r$ 相同的方向缩放；如果 $a&lt;0$ , 沿着$\vec r$ 相反的方向缩放，缩放的倍数为$a$. 向量加法和数乘是之后所有基于向量的其他概念的基石。向量空间的定义也依赖于这两个操作。 坐标系统( coordinate system)现有两个标准正交向量$\vec i,\vec j​$ 定义一个 vector space,$$\begin{align}\vec r &amp;= 3\vec i + 2\vec j=\begin{bmatrix}3\\2\end{bmatrix}\\\vec s &amp;= 2\vec i - 3\vec j = \begin{bmatrix}2\\-3\end{bmatrix}\end{align}$$ 上面的$\vec r$ 的意思是朝着$\vec i$ 的方向走 3 steps, 然后朝着$\vec j$ 的方向走2 steps； 同理，$\vec s$ 的意思是朝着 $\vec i$ 的方向走2 steps，然后朝着 $\vec j$ 的反方向走 3 steps。这个所谓的沿着方向走几个“steps”，是与平行四边形法则兼容的。 这样我们就在2个base vectors和向量加法、数乘操作的基础上定义了一个坐标系统:$\rightarrow$ 在以 $\vec i$, $\vec j$ 为基的空间中的任意一个向量 $\vec r$, 都唯一对应一个坐标。 数据科学中的向量首先要建立以下认知： 向量是空间中的一个点（vector is a point in some space） 空间有很多种 data space(with respect to measured data!) vector space parameter space function space， etc. 向量可以是：数据本身或者模型参数。这取决于你把什么内容组以向量的形式组织：是training data or parameters of a model. 有了上述认知，我们就可以采用基于向量的操作处理机器学习中所要解决的问题。 Getting a handle on vectors一个模型和向量有什么关系？其实，他们之间的关系基于如下因素： 模型含有参数（绝大部份模型都包含参数，参数是衡量模型能力大小的标志之一） 不同的参数即对应不同的具体函数 不同的函数在处理数据时性能不同 因此，适当的选取参数对于模型最终性能的表现起到了决定作用。 因此，当我们看到向量的时候，要问自己如下问题： 这个向量是模型参数的组织形式，还是数据的组织形式？ 这个参数是什么模型的参数？ 如何选取合适的参数，使得模型性能达到最好？ 这个所谓的模型“性能最好” 的测量标准是什么，如何评价好坏？ 探索参数空间（parameter space）一个例子一个随机变量$X\thicksim N(\mu,\sigma)$ , 其PDF$f_X(x)=\frac{1}{\sigma \sqrt{2\pi}}exp(-\frac{(x-\mu)^2}{\sigma ^2}).$ 其中$(\mu,\sigma)$ 就是其密度函数的参数（parameters），有如下结论：$$(\mu_1,\sigma_1)\ne (\mu_2,\sigma_2)\Rightarrow f_X^1\ne f_X^2$$换句话说，就是上一小节我们提到的：不同的参数对应不同的函数. 参见下图， 从参数到参数空间机器学习中一个最重要的任务是：Fit a model to data in order to predict the underlying distribution. 以高斯分布为例（前文），我们将它的两个参数 $(\mu,\sigma)$ 组织成向量的形式，$$\mathbf{p}=\begin{bmatrix}\mu \\\sigma\end{bmatrix}$$那么k组不同的参数$(\mu_1,\sigma_1),(\mu_2,\sigma_2),…,(\mu_k,\sigma_k)$将对应k个参数向量（parameter vector）: $\mathbf{p_1},…,\mathbf{p_k}$, 这些向量将张成一个 vactor space, 由于每个向量的components 都是随机变量$X$ 的密度函数 $f_X(x)$ 的参数，因此这个 vector space 也是一个 parameter space. 如何衡量参数估计的结果？（Evaluation metric of estimating parameters）之前所说的选择合适的参数使得模型性能好是一种通俗的说话，在机器学习或者统计的范畴里，这个问题叫做：参数估计（parameter estimating）。 由于不同的估计方法、人们不同的偏见（bias），使得面对同样的数据，我们估计出来的参数往往是不一样的。那么能否有一个大致的标准来指导我们进行参数估计呢？这个标准是有的（而且还不唯一！）： 我们需要使得我们估计的参数与某组客观存在的参数的差异/误差尽可能小！ 我们也把这个差异/误差叫做：Error, Loss, Residuals, etc.(The difference between the measured data and the model prediction.)SSR (Sum of Squared Residual):假设我们已经估计好了参数，那么我们可以使用这个模型进行在对新的数据进行预测。我们对模型的认知有如下几个方面，- 如果一个模型很好地拟合了measured data（training data），我们才说这是好的模型。- 参数可以取各种不同的值（values），我们知道参数的一些取值是比另一些好的（更好的fit data）。- 那么现在的问题是，如何判断好坏？如下图所示，粉色的是已经估计好参数的模型（确定了具体参数的密度函数），橘色代表data（frequency histogram）。Residual: 粉色和橘色在同一横坐标下的高度差。 SSR 定义：$$\begin{align}\text{SSR}(\mathbf{p})&amp;=|f_{data} - g_p|^2\\&amp;= \sum_{\mu}\sum_{\sigma}(\mu_{data}-\mu_g)^2 + (\sigma_{data} -\sigma_g)^2\end{align}$$ 我们希望SSR(p) 越小越好，也即上图中绿色的部分（重合部分）越大越好。 我们可以为SSR(p) 设定一个阈值，比如：threshold=0.00001，当其小于threshold的时候我们认为模型 $g_p$ 就已经足够好了。 之前我们提到，我们假设参数已经估计好了，于是在SSR的指导下，我们已经可以判断模型好坏了。但现在还遗留了最后一个问题：参数是怎么估计的？ 如何估计参数？因为每个参数向量(each parameter vector) 都代表了不同的函数， 每个参数向量都对应有一个SSR值。we can draw the surface of SSR values over the space spanned by these $\mathbf{p}$ , 比如前面例子中的 $\mu$ 和 $\sigma$. 下图是 Surface of SSR values 的展示， 我们采用一种 “Top-down” 的视角看这个surface，把它看作一个等高线图（contour map），其中每个等高线代表了SSR的一个固定取值（constant value）。 机器学习的目的是找到那个能够尽可能好地拟合数据的参数集（parameter set），有了我们前面的知识，这个问题就转化为 在下图所示空间中寻找一个最低点-全局最低（“这个空间”，就是指parameter space） 只要我们有办法在这个空间找到最低点，那么所有问题就都解决了。然而，找到最低点的方法是什么？答案是：梯度下降（Gradient descent）， Gradient descent-一种优化算法我们可以定义另一个与 $\mathbf{p}$ 在同一个空间的vector,$\Delta \mathbf{p}\rightarrow$它将告诉我们对$\mathbf{p}$ 做什么改变将会更好地拟合数据: 例如： a model with parameters $\mathbf{p^\prime=p+\Delta p}$ will produce a better fit to data, if we can find a suitable $\mathbf{\Delta p}$. 梯度下降是本系列课程中第二门课程的重点内容之一，在此不做过多细节介绍。]]></content>
      <categories>
        <category>课程笔记</category>
        <category>Mathematics for Machine Learning</category>
        <category>Linear Algebra</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Linear Algebra</tag>
        <tag>Vector</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习中的信息论]]></title>
    <url>%2Farchive%2F2018-06-20%2Fyear-01-03-%E4%BF%A1%E6%81%AF%E8%AE%BA%E4%B8%AD%E7%9A%84%E5%87%A0%E4%B8%AA%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[Quantification of Information- From the perspective of a Specific Field为防止markdown渲染出现问题，提供一份pdf版的下载，链接 *** 近代通讯及计算机等学科的重要基础之一是信息论，核心内容涉及信息的产生、获取、表示、专递、转换。这就需要对于信息进行量化，将信息以某种数学的形式表示，用数学的理论来指导通信、计算机科学的工作。基于这一目的，我们首先考虑如下的一个“信息源”$—$ 输出一些列有序的（e.g. 某个系统）信号：$$\color{blue}{a_1, a_2, a_3, a_4, \cdots, a_i, \cdots, a_N}$$注意，上述输出序列（series of ordered outputs）是有序的，i.e. $a_1$ 是“最可能出现”的输出，$a_N$ 是“出现可能最小”的输出。现在有了输出序列（information）$—$ 物质基础，那用什么哪种数学方法进一步量化它呢？答案是$—$ 概率。 一个合理的测量信息的方法应该满足以下条件 一个输出信号$a_i$ 的“信息量(information content)” 仅仅取决于 $a_i$ 出现的概率$—$ i.e. $P_i$ , 而不是取决于$a_i$ 的值。 我们用这样一个函数$I(P_i)$ 表示 “信息量”，并将之命名为输出(e.g. $a_i$ )的 self-information. 并且，必需满足： $$\sum_i P_i =1$$ Self-information 是$P_i$ 的连续函数。 Self-information 是$P_i$ 的单调递减函数。 如果，$P_i = P_j \cdot P_k $ , 那么$I(P_i)= I(P_j) \cdot I(P_k) $ 实际上，只有对数函数[^1] 满足上述要求，因此self-information可以写成如下形式：$$\color{red}{I(P_i) = -\log(P_i)}$$因此，“整个输出序列”[^2] 的所包含的信息即为：该序列所有outputs的self-information的加权和，这个”权重“即为，每个输出$a_i$ 出现的概率$P_i$ (i=1,2, … ,N). 我们把这整个序列的信息命名为 “Shannon-Entropy” 或者 “Entropy”, 记作：$$\color{red}{H(X)=\sum_{i=1}^N P_i I(P_i) = -\sum_{i=1}^NP_i\log(P_i) =\sum_{i=1}^N \log(\frac{1}{P_i})^{P_i}}$$再次强调，entropy 和 self-information的关系为：前者是后者的加权和，权重为某个output出现的概率。 上述公式中的$X={a_1,a_2,\cdots,a_i, \cdots a_N}$, i.e. 整个输出序列 (sequence of outputs). 以上，是从一个通信领域的一个具体情况说明了information及若干相关概念如：self-information, entropy等的来源。下面的内容，会从信息论理论的角度来阐述、引出一些核心概念，其含义更加广泛、更加本质。 Self-Information(自信息)顺着第一部分的内容，与其并不冲突。而是从更加宽阔的视角和更加深的理论出发，基于信息论和概率来讲述信息的核心概念。简而言之，在信息论中，self-information or surprisal 是用来刻画:$$\color{red}{当抽样到一个随机变量时所附带的不确定性（uncertainty）或者非同寻常的（superise）程度}$$鉴于随机变量和随机事件之间有对应关系[^0]（i.e. 一个随机变量是一个定义在随机事件上的实值函数），下文中random variable 和 event 会根据具体的context交互使用，读者只须明白他们的含义和对应关系就好。 根据上述定性的描述，我们不难认同如下结论： 当information接收端事先已确定地知道需要传递的消息（message ）的内容（the content of a message is known priori with certainty），那么这个消息（message）的传递不会带来任何的information。 相应地，与本文第一部分类似，我们同样给出一个序列 $—$ 随机变量$x$ 的所有可能取值： $$x_1, x_2, \cdots, x_n, …,x_N$$ 一如前文所述，序列中任何一个 $x_n$ 可看作一个随机事件（event）。该随机事件的self-information表明了该事件出现的不确定性（可能性取负即可），因此 self-information 仅仅取决于该随机事件 (event) 发生的概率，而与别的因素无关, 换句话说一个随机事件/变量 的self-information是该随机事件/变量概率的函数。i.e.$$\color{red}{I(x_n)=f(P(x_n))}$$ 完整的写法应该是：$I(X=x_n)=f(P(X=x_n))$ 此外函数 $\color{blue}{f(\cdot)}$ 还需具备以下2个性质： If $P(x_n)=1$ then $I(x_n)=0$ , if $P(x_n)&lt;1$ then $I(x_n)&gt;0$ . 对于两个independent 的随机事件$A, B$, 如果另一个随机事件 $C$ 是它们两个的intersection 事件，则有：$I(C)=I(A\cap B)=I(A)+I(B)$ . 因为有，$C=A\cap B$, 所以$P(C)=P(A\cap B)=P(A)P(B)\Longrightarrow f(P(C))=f(P(A)P(B) \Longrightarrow I(C)=f(P(A)P(B))$ , 由2 中公式可知，$I(C)=I(A)+I(B)=f(P(A))+f(P(B))$ , 从而有：$\color{blue}{f(P(A)P(B))=f(P(A))+f(P(B))}$ . 从而得知，函数$f(\cdot)$ 满足可加性：$$f(x\cdot y) =f(x) + f(y)$$考虑到以上限制条件，只有对数函数符合要求（底数无关大局），因此一个随机事件/变量 $X$其取值为 $x_n$时self-information 定义为：$$I(x_n)=-\log(P(x_n))=\log\frac{1}{P(x_n)}$$ 与第一部分的定义是一致的。但是要注意的是: self-information 针对的是一个随机变量单独的某一个outcome 而言,（i.e. $\omega_n$)来衡量它的不确定性(信息含量)。不是整个随机事件/变量序列 or 某一个随机变量总体 (i.e. $x_1, \cdots, x_N$)的 不确定性。 Point Mutual Information(点互信息)Pointwise mutual information(PMI) or point mutual information，是针对两个随机变量，对他们之间的关联性(当然，同时也是 无关性)进行measure的量，常用在统计与信息论中。而之前讲到的self-information 是衡量一个(离散)随机变量在某一个固定取值上的不确定性(uncertainty)，注意区分；此外，PMI 涉及的是两个random variable各自单独取一个固定值，后面将要讲的MI 涉及的是两个random variables可能的取值集合，须注意以上区别。 Formal DefinitionThe PMI of a pair of outcomes x and y belonging to discrete random variable $X$ and $Y$ quantifies the discrepancy between the probability of their coincidence given their joint distribution and their individual distribution, assuming independence. Mathematically, $$\begin{equation}\begin{split}pmi(x;y)&amp;\equiv \log \frac{p(x,y)}{p(x)p(y)} \\&amp;=\log \frac{p(x|y)}{p(x)} \\&amp;=\log \frac{p(y|x)}{p(y)}\\\end{split}\end{equation}$$ PMI的性质根据PMI的定义，我们可以知道其有如下几个性质： 对称性：$pmi(x;y)=pmi(y;x)$ 取值可以正、负、零。 如果$X$ and $Y$ 是相互独立(无关, i.e. independent)的, $pmi(x;y)=pmi(y;x)=0$ 当 $X$ 和 $Y$ 完全关联的时候，i.e $p(x|y)\, or\, p(y|x)=1$ , PMI 会取最大值。 有上下界(bounds)：$-\infty \leqslant pmi(x;y) \leqslant min {-\log p(x), -\log p(y) } $ 如果固定$p(x|y)$, PMI是关于$p(x)$ 的单调递减；如果固定$P(y|x)$, PMI是关于$p(y)$的单调减函数。 PMI与 Self-information、 mutual information的关系TO-DO Chain-rule for PMITO-DO Mutual-Information(互信息)MI是点PMI的自然延伸，前面我们讲到PMI是针对两个随机变量各自均取一个固定值的情形，用以衡量这两个随机变量各取对应固定值时的关联性或者无关性。MI 也是针对两个随机变量，不同之处在于它针对这两个随机变量的可能取值集合而言，那自然容易联想到其定义是一个基于PMI的加权求和形式，权重为这两个随机变量的联合分布$—P(X,Y)$ , i.e. MI是 $pmi(x;y)$ 的数学期望。 从信息论的角度看，MI quantifies the “amount of information or information content” obtained about one random variable, through the other random variable. 不像相关系数(correlation-coefficient) 局限于 read-valued的随机变量, MI有更大的广泛性并用以判定联合分布$P(X,Y)$ 与边缘分布的乘积$P(X)P(Y)$ 的相似程度(或 差异程度)。 Formal Definition形式化地，有两个离散型随机变量$X$ with image $\mathcal X$ 和 $Y$ with image $\mathcal Y$ 并且$X$ 和 $Y$ 服从同一概率分布$P$ 其他们的联合概率分布为 $P(X,Y)$ 。则$X$和 $Y$的 mutual information 有如下定义：$$I(X;Y)=E[pmi(X,Y)]$$ 完整写法应该为$ I_{(x,y)\sim P(X,Y)}(X;Y)=E_{(x,y)\sim P(X,Y)}[pmi(X,Y)] $ 可以显示地写为：$$I(X;Y)=\sum_{x\in \mathcal X}\sum_{y\in \mathcal Y}p(x,y)\log \frac{p(x,y)}{p(x)p(y)}$$ 可以明显看到显示定义公式，MI为PMI的数学期望。 Motivation为什么需要这样一个概念呢？直观上理解，MI 测量了或者说衡量了两个随机变量 $X$ 和 $Y$ 共享的信息：它可以衡量当其中一个随机变量已知的话，可以多大程度上减少另一个随机变量的不确定性。 比如， 如果$X$和$Y$时相互独立的，那么知道 $X$ 不能给出关于$Y$的任何信息，换句话说，就是当我们知道了$X$对于降低$Y$的不确定性没有任何帮助，反之亦然，那么$I(X;Y)=0$. 考虑另一种极端情况，如果$X$ is a deterministic function of $Y$ 并且$Y$ is a deterministic function of $X$, 那么$X$传达或者包含的所有信息均与$Y$是共享的，也即：$X$ 完全决定了$Y$的值，反之亦然。结果，这种情况下，$X$和$Y$的互信息$I(X,Y)$与单独包含在$X$或$Y$中的不确定性是一样的, 其实就是：$X$的熵 $H(X)$ 同时也是$Y$的熵$H(Y)$ ).（一种特殊的情况是，$X$ 和$Y$是同一个随机变量。） MI 的性质 非负性：$I(X,Y)\geq 0$ (如何证明？) 对称性：$I(X;Y)=I(Y;X)$ $I(X;Y)=0$ if and only if $X$ and $Y$ are independent random variables. MI 与其他量的关系 与条件熵与联合熵的关系（Relation to conditional and joint entropy） TO-DO 与 KL divergence 的关系 VariationsTO-DO Entropy(熵)针对离散型随机变量，在本文第一部分已经对entropy进行了定义，即：一个输出序列的entropy 是 该序列中各个输出(output)的self-information的加权和，其权重为各个对应的output出现的概率。形式化如下：$$\begin{equation}\begin{split}&amp;H(X)=\sum_{n=1}^N P(x_n)I(x_n)= -\sum_{n=1}^N P(x_n)\log P(x_n)\, , \\&amp;also\ denoted\ H(P)\, , where\ P \ is \ the \ 「PMF」 \ of \ random \ variable \ X. \\end{split}\end{equation}$$ 从更本质（概率统计）的角度我们可以 “重新审视 Entropy 的定义 ”，上述定义的形式从概率角度看来，不就是 “随机变量的函数的数学期望 ” 吗？将上述定义从概率角度 “拆解” 如下： 随机变量的取值：$x_1,\cdots,x_n,\cdots,x_N$ （离散） 离散随机变量：$X$ 随机变量的函数：$I(x_n)$ (i.e. $I(x_n)=-\log P(x_n)$)，$I(x_n)$ 本身也是一个随机变量。 数学期望的定义 离散随机变量：$E[g(X)]=\sum_{\rm x} p(x)g(x)$ 连续随机变量：$E[g(X)] = \int_{-\infty}^{+\infty}g(x)f(x)dx$ 对比 Entropy 的定义形式，即可知其概率上的本质乃为 Expected Self-Information or Expected Information Content. 因此，我们可以得到另外一种角度的形式定义。同时,尤其应该注意到截至目前为止，entropy的定义仅仅针对一个、离散型的随机变量$X$ ，考虑其在某个image($\mathcal X$) 上所含有的uncertainty$—$ entropy. Formal Definition对于一个离散随机变量$X$, 它的可能取值为 ${x_1,\cdots, x_n}$ 并且 其概率质量函数(probability mass function)为 $P(X)$,$$\color{red}{H(X)= E_{x\sim P }[I(x)]=E_{x \sim P}[-\log P(x)]=-E_{x \sim P}[\log P(x)]}$$这里，$E​$ 期望操作符，$I​$ 是$X​$的self-information or information content. $I(X)​$ 本身就是一个随机变量。Entropy可以显式地写为：$$\color{red}{H(X)=\sum_{i=1}^n P(x_i)\, I(x_i)=-\sum_{i=1}^n P(x_i)\log_b P(x_i),}$$其中，$b$是对数函数的底数，通常取 2, e, 10等。 注意： 这里定义的 Entropy 是针对一个随机变量$X$ 的可能的取值集合而言（i.e. image is $\mathcal X$ ），而不是仅仅针对$X$的某一个具体的取值而言。 Joint Entropy在信息论中，joint entropy 是对 “由若干随机变量组成的集合” 的不确定性的一种衡量[^4]. Formal DefinitionThe joint entropy of two discrete random variables $X$ 和 $Y$ is defined as:$$\color{red}{H(X,Y)= -\sum_{x\in \mathcal X} \sum_{y\in \mathcal Y} P(x,y)\log [P(x,y)]}$$Where $P(\cdot)$ is joint probability distribution, $\mathcal X$ and $\mathcal Y$ is image of random variable $X$ and $Y$, $x$ and $y$ are perticular values of $X$ and $Y$. For more than two random variables $X_1,…, X_n$ 可以扩展为：$$\color{red}{H(X_1,…,X_n)= -\sum_{x_1} \cdots \sum_{x_n} P(x_1,…,x_n) \log [P(x_1,…,x_n)]}$$ 联合熵的性质 非负性：$$\begin{equation}\begin{split}&amp;H(X,Y)\geq 0 \&amp;H(X_1,…,X_n)\geq 0\end{split}\end{equation}$$ 对称性：$H(X,Y)=H(Y,X)$ 若干个随机变量的联合熵，不小于单个随机变量的熵 The joint entropy of a set of random variables is greater than or equal to all of the individual entropies of the random variables in the set.$$\begin{equation}\begin{split}&amp;H(X,Y) \geq max{ H(X),H(Y) } \&amp;H(X_1,…,X_n) \geq max { H(X_1),…,H(X_n)}\end{split}\end{equation}$$​ 若干的随机变量的联合熵，不大于 单个随机变量的熵的和 The joint entropy of a set of random variables is less than or equal to the sum of the individual entropties of the random variables in the set. 以下不等式中，当且仅当 $X$ 和 $Y$ 相互独立(independent)的时候等号成立。$$\begin{equation}\begin{split}&amp;H(X,Y) \leq H(X)+H(Y)\&amp;H(X_1,…,X_n) \leq H(X_1)+…+H(X_n)\end{split}\end{equation}$$ Joint entropy与其他量的关联 与互信息(mutual information)的关联 $I(X;Y)=H(x)+H(Y)-H(X,Y)$ 与条件熵(conditional entropy)的关联 $H(X|Y)=H(X,Y)-H(Y)$ 或者写 $H(X,Y)=H(X|Y)+H(Y)$ and $H(X_1,…,X_n)=\sum _{k=1}^n H(X_k|X_{k-1},…,X_1)$, 这有些类似概率性质中的chain of rule(链式法则). Conditional Entropy(条件熵)定性刻画：为了描述 ”当已知一个给定随机变量X的值时，另一个随机变量Y可能的取值情况“ 所需要的信息总量 (amount of information), 称之为条件熵。The entropy of $Y$ conditioned on $X$ is written as: $H(Y|X)$ . Formal DefinitionIf $H(Y|X=x)$ is the entropy of discrete random variables $Y$ conditioned on the discrete random variable $ X$ taking a certain value $x$, then $H(Y|X)$ is the result of averaging $H(Y|X=x)$ over all possible values $x$ that $X$ may take. 给定一个离散随机变量$X$ 及它的像(image) $\mathcal X$ 和 离散随机变量$Y$及它的像(image) $\mathcal Y$ ，则，$Y$在给定$X$时的条件熵 被定义为：$H(Y|X=x)$对 $x$ 每一个可能的值的加权和，其权重为$p(x)$[^3]:$$\begin{equation}\begin{split}H(Y|X) &amp;= \sum_{x \in \mathcal X} p(x)\, H(Y|X=x) \\&amp;=-\sum_{x \in \mathcal X}p(x)\sum_{y\in \mathcal Y} p(y|x)\log p(y|x) \\&amp;=-\sum_{x\in \mathcal X}\sum_{y\in \mathcal Y}p(x,y)\log p(y|x) \\&amp;= -\sum_{x\in \mathcal X}\sum_{y\in \mathcal Y}p(x,y)\log \frac{p(x,y)}{p(x)} \\&amp;=\ \ \ \sum_{x\in \mathcal X}\sum_{y\in \mathcal Y}p(x,y)\log \frac{p(x)}{p(x,y)} \\\end{split}\end{equation}$$ *Cross-Entropy(交叉熵)交叉熵(cross-entropy) 与之前讲的所有对于information content 或者 uncertainty的measure 都不同的的地方在于，前者们针对的是一个或多个随机变量去一个特定的值或在一个image上取所有值的情况；而交叉熵是针对两个不同的概率分布(probability distribution)，关注的对象从random variable转移到了 probability distribution. 也就是说，之前无论是对一个随机变量还是多个随机变量的某些方面的measure，都是限定在同分布(基于一个probability distribution$P(\cdot)$ ), 而现在要涉及2个概率分布：$P(\cdot)$ 和 $Q(\cdot)$ . Formal definitionCross-entropy between two probability distributions $P$ and $Q$ over the same underlying set of events measures the amount of information[^5]needed to identify an event drawn from the set, if a coding scheme is used that is optimized for an “unnatural” distribution $Q\, $, rather than the “true” distribution $P$ . 现在，我们在同一个随机变量 $\rm X$ 上有两个概率分布函数 $P$ 和 $Q $, 这两个分布的交叉熵定义如下：$$\color{red}{H(P,Q)= E_{X\sim P}[-\log Q(x)]=-E_{X\sim P}[\log Q(x)]}$$对于 $X$ 是离散随机变量的情况，上述定义可以具体地写为：$$\color{red}{H(P,Q)= -\sum_x P(x)\log Q(x)}$$需要注意的是，在交叉熵的定义中 $P$ 是那个“生成数据的分布” 也即 “true distribution”，$Q$ 是那个我们需要通过优化参数得到的分布。两者有主次之分，$P$ 为主，$Q$ 为次。 去其它量的关联 与KL divergence的关联 $H(P,Q) = H(P) + D_{KL}(P||Q)$ *KL divergence (relative entropy)KL divergence 是衡量一个概率分布与另一个分布的差异度。 同时，KL divergence并不是一种“真正的” metric， 因为 $D_{KL}(P||Q) \neq D_{KL}(Q||P) $, 也就是说它不具有对称性。 Formal Definition$$\color{red}{D_{KL}=E_{X\sim P}[\log \frac{P(x)}{Q(x)}]=E_{X\sim P}[\log P(x)-\log Q(x)]}$$ 离散随机变量的KL divergence$$\color{red}{D_{KL}(P||Q)=\sum_x P(x)\log \frac{P(x)}{Q(x)}=-\sum_x P(x)\log \frac{Q(x)}{P(x)}}$$换句话说，是 $P$ 和 $Q$ 的对数差异[^6]的期望，权重为 $P(x)$. 连续随机变量的KL divergence$$D_{KL}(P||Q)=\int _{x\in \mathcal X}p(x)\log \frac{p(x)}{q(x)}\,dx$$Where $p$ 和 $q$ denote the densities of $P$ 和 $Q$ . 与其他量的关联 与cross-entropy的关联 $$D_{KL}(P||Q) = H(P,Q) - H(P)$$ 总结 Self-information 是针对一个随机变量的某一给定取值；PMI针对两个随机变量分别给定某个取值；MI是针对两个随机变量可能的取值集合。 Shannon entropy 是针对一个随机变量的可能取值的集合，是self-information的期望，i.e. 自信息的加权和，权重为$P(x)$ ; MI 是 PMI的期望，i.e. 加权和，权重为$X$ , $Y$的联合分布$ p(x,y)$ ; Joint entropy 是针对多个随机变量的可能取值集合，可以看作是一个随机变量序列的shannon entropy，也可以看作是一个随机变量的self-information的加权和，权重为联合分布。 Cross entropy是针对两个概率分布的，具有对称性，与KL divergence 关系密切；在machine learning的context中，经常作为分类问题的 loss函数$—$交叉熵loss，也叫 logistic loss 和 log loss(当标签为{+1, -1}时候) 。 KL divergence是针对两个概率分布的，非对称性，与cross entropy 关系密切。 [^0]: 根据随机变量的的定义，其一个具体取值对应一个实验结果的集合(a set of outcomes)；根据随机事件的定义可知，进而对应一个随机事件。[^1]: 不同的底数选取仅仅是不通信息单元（如：bytes, bits, nats）下信息量不同程度的rescaling罢了，不影响问题本身。[^2]: 每一个输出可以看作一个随机变量或者随机事件，因此整个随机序列可以看作N个随机变量乘积。[^3]: $p(x)$ 是 $P(X=x)$ 的简写。 [^4]: joint entropy is a measure of the uncertainty associated with a set of random variables.[^5]: 也常称为 information content, e.g. the average number of bits needed.[^6]: $\log P(x) - \log Q(x)$]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Entropy</tag>
        <tag>Information</tag>
        <tag>Cross Entropy</tag>
        <tag>熵</tag>
        <tag>交叉熵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一.The Learning Problem]]></title>
    <url>%2Farchive%2F2018-06-20%2F%E4%B8%80-The-Learning-Problem%2F</url>
    <content type="text"><![CDATA[前言这一系列文章是我在学习台大林轩田教授的机器学习课程的笔记，要比较好地理解相关内容，需要以下学科的基础知识： 微积分 概率论 统计 线性代数 A productive programming language (e.g. Python, C++, etc.) PS: 如果有一点优化（optimization）的知识就更好了。 本文纯属个人的课程笔记，因此采用便于自己理解和记忆的方式撰写而成，并没有考虑读者的知识背景，敬请谅解。 课程大纲 When Can Machines Learn?（何时可以使用机器学习） 第一讲：The Learning Problem（机器学习问题概述） 第二讲：Learning to Answer Yes/No [二元分类问题] 第三讲：Types of Learning（各种机器学习问题） 第四讲：Feasibility of Learning（机器学习的可行性）(公布作业一) Why Can Machines Learn? （为什么机器可以学习） 第五讲：Training versus Testing（训练与测试） 第六讲：Theory of Generalization（泛化的原理） 第七讲：The VC Dimension（VC纬度） 第八讲：Noise and Error（噪音与误差）（公务作业二） 以上就是台湾大学林轩田老师讲授的 “Machine Learning Foundations”课程的大纲，可见这门课是侧重夯实机器学习的基础，为下一门姐妹课“机器学习技巧”做铺垫。 课程共8周，每周投入时间不少于3+小时比较妥当。 The Learning ProblemA Concrete Definition Improving some performance measure with experience learned from data. 上面这句话的意思是指机器学习是这样一个过程: 用从数据中学习到的某种经验，并在某种度量标准之下提升性能的过程。关于机器学习的定义，最权威的定义来自于CMU的Tom Mitchell教授，他是世界级机器学习专家和开拓者，相关内容可以参考其经典著作：《Machine Learning》- Chapter 1. Formalization of Machine Learning之前的定义是一种观念上的定义，要使得机器学习可以具体实现，就要借助数学工具，这就需要我们能够给出形式化的定义。 Basic Notations Input: $\mathbf{x}\in\mathcal{X}$ Output: $y\in\mathcal{Y}$ $\text{Some }\color{blue}{\text{unknown pattern }}\text{to be learned} \Leftrightarrow \color{blue}{\text{Target Function}}$: $f: \mathcal{X}\rightarrow\mathcal{Y}$ $\color{blue}{\text{Data}}\Leftrightarrow \text{Training Examples}$: $\mathcal{D}={(\mathbf{x}_1,y_1),(\mathbf{x}_2,y_2),…,(\mathbf{x}_N,y_N)}={(\mathbf{x_n},y_n)}_{n=1}^N$ $\color{blue}{\text{Hypothesis}}\Leftrightarrow \textbf{Function or some kind of functions with good performance}​$: $g:\mathcal{X}\rightarrow \mathcal{Y} \text{ (Learned from the data.)}$ 上述基本概念有一个隐含的假设： $f$ 是数据背后隐藏的“真理”，这个“真理”我们认为他是以函数或者叫做映射(mapping)的方式表现的。而$\mathcal{D}$ 是数据集，我们认为它是由 $f$ 生成的。 这个概念应该这样理解：某个“真理”的存在对应着某种pattern，这种pattern由真理主宰下生成的数据来表现（implicit），而机器学习就是从数据中学到一个或者一类函数$g$，使得其与真理$f$ 尽可能的近似(如何定义近似？后面的课程会讲到)。 总之，机器学习是这样一个过程：$$\lbrace (\mathbf{x}_n,y_n)\rbrace_1^N \color{blue}{\text{ from}}\ \color{blue}{f} \rightarrow \text{Machine Learning}\rightarrow \color{blue}{g}$$ Practical Definition of Machine Learning 对上图做以下解释： Target $\color{blue}{f} \color{red}{\text{ unknow }}$(i.e. 没有解析式or可编程式的定义) Hypothesis $\color{blue}{g}$ is hopefully $\approx \color{blue}{f}$, but possibly different from $\color{blue}{f}.$ Assume $\color{blue}{g}\in \mathcal{H}={h_k}$ $h_1:\text{annual salary}\ge 100000\text{ rmb.}$ $h_2:\text{debt}\le 20000\text{ rmb.}$ $h_3:\text{year in job}\ge 3\text{ years.}$ $\mathcal{H}:\color{blue}{\text{Hypothesis set}}$ can contain good or bad hypothesis. Up to A to pick the “best” one as $\color{blue}{g}$ Summary根据前文，我们可以得出以下结论：$$\begin{align}&amp;\color{red}{\textbf{Learning Model}= \mathcal{A}\text{ and }\mathcal{H}}\\&amp;\color{red}{\textbf{Machine Learning:}} \text{ Exploit data }\color{blue}{\mathcal{D}} \text{ to compute hypothesis } \color{blue}{g} \text{ that } \color{purple}{\text{approximates }}\text{Target } \color{blue}{f}.\end{align}$$ END]]></content>
      <categories>
        <category>课程笔记</category>
        <category>Machine Learning Foundations</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>NTU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Language Model-(二)]]></title>
    <url>%2Farchive%2F2018-06-15%2FLanguage-Model-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[Markov Models在上一篇文章中对语言模型进行了感性认知和形式化地定义，并给出一个最简单的LMs,虽然它不强大。 现在基于我们之前的形式化定义，系统性地分析：如何estimate 一个句子的联合概率？ 考虑一个随机变量序列，$X_1,X_2,…,X_n$ . 每个随机变量的取值都可能是 $\color{blue}{\mathbb{V}}$ 中的任意一个值。现在我们假设：序列的长度 n 是一个固定的值，比如：n=100. 我们的目标是：面对任意序列 $x_1,x_2,…,x_n$ ，其中- $n\ge 1$, $x_i \in \color{blue}{\mathbb{V}}$ for $i=1,2,…,n-1$ 我们需要estimate 一个联合概率：$$P(X_1=x_1,X_2=x_2,…,X_n=x_n)$$根据概率论中的链式法则(Chain Rule):$$\begin{align}&amp;P(X_1=x_1,X_2=x_2,…,X_n=x_n)\\&amp;= P(X_1=x_1)P(X_2=x_2|X_1=x_1)…P(X_n=x_n|X_1=x_1,…,X_{n-1}=x_{n-1})\\&amp;=\prod_{i=1}^{n} P(x_i|x_{1}^{i-1})\end{align}$$ Markov Assumption上面，我们已经将联合概率转化为一些列条件概率的乘积，但似乎亦然无法帮助我们计算一整个句子的联合概率。因为我们不知道什么方法能够计算$P(x_i|x_1^{i-1})$, 也即：我们无法计算给定一个特定词之前很长的词序列的情况下，该词出现的概率。正如前一篇文章提到的那样，我们无法数出 $C(x_1,x_2,…,x_{i-1},x_i)$, 因为句子是无穷的，任何一个特定的context都可能是从来没有出现过的。 因此，我们需要通过一些假设约束来使得这个计算可行。马尔可夫假设背后的朴素想法是这样的： 在一个随机变量序列,$X_1,X_2,…,X_n$ 中, 任何一个随机变量取某一个特定值的概率只与它之前的少数几个随机变量相关，而与离它更远的随机变量没有关系。 我们现在形式化地表示马尔可夫假设， 一阶马尔可夫假设(First-Order markov assumption)： $P(x_i|x_1^{i-1})\approx P(x_i|x_{i-1})$，从而整个随机变量序列的联合概率为$$P(X_1=x_1,X_2=x_2,…,X_n=x_n)=\prod_{i=1}^{n} P(x_i|x_{i-1})$$ 二阶马尔可夫假设(Second-Order Markov assumption): $P(x_i|x_1^{i-1})\approx P(x_i|x_{i-2},x_{i-1})$, 此时整个随机变量序列的联合概率为$$P(X_1=x_1,X_2=x_2,…,X_n=x_n)=\prod_{i=1}^{n}P(x_i|x_{i-2},x_{i-1})$$ 这样，我们计算联合概率就简单多了，将这个思想用来建模句子的联合概率，就是N-Grams Language Model。 N-Grams 语言模型将上述概率理论应用于语言模型，即：estimate 一个句子的联合概率：$P(w_1,w_2,…,w_n)$ 基于Markov 假设，很容易理解N-Grams语言模型。这里一用Dan Jurafsky一句很形象的话来描述： The intuition of the N-gram model is that instead of computing the probability of a word given its entire history, we can approximate the history by just the last few words. Bigram 用来 $P(w_n|w_{n-1})$ 近似$P(w_n|w_1^{n-1})$. 换句话说，我们不是计算：$$P(\text{university}|\text{Clition’s book store near my})$$而是计算：$$P(\text{university}|\text{my})$$总之，我们做了以下近似：$$P(w_n|w_1^{n-1})\approx P(w_n|w_{n-1})$$ N-gram 类似的思想，我们做以下近似：$$P(w_n|w_1^{n-1})\approx P(w_n|w_{n-N+1}^{n-1})$$ 计算整个句子的联合概率如果基于bigram模型，整个句子的联合概率可以按照下面公式计算：$$P(w_1^{n})\approx \prod_{i=1}^n P(w_i|w_{i-1})$$可见只要我们会计算$P(w_i|w_{i-1})$ , 句子的联合概率我们就可以计算了。那么现在的问题是，我们如何计算bgrams or N-gram 概率呢？一个直观的估计概率的方法叫做极大似然估计(Maximim likelihood estimation,MLE). 极大似然估计（MLE）用MLE对N-Grams模型的参数进行估计：从语料库中计数，并把这个计数归一化(Normalize), 使得计数在0，1之间。 举个例子，要计算一个具体的bigram：$\text{xy}$ 的概率，我们要数一下$C(\text{xy})$, 然后用所有第一个字是 $\text{x}$ 的bigram的个数进行归一化，即：$$P(w_n|w_{n-1})=\frac{C(w_{n-1}w_n)}{\sum_{w} C(w_{n-1}w)}$$其实，我们可以简化上述公式：因为$C(w_{n-1}w_n)=C(w_{n-1})$ . 即：以 $\text{x}$ 为首词的 bigram 的个数必然等于 $\text{x}$ 出现的次数。所以，$$P(w_n|w_{n-1})=\frac{C(w_{n-1}w_n)}{C(w_{n-1})}$$ 一个具体例子-bigram终于到了具体例子了，我平时在看书的时候最怕全是逻辑描述、定义、公式而没有具体例子。实际上具体例子对于概念和问题的理解是非常重要的！ 考虑我们现在有一个微型语料库（Corpus），只包含三个句子。 我们需要在语料库中每个句子的开头添加个一个特殊符号：[BOS] 标明这是句子的开始。 在每个句子的结束也添加一个特殊符号：[EOS] ，标明这是句子的结束。 语料库为： [BOS] I am Sam [EOS] [BOS] Sam am I [EOS] [BOS] I do not like green eggs and ham [EOS] 我们现在计算一些bigrams的概率： $$\begin{align}&amp;P(\text{I|[BOS]})=\frac{2}{3}, &amp;&amp;P(\text{Sam|[BOS]})=\frac{1}{3}, &amp;&amp;&amp;P(\text{am|I})=\frac{1}{3}\\&amp;P(\text{[EOS]|Sam})=\frac{1}{2}, &amp;&amp;P(\text{Sam|am})=\frac{1}{2}, &amp;&amp;&amp;P(\text{do|I})=\frac{1}{3}\end{align}$$ 通用情况的MLE N-gram参数估计： $$P(w_n|w_{n-N+1}^{n-1})= \frac{C(w_{n-N+1}^{n-1}w_n)}{C(w_{n-N+1}^{n-1})}$$ 也就是就是我们上面讲过的：用一个特定的N-grams的个数除以其prefix的个数。这个比值我们称为相对频率（relative frequency）. 这种方法在MLE中是估计参数的一种常用手段（不是唯一手段）。在MLE中，在给定模型的情况下，最后的结果参数集(resulting parameter set)会最大化训练数据 T 的似然，i.e. $P(T|M)$。 现在我们基于bigram语法计算一下整个句子的联合概率： $$\begin{align}P(\text{[BOS] I am Sam [EOS]})&amp;= P(I|[BOS])P(am|I)P(Sam|am)P([EOS]|Sam)\\&amp;=\frac{2}{3} \cdot \frac{1}{3}\cdot \frac{1}{2}\cdot \frac{1}{2}\\&amp;=0.056\end{align}$$ 以上的内容，就是所谓基于计数的N-gram语言模型（Count-based N-gram Language Model），此外还有Neural Language Model等其他模型，会在之后的文章中陆续介绍。 N-Gram 能捕获什么？如果我们的语料库足够大的话，我们计算的 bi-gram 概率确实能够encode一些语言学现象，比如syntantic 动词之后往往接名词、代词、介词(这类bigram的概率较高) 形容词之后往往接名词 有些可能捕获不是syntantic现象，而是个人习惯： I want… I should…. 有些可能捕获的是文化方面的现象： Chinese food… English book… 总之，N-gram 模型确实可以捕获到很多语言学、文化、社会、个人偏好等现象，所谓 Encode Some Facts。可想而知，随着N不断变大，Trigram, 4-gram, 5-gram 等将会encode 更多有意义的信息。 一些特殊问题上面的例子为了计算简单，我们举了基于bigram的例子。实际上，当我们的语料库足够大的时候，Trigram，乃至4-gram，5-gram是更加合理的选择（更加强大！）。这里我们以 Trigram为例 进行说明。 在我们选择 Trigram 模型时，我们对语料中每个句子开始和结尾的context 都需要进行扩展。即：语料中的句子应该处理成这样： $$\text{[BOS] [BOS] I want to drink beer [EOS] [EOS]}$$ Log format of probability. 我们在计算和表示语言模型的概率的时候常常采用它的对数形式。因为概率值是一个[0,1]区间的数值，语言模型中概率值是连乘的形式，因此一个句子的联合概率可能会是一个非常小的数字，可能会导致下溢出。因此我们采用Log Probability 来避免下溢出的问题。 在log space 中做加法与在linear space做乘法是等价的。我们所有对log probability 进行的计算和存储都可以转化回概率, $$ p_1\times p_2 \times p_3…\times p_n = exp(\log p_1 +\log p_2 +…+\log p_n)$$ 如何评价语言模型评价语言模型的方式主要分为两类。 外部性方法（Extrinsic）将语言模型嵌入到一个实际的应用中，然后测量这个应用的性能的提升有多少，这本质是一种End-to-End的评价方式，也是最好的评价方式。Extrinsic 的评测方式是唯一的一种可以评价一个组件中性能的提升是否真的帮助了你的任务的评测方式。其过程如下： 找一个实际Task（这个任务必须以LM作为其组件(components)之一） End-to-End的训练 Task with $LM_a$ , 计算性能, $P_a$.（比如：Acc，P，R，F） Task with $LM_b$, 计算性能, $P_b$. 比较 $P_a$ and $P_b$，得到比较的结果$C$. ( Compare with some certain metric.) 重复步骤2（N-次，N=0,…,1），计算加权平均的$C_{avg}$, 以此来衡量 $LM_a$ 与 $LM_b$ 哪个好，有多好。 然而，这种“最好”的评测方式有一个很大的缺点，那就是为了评测语言模型本身性能的好坏，你还需要找另外一个实际的任务，并以end-to-end的方式训练，比较若干次训练结果的（训练若干次）。这个整个过程非常昂贵的(时间、计算资源)。 内部性方法（Intrinsic）鉴于Extrinsic的评测方法的缺点，我们希望有一种测量指标可以快速地评测语言模型其潜在性能。Intrinsic 的评价方法就是一种可以独立于具体应用来评测一个语言模型的性能的评价方法。 用Intrinsic的评价方法来评测语言模型，其过程如下： 在training set（training corpus） 上训练我们的语言模型 $LM_a, LM_b$. 用已经训练好的语言模型 $LM_a,LM_b$ （trained），来model test set （test corpus）。以此比较哪个语言模型更好。 然而，所谓的 “model test set“ 是什么意思？ 答案很简单：哪个模型给 test set 赋予了更高的概率-即：更准确地在test set上做出了预测，那么它就是一个更好的模型。 其核心institution就是：在给定两个概率模型（probability model）的情况下，更好的那个模型更好地拟合了test data，或者说更好的预测了test data的细节，也就因此会对test data 赋予一个更高的概率。 此外还需要注意几个问题： test set 不能和 training set 有重合。 如果重合，那么在训练model 的时候就已经引入了一些偏见（bias），即：model更加侧重那些重合的部分（可以这样考虑，同样的句子多次被model拟合，模型就更加偏向给这些句子打更高的概率），这会使得在 test set 上的预测出奇的好，这很容易理解：因为模型已经见过一大部分这些数据了，模型对它们“不陌生”，自然就预测的好。但实际上，这对模型的泛化（generalization）能力有很大的影响。 有些时候，我们还需要一个dev set. 那corpus 如何划分为：training set， dev set，test set呢？ 我们需要尽可能多的training data，因为我们希望模型learning 的足够充分（不要欠拟合！） test set 不能太小，因为太小的话它的代表性不够强。 上述两条似乎有些冲突，此刻怎么办呢？我们希望选择：能够使我们有足够的统计信息来评测两个语言模型在统计上显著性差异的最小test set。 困惑度（Perplexity）在具体的实践中，我们往往不采用raw probability 作为metric来评价语言模型，而是采用了一个变种-perplexity。 定义： “The perplexity(sometimes called PP for short) of a language model on test set is the inverse probability of the test set, normalized by the number of words.” 给定一个test set, $W=w_1w_2…w_N$ :$$\begin{align}PP(W) &amp;= P(w_1w_2….w_N)^{-\frac{1}{N}}\\&amp;=\sqrt[N]{\frac{1}{P(w_1w_2…w_N)}}\end{align}$$我们可以用Chain-Rule来扩展上述公式，$$PP(W)=\sqrt[N]{\prod_{i=1}^N\frac{1}{P(w_i|w_1…w_{i-1})}}$$因此，如果我们基于一个bigram 语言模型来计算perplexity，我们得到：$$PP(W)=\sqrt[N]{\prod_{i=1}^N \frac{1}{P(w_{i}|w_{i-1})}}$$根据上述公式可知：word sequence的条件概率越高，perplexity的值越小，反之亦然。因此，minimize perplexity等同于maximize test set上的probability。 有以下2个细节需要注意： 上面公式中我们的计算是基于 test set中所有words 的序列，这就必然会跨越很多句子的边界。因此我们在计算概率的时候要记得加入[BOS] 和 [EOS]，也就是句子开始和结束的标志符。 我们还需要在test set中token总数N中计入每个句子的[EOS]，切记：不包括[BOS]!! 一个例子-计算perplexityTO-DO… 泛化和计数0的问题平滑-SmoothingKneser-Ney Smoothing困惑度与熵的关系后记]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>N-Grams</tag>
        <tag>Markov Models</tag>
        <tag>Markov Assumption</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Language Model-(一)]]></title>
    <url>%2Farchive%2F2018-06-15%2FLanguage-Model-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[感性认识-什么是Language Model 感性定义：能够对一个词序（word sequences）赋予一个合理的概率的模型，我们称之为语言模型-Language Model[^1]。 例子： “我明天请你去吃…..” 上述这个句子是一个所谓的 word sequences, 最后一个词是吃， 如果现在问你一个问题，你认为吃后面最可能出现的词是什么？从以下几个选项：饭、烤肉、足球、计算机，中选择一个。 正常人类都会认为饭 和烤肉 出现的概率要高些，而 足球 和 计算机 基本不可能。为了表示可能的程度，应该赋予这些词一个概率，如：$$\begin{align} &amp;P(\text{next_word= 饭}) =0.48 \\ &amp;P(\text{next_word= 烤肉})=0.48 \\ &amp;P(\text{next_word= 皮球})=0.03\\ &amp;P(\text{next_word= 计算机})=0.01 \end{align}$$ 上述的概率表明了我们对这个问题的认知，即：有些词是可能出现的，而有些词是不可能出现在吃 的后面。 从定义及上述例子即可得知，语言模型的作用是： 为每一个可能的next word 赋予一个概率。 给整个句子（word sequence）赋予一个概率。 其实，1和2 是等价的，下文会逐渐介绍。总而言之，语言模型就是使用概率、统计等工具来判断一个word sequence 是一个符合人类认知的句子的可能性的模型。 形式化定义基于上述感性的认识，在我们尝试形式化地定义Language Model[^2]. 首先，我们有一个英文语料（corpus），它由非常多的英语句子构成。 我们记 $\color{blue}{\mathbb{V}}$ 为该语料中所有词的集合，即：$\color{blue}{\mathbb{V}}={apple, football, cat, cats,…}$在实际应用中，$\color{blue}{\mathbb{V}}$ 通常非常大，几万、几十万，，但它是个有限集。 一个句子$s$ 是一个词序列，$$s = x_1x_2…x_n\$$其中，n&gt;=1。我们有 $x_i\in \color{blue}{\mathbb{V}}$ for $i\in{1,2,…,n-1}$ . 我们一般假设最后一个 $x_n$ 是一个特殊符号，如：STOP ，用来标记句子结束，并且它不属于 $\color{blue}{\mathbb{V}}$. 我们所处理的句子是如下这种样子： The dog barks STOP The boy smile STOP A girl is reading a book STOP …… 基于$\color{blue}{\mathbb{V}}$ ，我们可以定义一个集合$\color{blue}{\mathbb{S}}$ ，其为用$\color{blue}{\mathbb{V}}$ 中的词组成的所有句子集合。$\color{blue}{\mathbb{S}}$ 显然是一个无限集合，因为句子可以是任意长度的。在稍后给出 Language Model形式化定义前，先做几个说明。 表示一个特定的随机变量$X_i$ 取某个值$x_i$ 的概率，i.e. $P(X_i = x_i)$, 我们简记为：$P(x_i)\ or\ p_X(x_i)$. 我们以后简记一个长度为$n$ 的句子：$x_1,x_2,…,x_n$为 $x_{1:n}$ 或者 $x^n_1$. 我们将句子（词序列）中的每一个词看作一个随机变量。那么，句子中每个词取一个特定值时整个句子的联合概率应该是：$P(X_1 = x_1,X_2=x_2,…,X_n=x_n)$, 我们简记为：$P(x_1,x_2,…,x_n)$. 定义1(Language Model)： 一个Language Model 由一个有限集合$\color{blue}{\mathbb{V}}$ 及一个函数$p(x_1,x_2,…,x_n)$ ,满足如下两个约束： 对于任何 $&lt;x_1,x_2,…,x_n&gt;\in\color{blue}{\mathbb{S}}$, $p(x_1,x_2,…,x_n)\geq 0$. 并且：$$\sum_{&lt;x_1…x_n&gt;\in \color{blue}{\mathbb{S}}}p(x_1,x_2,…,x_n)=1$$这样一来，$p(x_1,x_2,…,x_n)$就是一个定义在在集合 $\color{blue}{\mathbb{S}}$ 中句子上的概率分布, 即：随机变量${X_1,…,X_n}$ 的PMF（probability mass function）. 定义完成了。那接下来, 在给定一个词之前的词(history)，如何计算（estimate）计算该词出现的概率，或者如何计算整个句子的联合概率呢？ 一个最简单的语言模型(Simple LMs)Task 1:考虑这样一个任务，计算$P(w|h)$。其中，$w$ 是一个具体的词，$h$ 代表history，即：$w$之前出现的词序列。 具体来讲，我们让 $h$=”The book store near my”, $w$ = “university”。计算：$$P(\text{university}|\text{The book store near my})$$一种最简单估计(estimate)上述概率的做法就是计算相对频率(relative frequency counts)： 我们在语料库中数一下 “The book store near my” 这个word sequence 出现的次数，记为：$C(\text{The book store near my})$. 我们继续数”The book store neary my university” 出现的次数。记为：$C(\text{The book store near my university})$. 计算二者的相对频率。 这样就可以如下回答这个问题，在我们看到history $h$ 的所有次中, 有多少次它背后跟的是 $w$：$$P(\text{university}|\text{The book store near my})=\frac{C(\text{The book store near my university})}{C(\text{The book store near my})}$$Task2: 我们要计算整个句子的联合概率（joint probability）：$p(s)$, i.e. $P(\text{The book store near my university})$. 我们只需要回答这样一个问题：基于$\color{blue}{\mathbb{V}}$ 的所有6个word sequences中，有多少个是 “The book store near my university”. 因此：$$P(\text{The book store near my university})=\frac{C(\text{The book store near my university})}{\color{blue}{\mathbb{|V|}}^6}$$好了，这个简单语言模型针对两种任务的计算过程就结束了。你有一个语料，你构建一个词汇表$\color{blue}{\mathbb{V}}$, 然后按照上述方式计算就好了。 我们现在看这种基于Task1，Task2简单模型能否充分满足我们的要求： 在“估计next word的概率” 中，这个模型要求我们能够数出来一个word sequence/句子在语料中出现的个数。现实中这种模型非常容易出现问题，因为上述例子中的句子简单的扩展，就会导致计数结果为0. 比如把句子改为：“Clinton‘s book store near my university.” 这句话在预料中就很难找到，甚至在google 中也无法找到这个句子。分子分母都为0，模型不work。 在估计整个句子的联合概率问题中，我们其实是要回答：基于$\color{blue}{\mathbb{V}}$，所有的由6个词组成的词序列中，”The book store near my university” 有多少个。分母太大了，尤其是当句子长度L变得比较大的时候，比如：25，会导致估计过程将十分艰难。 显然，我们需要更加鲁棒、强大的LMs. 当然，在此前我们仍然需要补充更多一点概率知识，因为新的LM需要概率作为支撑，下一篇文章会介绍Markov Models 以及基于它的 N-Grams LMs。 （PS: 强大的模型都需要概率作为支撑，记住这个公理就好）。 参考资料 [^1]: Speech and Language Processing-Chapter 4- Language Modeling with N-Grams by Dan Jurafsky.[^2]: Language Modeling- Course notes for NLP by Michael Collins.]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Language Model</tag>
        <tag>N-Grams</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Paper Daily]]></title>
    <url>%2Farchive%2F2018-06-11%2Fpaperdaily%2F</url>
    <content type="text"><![CDATA[20180611##name1]]></content>
      <categories>
        <category>Work</category>
      </categories>
      <tags>
        <tag>paper</tag>
        <tag>reading list</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Product in Linear Algebra]]></title>
    <url>%2Farchive%2F2018-05-25%2FProduct-in-Linear-Algebra%2F</url>
    <content type="text"><![CDATA[线性代数(Linear Algebra)中的向量(vector)、矩阵(Matrix)、张量(Tensor)之间涉及最基本的运算之一就是：Product，本文对此做一个简要的总结。 一. Vecotr-Vector MultiplicationDot product(or inner product)###Outer Product]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Linear Algebra</tag>
        <tag>Matrix Product</tag>
        <tag>Vector Product</tag>
        <tag>Tensor Product</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Attention in NLP(二)]]></title>
    <url>%2Farchive%2F2018-04-08%2FAttention-in-NLP-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[什么是Self-Attention?哈哈，这是一个测试。 $\textcolor{red}{哈哈，这是一个测试。}$ Self-Attention 的几个variants1.2.具体应用领域Natural Language InferenceSemantic Role LabelingSemantic RelatednessSentiment/Text ClassificationAspect-level Sentiment Classification总结]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Attention</tag>
        <tag>Self-Attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Attention in NLP(一)]]></title>
    <url>%2Farchive%2F2018-04-01%2FAttention-in-NLP-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[Attention 前传NLP中Attention的基本思想是：当处理一个句子[^1] 的时候，不是平等地关注句子中所有的部分[^2]，在每个时刻应该有选择地关注最应该关注的部分，而其他部分则相对地忽略。这种对不同部分关注程度不一样的机制就称为“attention”，它是通过赋予句子不同部分不同的权重来实现的：重要的部分权重高；次要的部分权重低。 总而言之，Attention思想的目标是：计算多个不同信息各自重要程度，选出中最重的那部分并加以利用。 NLP 中的 AttentionNLP中Attention思想的使用最初来自于机器翻译领域的一篇paper[^paper1], 使用Encoder-Decoder的框架在机器翻译任务重采用了Attention机制，极大地提升了翻译性能。鉴于Attention最初出现的场景如此，因此下面首先介绍通用Encoder-Decoder框架，然后介绍在自然语言处理领域其具体表现形式，并由机器翻译为切入点详细介绍Attention机制的工作原理、分支、内部模块的实现细节。 Encoder-DecoderEncoder-Decoder框架本身和Attention机制没有必然联系，它是独立存在的一种框架，而且在NLP研究领域中它出现的要比 Attention 机制早得多。只要一个问题的解决需要将输入数据先转换成一个中间态，然后再将中间态转化为最终结果，这就符合编码器-解码器的范式，就可以在Encoder-Decoder框架之下考虑解决该问题。所谓Encoder-Decoder框架，顾名思义可以这样理解： 有一个编码器-Encoder 对应有一个解码器-Decoder Simplified-Algorithm： 将数据 $\mathbf{X}$ 送入Encoder，输出一个编码的结果 $\mathbf{C}$（也就数据的中间态） 将 $\mathbf{C}$ 送入Decoder，输出最终结果 $\mathbf{Y}$ 一个通用的Encoder-Decoder框架可以由下图来描述： 具体在NLP领域中，中间状态$\mathbf{C}$ 就是所讲的Text Representation(文本表示)或者Text Encoding，在distributed representation的方法论中又叫做Embedding，为行文简化起见，以后在没有歧义的语境中统称为Representation[^3]。其对应的框架图就更加具体一些： 可以看到，只有中间状态发生了变化，即：文本的某种表示（包含了语法、语义、情感等各种信息）。 机器翻译（其他NLP的任务均可转化为此种类型的问题）中，Encoder-Decoder框架图就更加具体：输入 $\mathbf{X}$ 和输出 $\mathbf{Y}$ 均为序列[^4]，且输入为 Source语言（比如：英语）的句子，输出为 Target 语言（比如：汉语）的句子，Text representation $\mathbf{C}$ 通常是sentence representation(sentence embedding)。上面涉及的各个符号也就更加具体，通常为如下所示：$$\begin{align}&amp;\mathbf{X: {x_1,x_2,\dots,x_m}} \text{输入序列}\\&amp;\mathbf{Y: {y_1,y_2,\dots,y_n}} \text{输出序列}\\&amp;\mathbf{C}: \text{Text Representation，i.e. 文本表示（数据中间态）}\\&amp;\textbf{Encoder}: 函数，为线型或非线性。\\&amp;\textbf{Decoder}: 函数，为线性或非线性。\end{align}$$在这种具体情况下Encoder-Decoder框架入下图所示： 其中各符合含义如之前所示。$\mathbf{x_i}(i=1,\dots,m), \mathbf{y_j}(j=1,\dots,n)$ 是word vector，分别对应source语言和target语言，在distributed representation的语境下，即为：word embedding. $\mathbf{C}$ 是句子的representation，通常为vector[^5] $\in \mathbf{R^h}$. 其实NLP领域中的问题均可以某种程度上转化为机器翻译任务中的这种Encoder-Decoder框架，然后加以解决[^6]。机器翻译/NLP中的Encoder-Decoder可以这样理解：可以把它看作是用于处理由一个词序列（句子，篇章 etc.）生成另一个词序列（句子，篇章 etc.）的通用框架。[^7] 给一个句子对, 记为：$\color{blue}{&lt;Source, Target&gt;}$ ，我们的目标是给定输入句子 $\color{blue}{Source}$, 并通过Encoder-Decoder框架来生成目标句子 $\color{blue}{Target}$. 这里的称为 Source 和 Target 只是表明他们分别是Encoder-Decoder的输入输出，它们可以是同一种语言，也可以是不同语言；可以是机器翻译的场景，也可以是SRL、Text entailment的场景。Source 和 Target 分别由各自的单词序列构成：$$\begin{align}&amp;\color{blue}{Source}={\mathbf{x_1,x_2,\dots,x_m} }\\\&amp;\color{blue}{Target}={\mathbf{y_1,y_2,\dots,y_n} }\end{align}$$所谓Encoder，顾名思义就是对输入句子Source进行编码，将输入句子通过一系列非线形变换转化成为中间状态-representation:$$\mathbf{C=E(x_1,x_2,\dots,x_m)}$$解码器Decoder的任务是，根据Source 的representation $\mathbf{C}$ 和之前Decoder端已经生成的历史信息 $\mathbf{y_1,y_2,\dots,y_{i-1}}$ 生成$i$ 时刻要生成的单词 $\mathbf{y_i}$ ：$$\mathbf{y_i=D(y_1.y_2,\dots,y_{i-1}, C)}$$每个 $y_i$ 都这样依次产生，这个过程迭代进行最终生成 Target= ${\mathbf{y_1,y_2,\dots,y_n}}$ 。那么整体看来，就是整个系统根据 Source句子生成了目标句子Target。如果Source句子是中文，Target是英文，那么这就是解决机器翻译问题的Encoder-Decoder框架；如果Source是一个包含的文本或句子，Target 是带有entity B-I-O的文本，这就是解决实体抽取问题的Encoder-Decoder框架；如果Source是一篇文章，Target是概括性的几个句子，这就是解决文本摘要的Encoder-Decoder框架；如果Source是一句问句，Target 是一个回答，这就是解决QA问题的Encoder-Decoder框架…… 无论Encoder 还是 Decoder，其本质就是一个函数，只不过这是一个复杂的多重复合/嵌套 函数，通过对数据多次的非线性及线型变换，分别做编码和解码而已。在Neural Networks-based 的模型中，Encoder和Decoder 可以分别是RNN，LSTM，GRU，CNN及各种composition-based neural networks等等，只不过Encoder 和 Decoder 均为RNNs的情况比较多而已，RNNs的使用并其无必然性。 结合本文最初讲的通用Encode-Decoder框架，不难理解其实其他模态的数据比如：音频、视频、图像、时间序列数据等均可以在这个框架下进行处理。常见的应用有：语音识别，Image Caption，Visual QA 等等。可见，Encoder-Decoder框架的用途是极其广泛的。 为简化起见，之后的例子均假设选择RNNs作为Encoder和Decoder。 Attention Model本节先介绍在机器翻译任务中最为常见的Soft Attention模型的基本原理，之后抛开Encoder-Decoder框架抽出Attention的本质思想，解释Attention机制作为一个独立存在的必要性及合理性，并介绍自17年以来非常火的Self-Attention的思想。 之前讲的Encoder-Decoder框架是没有结合Attention的，为也就是它没有明确地将“不同部分信息的重要性不同”这一因素专门考虑并融合到框架中。请观察其 Target句子的生成过程：$$\begin{align}&amp; \mathbf{y_1 = D(\color{blue}{C}) }\\&amp; \mathbf{y_2 = D(y_1,\color{blue}{C})}\\&amp; \mathbf{y_3 = D(y_1,y_2,\color{blue}{C})}\\&amp; \cdots \\&amp; \mathbf{y_n = D(y_1,y_2,\dots,y_{n-1},\color{blue}{C})}\end{align}$$其中D就是Decoder，也即用来解码的非线性函数。从上式可以看出，无论生成Target句子的那一个word，Decoder使用的Source句子的表示 $\mathbf{\color{blue}{C}}$ 是一样的，没有任何区别。 而表示 $\mathbf{C}$ 是根据 Source句子的每个单词经过Encoder 编码生成的，本质上 $\mathbf{C}$ 是作为生成Target单词时的Context 被Decoder考虑并加入到解码器的非线性函数 $\mathbf{D}$ 中。换句话说，现有框架下在生成 Target句子中的每个单词的时候考虑的 Context 是一样的，Source句子中的每个单词对每个 $\mathbf{y_j}$ 贡献是相同的[^8]。显然这是比较违反直观的，直观和经验上都告诉我们：同一个句子、篇章、文档中的不同的词在不同时刻的重要性显然不应该相同。 以机器翻译问题为例就可以很清晰地看到这一点： 比如现在要将英语句子 Source=”Tom chase Jerry.” 翻译成中文句子 Target=”汤姆追逐杰瑞”，在Encoder-Decoder框架下，是逐步生成中文单词[^9] “汤姆” “追逐” “杰瑞”。 在翻译“杰瑞”这个中文单词的时候， 之前讲的Encoder-Decoder模型中，每个英文单词对于翻译Target 单词的贡献是相同的，这显然是不对的。在翻译Target单词 “杰瑞”的时候，显然 Source单词 “Jerry” 提供了最重要的信息，而“Tom” 和 “chase” 相对次要的多。因此，对于“在时刻$i$ ,Source句子中不同单词的重要性是不相同的” 这一现象进行针对性地建模，使得我们每生成一个Target 单词的时候都能对Source句子中最重要的单词给予更多的关注从而提升翻译性能，是十分必要的。这个显示的建模方法就是 Attention机制。 当句子较短的时候，没有Attention机制的Encoder-Decoder模型问题还不大，毕竟dependency较短，将Source句子编码为一个表示 $\mathbf{C}$ 还不至于损失过多的有用信息。但如果句子较长，dependency距离太远，此时Source句子的所有信息都编码为 $\mathbf{C}$ ，就会损失较多的语法、语义信息，对翻译很不利。此时，单词自身的信息已经消失，可想而知会损失很多细节信息，而 Attention 机制让我们有了第二次[^10]利用 Source句子的机会，让我们将编码阶段Encoder “损失的信息” 利用起来。 上面的翻译过程，如果引入 Attention机制的话，在翻译 ”杰瑞“ 的时候应该体现英文单词对于 “杰瑞” 重要程度的不同，比如给出一个类似下面的权重分布值（概率分布）： （Tom，0.2）（chase, 0.3）（Jerry, 0.5） 每个英文单词的权重值，代表了Attention机制分配给每个英文单词的注意力大小。这对于正确翻译单词是有帮助的，因为引入了新的信息使得最相关部分占比最大。 同理，Target句子中的每个单词都应该有其对应的Source句子中单词的注意力分配权重的概率分布信息。这就以为着：每生成一个 Target单词 $\mathbf{y_i}$ , 原先都是相同且不变的“通用”表示(Context) $\mathbf{\color{blue}{C}}$ 会被替换成随着每个 Target单词 $\mathbf{y_i}$ 不断变化着的动态表示(Context) $\mathbf{\color{blue}{C_i}}$. 这就是理解 Attention的关键，即：由固定的中间Representation $\mathbf{C}$ 换成了由Attention机制根据当前输出单词 $ \mathbf{y_i}$调整成的变化的 $\mathbf{C_i}$ 。 引入 Attention机制的Encoder-Decoder 框架如下图所示： 生成Target句子单词的过程变成如下所示：$$\begin{align}&amp;\mathbf{y_1=D(\color{blue}{C_1})}\\&amp;\mathbf{y_2=D(y_1,\color{blue}{C_2})}\\&amp;\mathbf{y_3=D(y_1,y_2,\color{blue}{C_3})}\\&amp;\cdots \\&amp;\mathbf{y_n=D(y_1,\dots,y_n,\color{blue}{C_n})}\end{align}$$其中每个 $\mathbf{\color{blue}{C_i}}$ 代表Source句子中每个单词的注意力分配概率分布，对比上面的英汉翻译来讲，其对应的信息可能如下： $\mathbf{\color{blue}{C_1}}$= $\mathbf{C}$(“汤姆”) = g( 0.6$\ast$f(“Tom”)， 0.3$\ast$f(“chase”)， 0.1$\ast$f(“Jerry”) ) $\mathbf{\color{blue}{C_2}}$= $\mathbf{C}$(“追逐”) = g( 0.2$\ast$f(“Tom”)， 0.6$\ast$f(“chase”)， 0.2$\ast$f(“Jerry”) ) $\mathbf{\color{blue}{C_3}}$= $\mathbf{C}$(“杰瑞”) = g( 0.1$\ast$f(“Tom”)， 0.2$\ast$f(“chase”)， 0.7$\ast$f(“Jerry”) ) 其中 f 代表Encoder中将Source句子中的单词转化为其word representation的函数，比如常见的采用RNN的Encoder中，f 就是RNN在某个时刻 t 的隐层输出$h_t$ . 实际使用过程中可以采用各种不同的函数来代替RNN都是可以的，区别无非是该函数表示能力强弱的问题；g 代表Encoder根据每个单词的word representation合成整个句子representation的函数[^11], 现在普遍做法中，g 函数就是对构成元素进行加权求和，即：$$\mathbf{\color{blue}{C_i = \sum_{j=1}^{L_s}\alpha_{ij}h_j}}$$其中，$L_s$ 表示Source句子的长度，$\mathbf{C_i}$ 表示当翻译Target单词$\mathbf{y_i}$ 是的Context，即此刻Source句子的representation，$\mathbf{h_j}$ 是Encoder处理Source句子中第$\mathbf{j}$个单词时得到的representation（$\color{Red}{!!}$ 这里往往不仅仅是Source句子中地$\mathbf{j}$个单词的表示，因为这个表示可能包含了之前各个单词的implict information，比如在使用RNNs的Encoder中。因此，更准确的说法是：$\mathbf{h_j}$ 是Encoder在时刻$\mathbf{t=j}$ 时对Source句子的表示 ）, $\mathbf{\alpha_{ij}}$ 代表在翻译Target句子中第 $\mathbf{i}$ 个单词时候由Attention机制分配给Source句子中第 $\mathbf{j}$ 个单词的注意力分配权重。 数学公式对应的过程如下图所示： 现在还有一个问题：生成Target句子的某个单词，比如“汤姆”的时候，Attention机制是如何给Source句子中的单词分配注意力权重的概率分布呢？为了说明这个问题，我们需要细化Encoder-Decoder框架，在一种具体的配置中解释这个问题。我们选择Encoder和Decoder短都使用RNN来举例，采用RNN的引入Attention机制的Encoder-Decoder框架如下： 其中 $\mathbf{h_j^s}$ 是RNN在Source句子中第$\mathbf{j}$ 个时刻/单词的隐层表示，$\mathbf{h_i^t}$ 是Target句子第 $\mathbf{i}$ 个时刻[^12]的隐层表示。$\mathbf{h_m^s}$ 是Source句子最后一个时刻 RNN的隐层表示，也即：整个Source句子在Encoder端的最终representation $\mathbf{C}$。$\mathbf{}$ 是句子标志句子结束的符号。 在上述框架下，Attention机制生成对Source句子中单词的权重概率分布的过程如下所示： 其中，$\mathbf{\hat\alpha_{ij}}$ 是Decoder端生成Target句子第$\mathbf{i}$ 个单词时，在Encoder端对Source句子中第 $\mathbf{j}$ 个单词分配的注意力权重，$\mathbf{\alpha_{ij}}$ 是经过$\mathbf{sofrmax()}$ 归一化之后的$\mathbf{\hat\alpha_{ij}}$，$\mathbf{C_i}$ 含义同前文。$\mathbf{score()}$ 是一个打分函数，计算Decoder端的向量$\mathbf{h_i^t}$ 与Eecoder端的$\mathbf{h_j^s}$ 的相关分数。$\mathbf{h_0^t}$ 是在Decoder端的“第一个”向量，用它来生成第一组注意力权重分配的概率分布，其通常由2种方式生成： 随机初始化，通过然后通过整个Encoder-Decoder的训练过程fine-tuned。 通过其他手段对Source句子进行表示，将得到的表示赋予 $\mathbf{h_0^t}$ . 最终随同Encoder-Decoder 一起训练。其他别的手段通常为： 另一个RNNs, e.g. BiLSTM, LSTM, GRU, other Gated NNs… CNNs Unsupervised Sentence Embeddings Knowledge Base generated representation. 得到$\mathbf{h_0^t}$ 之后，通过 $\mathbf{\hat\alpha_{ij}=score(h_i^t, h_j^s)}$ [^13]再经过$\mathbf{softmax()}$ 从而可以得到最终的注意力权重分配分布$\mathbf{\alpha_{ij}}$。现在就剩最后一个问题了，$\mathbf{score()}$ 函数长什么样子？$$\mathbf{\color{blue}{score(u,v)}}=\begin{cases}u^Tv, &amp;\text{dot product.}\\u^T W_a v, &amp;\text{bilinear, also called general.}\\\omega^Ttanh(W_a[u;v])\ or\ \omega^Ttanh(W_u u + W_v v), &amp;\text{1 layer MLP(i.e. concat)}\end{cases}$$公式中 $W_a, \omega ,W_u,W_v$ 都是参数，与整个网络一起训练。 绝大多数Attention模型都是采用上述框架和流程计算注意力分配权重的概率分布，从而达到多次利用Source句子的重要信息提升任务性能。 以上就是经典的Soft Attention模型，如何理注意力分配权重的概率分布呢？在机器翻译的任务中，$\mathbf{score(h_i^t, h_j^s)}$ 理解为Target句子中第$\mathbf{i}$ 个单词与 Source句子中第 $\mathbf{j}$ 个单词的对齐概率，因此一些文章中也常常把$\mathbf{score()}$ 写成 $\mathbf{align()}$. 所谓对齐，就是说在生成的Target句子中某个单词与Source句子中的哪个或哪些单词是对应的；概率越大，说明越可能有对应关系。 Attention的本质是什么？将面讲过，Attention是独立于Encoder-Decoder框架的存在，单独提出来思考有利于把握Attention的本质思想。在[^paper3] 这Google Brain团队的这篇名文中，作者有一段经典描述现引用如下： An attention function can be described as mapping a query and a set of key-value pairs to an output,where the query, keys, values, and output are all vectors. The output is computed as a weighted sumof the values, where the weight assigned to each value is computed by a compatibility function of thequery with the corresponding key. 引文点明了Attention的本质思想，可以通过下图来理解： 如上图所示，我们可以这样来看待Attention机制： 将Source句子看作是由一系列 key-value 对儿组成，每个对儿对应着Source句子中的一个单词。此时给定Target中某个元素Query，计算Query 与 Source端每个key 的相似性或相关性，即得到每个key对应value的权重系数，然后将Source端所有value值加权求和，即得到最终的Attention值。所以，Attention机制本质上是对Encoder端Source句子中的Unit Information(word embedding)进行加权求和，而用Query和key来计算value的权重; 权重大的意味着其对应的value(information)对于生成Target 相对重要，权重小的，意味其信息相对不那么重要。这个本质思想可以总结为以下公式：$$\mathbf{Attention(\color{blue}{Query, Source})} = \sum_{i=1}^{L_s} \mathbf{Similarity(\color{blue}{Query, key_i})\ast value_i}$$其中，$\mathbf{L_s}$ 是Source句子长度。在之前用来举例的机器翻译任务中，Key 和 value相同，指的是同一个东西即：key=value ，导致不能轻易地看出这种本质思想。 另外一种理解是，将Attention机制看作是一种软寻址(Soft Adressing)：Source 可以看作存储器的内容，元素由地址 key 和值 value组成，当前有一个 key=Query的查询，目的是取出存储器中对应的 value值。通过Query和存储器中的每个key的相关性来寻址，之所以说是软寻址，是因为不像一般的寻址只取出一个地址匹配的值，而是可能从每个key地址中都取出内容，取出内容的重要性由Query 和key 的相关性决定，之后对所有key地址对应的value值进行加权求和，权重即对应的相关系数，最终得到的结果即为 Attention值。不少科研人员将 Attention机制看作Soft-Adressing的特例，也是有其道理的。 Attention的计算过程在上一节已经见过，下面脱离Encoder-Decoder框架做一次总结。绝大部份Attention的计算过程可以归纳为2个过程：1）根据Query 和 key 计算权重系数；2）更具权重系数对value值进行加权求和。 第一个过程又分为2个阶段：确定Query；计算Query与key的相关性。可以将Attention的计算过程抽象为如下图所示的3个阶段： 确定Target中的Query（上一节提到的$\mathbf{h_0^t}$ ） 随机初始化 通过其他表示手段得到，e.g. RNNs, CNNs, Unsupervised Embedding, KB-based representation. 计算$\mathbf{similarity()}$ score(), 有3种常见方式（见上小节） normalization softmax() smooth 根据权重系数对value值进行加权求和，即： $$\mathbf{Attention(\color{blue}{Query, Source})}=\sum_{i=1}^{L_s} \color{blue}{\alpha_i }\ast value_i$$ 只不过，在NLP大多数任务中，key-value 对儿中的key和value是相同的，即：key = value。这在大多数任务的实现中可以明显地看到这一点。大家在实现自己模型的时候注意到这一点即可。 Self-AttentionSelf-Attention 也被称为Intra Attention, 2017年初开始获得了极为广泛地应用，比如 Google的机器翻译[^paper3] ，自然语言理解，语义角色标注（SRL）[^paper4] 等等。 传统的Attention计算过程涉及Target和Source两个方面，需要 Target中的元素 Query 与Source中的每个元素进行计算才能得到。这在Encoder-Decoder 框架下的机器翻译任务中很好理解，这时Target和Source是不同的， 其本质为：Target句子中的元素与 Source句子中元素的对齐程度/概率。 那如果一个NLP任务的 Target 和 Source 相同呢，或者根本就不符合Encoder-Decoder框架的形式而没有所谓的Source 和 Target呢？这个时候就可以使用Self-Attention。顾名思义，Self-Attention指的是在句子内部元素之间进行Attention值的计算，比如：在Source句子内部或者Target句子内部元素之间计算Attention Value。可以理解为传统Attention机制中Source=Target的特殊情况。 其实，我们也很容易将Self-Attention机制引入传统Encoder-Decoder框架，即：除在Target 和 Source之间计算Attention之外，分别在Source元素之间和Target元素之间再次进行Self-Attention计算。 下图可视化地表示了 Self-Attention机制在句子内部究竟学到了什么特征或者提供了什么informative的信息： 图中展示了Self-Attention可以捕获到句子中单词间的语法特征（短语结构：making….more difficult） 上图中展示了Self-Attention 可以捕获句子中单词间的语义特征（指代关系：its 指代 Law 的 Application） 很明显可以看出： Self-Attention可以捕获长距离依赖关系(long dependency), 可以避免RNNs需要不断叠加之前时刻的信息到当前时刻所带来的特征交错复杂难以学习的问题；Self-Attention计算的时候，句子中的任意两个元素直接两两计算权重，单词间距离远近没有影响，不需要像RNNs模型那样等待之前时刻都计算结束才能进行当前时刻的计算，因此可以并行计算提高效率，这是Self-Attention模型的极大优势；相比于CNNs的model，它不需要非常多的kernel所带来的太多参数，也不会有不同的卷积窗overlap所带来的特征组合过于复杂的问题，难以学习到句子中单词间清晰的依赖关系，因此不能很好学到语法结构。 ConclusionAttention机制已经成为NLP领域的一个dominant方法，最新发展是基于 Self-Attention模型的各种变种以及与其他模型如：传统的Attention，LSTM，Bi-LSTM，CNN，Reinforcement Learning等相结合，得到各种组合模型并应用到NLP的各个任务中。2018年的此时此刻，Attention模型依然在发挥着巨大的威力，目前来看能和它匹配的模型就数基于 reinforcement learning 的模型了。个人预测2018年这两种方法依然会是dominant的方法，值得深入研究并迅速占领还没有尝试过的组合，切入到一个具体的NLP任务上出paper! 另外一个新兴起的model是Hinton大神的Capsule，其在NLP领域的应用还非常少，截止2018年4月4日，目前只发现2篇paper采用Capsule做NLP任务，一篇是www2018年上的情感分类文章，另一个是做Text classification的文章，这个方向也非常值得关注！！ PS: Attention权重的概率分布计算下标说明不同的具体实现对使用Decoder端Target句子的哪个时刻隐层表示来与Encoder端Source句子的每个时刻的表示进行score()计算来得到权重概率分布的选择不一，因此下标往往不统一。在生成Target句子的第$\mathbf{i}$个单词的时候， 有些[^paper1]采用Target句子第$\mathbf{h_{i-1}^t}$ 与Source句子的每个单词表示进行计算，得到注意力权重分配的概率分布；有些[^paper2]采用$\mathbf{h_i^t}$ 来计算。所以在下标的确认过程中万一感到混乱，往往是没弄清楚作者是用前一时刻 or 当前时候的 hidden state去计算注意力权重概率分布的，弄清楚这一点，就不会感到下标混乱了。 致谢本文主要参考内容为： 张俊林博士的文章- 《深度学习中的注意力机制》文章链接 罗凌同学的文章 - 《自然语言处理中的自注意力机制（Self-Attention Mechanism）》文章链接 苏剑林同学的文章- 《一文读懂「Attention is All You Need」| 附代码实现》文章链接 Blog-Attention Mechanism Reference and Concept Explanation[^1]: 通常是句子，也可以是paragraph，document 等不同粒度的token(word, char, etc.) sequence；还可以是dependency tree等树结构。[^2]: 一个character，token，entity，phrase，sentence，paragraph 等语言单元均可，这里用单词是因为单词是最常见的语言单元，为简化起见。[^3]: 有些文章中也叫做Semantic representation or Semantic Encoding，但是我们知道自然语言中学习到的表示通常不仅包含Semantic，也同时包含Syntactic information，Sentiment information等其他信息。因此这里采用一个更加粗粒度的说法：Text representation。[^4]: 通常为word sequenc，也可以是character sequence 或者其他的sequence。比如在 entity extraction的任务重，就是B-I-O标签的sequence，在SRL 和 POS Taging中是其他类型的序列。[^5]: 也可以是一个matrix或者多维的tensor，视具体的应用及模型而定。[^6]: 据不记得名字的某个大牛说过：专业的NLPer遇到NLP中的问题，自然会转化为序列问题看待，即使原问题形式上不会一个序列问题。[^7]: 这里的词序列只是最常用的一种情况，实际上可以是任何的token sequence，比如：character，tag，label等的不同粒度语言单元的序列。[^8]: 严格来讲，这样说不完全正确，因RNNs在对Source句子进行编码的时候其实implicitly地考虑了不同词的重要性。这是由RNNs网络本身的特性决定的；只是没有“郑重”地将这个问题建模而已。[^9]: 英文的word对应中文的词，character 对应中文的字。[^10]: 第一次利用整个句子是在Encoder阶段。[^11]: g的功能是将每个单词的表示合成整个句子的表示，因此可以叫做 composition function。[^12]: 在不同的具体实现中，在Decoder端i 时刻不一定是Target句子生成第i 个单词，也可能是第i-1 个单词。这仅仅是实现细节上的区别，不影响整个的理论框架。[^13]: 这里注意本文最后的部分，对下标进行说明。[^paper1]: 《Neural Machine Translation by jointly Learning to Align and Translate》Bahdanau et al., ICLR, 2015.[^Paper2]: 《Effective Approaches to Attention-based Neural Machine Translation》Luong et al., EMNLP, 2015.[^paper3]: 《Attention is all you need》Vaswani et al., NIPS, 2017.[^paper4]: 《Deep Semantic Role Labeling with Self-Attention》Tan et al., AAAI, 2018.]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Attention</tag>
        <tag>Encoder-Decoder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CMU-Neural Networks for NLP-学习笔记（一）：Introduction]]></title>
    <url>%2Farchive%2F2018-03-30%2FCMU-Neural-Networks-for-NLP-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9AIntroduction%2F</url>
    <content type="text"><![CDATA[to-do (in coming 2 days)]]></content>
      <categories>
        <category>课程笔记</category>
      </categories>
      <tags>
        <tag>CMU</tag>
        <tag>Neural Works</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Milestone of Embedding-a Long Paper List]]></title>
    <url>%2Farchive%2F2018-03-28%2FMilestone-of-Embedding-A-Long-Paper-List%2F</url>
    <content type="text"><![CDATA[Begion et al. 2003todo Schwenk 2006to Collbert and Weston 2008to Chen and Manning 2014todo ##]]></content>
      <tags>
        <tag>Feature Embedding</tag>
        <tag>Word Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多元正态分布]]></title>
    <url>%2Farchive%2F2018-03-26%2F%E5%A4%9A%E5%85%83%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[$\color{blue}{Updating \cdots}$在概率或者统计中，多元正态分布（多元高斯分布）是一元正态分布的扩展。其定义为: 定性描述 一个$k$维的随机向量，如果其$k$个分量的任意线性组合所构成的那个随机变量，都是一元正态分布的话，那么我们就说这个$k$维随机向量是服从$k$元正态分布。 $k$元正态分布常用来描述、逼近 这样的一组随机变量：实值随机变量的集合，这些随机变量之间（可能）有相关关系，并这些随机变量所构成的每一个cluster的中心都在某一个均值（集合中若干随机变量的均值）附近。 图示如下 概念及参数化 $k-dim$ 随机变量$\mathbb{X}=[X_1，X_2，\cdots,X_k]^T$ 的高斯分布可以写作：$\color{blue}{\mathbb{X} \sim \mathit{N(\mu,\Sigma)}\,\,Or\,\, \mathbb{X} \sim \mathit{N_k}(\mu,\Sigma) }$其$k-dim$ 均值（mean vector）为：$\color{blue}{\mu = E[\mathbb{X}]=[E[X_1],E[X_2],\cdots,E[X_k]]^T}$其$k \times k-dim$的协方差矩阵（covariance matrix）为：$ \color{blue}{\Sigma = E[(\mathbb{X}-E[\mathbb{X}])(\mathbb{X}-E[\mathbb{X}])^T]}\color{blue}{= [Cov[\mathbb{X_i},\mathbb{X_j}]; 1&lt;i,j&lt;k]} $ 要点： 均值 $\color{blue}{\mu}$ 是一个 $k-dim$ 的向量 协方差矩阵 $\color{blue}{\Sigma}$ 是一个$k\times k$ 的对称矩阵 定义一个随机向量 $\mathbb{X}=[X_1,X_2,\cdots,X_k]^T$ 如果满足如下3个等价条件，那么我们就说 $\mathbb{X}$ 服从多元高斯分布: 其分量（components）的每一个线性组合：$Y=a_1X_1 + a_2X_2 + \cdots +a_kX_k$ 均为一元正态分布。也就是说，对任何常数向量 $a\in \mathbb{R}^k$, 随机变量 $Y=a^T X$ 都有一个一元高斯分布。 todo todo 属性由定义立即可以得到以下结论： $\mathbb{X}$ 的每个分量都是正态分布的 $\sum_{i=1}^{k} X_i = X_1+X_2+\cdots+X_k$ 是正态分布的 每个边缘分布均是正态分布]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Multivariate Normal Distribution</tag>
        <tag>高斯分布</tag>
        <tag>多远高斯分布</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中如何排序]]></title>
    <url>%2Farchive%2F2018-03-15%2Fpython%E4%B8%AD%E5%A6%82%E4%BD%95%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[sort()具体形式：iterable.sort(cmp[, key[, reverse]])12a = ['3','5','1','7']a.sort() 要点： sort()是list的方法，只能用于list而不能用在其他的iterable对象 会修改调用它的list对象本身 sorted()具体形式：sorted(iterable, [key=None,reverse=False]) sorted() 常常用于对list进行排序，而且不改变list本身。实际可用于任何可迭代对象(iterable)123a = [3, 5, 1, 7]print(sorted(a)) ## [1, 3, 5, 7]print a ## [3, 5, 1, 7] reverse控制排序方向它的可选参数之一是： reverse. e.g. reverse = True, sorted(list, reverse = True) 将得到反向排序的结果。123strs = ['aa','BB','zz','CC']print(sorted(strs)) ## ['BB', 'CC', 'aa', 'zz']print(sorted(strs, reverse = True)) ## ['zz', 'aa', 'BB', 'CC'] key= ‘定制化排序‘可在排序之前对对象的每个元素做处理，然后对处理结果进行排序。另一个例子： 12strs = ['ccc', 'aaaa', 'd', 'bb']print(sorted(strs, key = len)) ## ['d', 'bb', 'ccc', 'aaaa'], 实现了对list中的字符串的长度进行排序 要点： 返回一个新的、排好序的list,不改变原始的iterable key 和 reverse 是两个可选参数，但必须作为keyword arguments被指定。e.g.: reverse = False key指定了一个只含一个参数的、用于从list的每个元素选择比较的对象的函数（比较绕口，其实就是指明比较list中元素的什么？长度？首字母？or 其他）：key = str.lower(). 默认值为None，即：直 接比较list中的元素 sort_values()详解：DataFrame.sort_values(by, axis=0, ascending=True, inplace=False, kind=’quicksort’, na_position=’last’)) 参数 说明 by 2 axis x ascending x inplace x kind na_position 返回值：storted obj(DataFrame)]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sort</tag>
        <tag>list</tag>
        <tag>dict</tag>
        <tag>排序</tag>
        <tag>列表</tag>
        <tag>字典</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy简明教程]]></title>
    <url>%2Farchive%2F2018-03-12%2FNumpy%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Numpy的核心功能是围绕ndarray（多维数组）展开。多维数组是计算机科学的概念，其与数学概念的对应关系如下：1-d array $\longleftrightarrow$ $\color{blue}{vector}$2-d array $\longleftrightarrow$ $\color{blue}{matrix}$3-d array $\longleftrightarrow$ $\color{blue}{tensor}$(大于3-dim的数组还是tensor)注意：这里的维度概念讲的不是一个vector或者array中component的个数，而是指index的个数。 numpy两个核心目的 numpy创建ndarray，并提供对应的增、删、查、改等操作。 提供数据操作的vectorization，提高效率。 主要属性1234567a.ndim #维数a.shape #维数及大小，e.g. (2,3),2维，2rows 3colsa.size #存储元素的个数a.dtype #所存储元素的数据类型a.itemsize #存储元素的字节数a.T #数组a的转置a.flat #铺平元素的一个迭代器 ndarray的所有元素必须是同一种类型数据，即：dtype一样。但dtype可以是各种其他的数据结构。 首先导入numpy包1import numpy as np 创建ndarray从list 生成12a = np.ndarray([1,2,3])b = np.ndarray([[1,2,3],[4,5,6]]) 直接生成12c = np.zeros(shape=(2,3),dtype=np.float32) #生成全0的2-dim array，2 rows and 3cols,元素类型为np.float32d = np.ones(shape=(2,3),dtype=np.float32) 间接生成12c2 = np.zeors_like(c)d2 = np.ones_like(d) 随机生成12np.random.rand() #均匀分布np.random.randn()#正态分布 常见操作upcoming…]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>numpy</tag>
        <tag>tutorials</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面上项目计划书纸质版提交流程]]></title>
    <url>%2Farchive%2F2018-03-12%2F%E9%9D%A2%E4%B8%8A%E9%A1%B9%E7%9B%AE%E8%AE%A1%E5%88%92%E4%B9%A6%E7%BA%B8%E8%B4%A8%E7%89%88%E5%AD%A6%E6%A0%A1%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[本文旨在说明自然科学基金面上项目计划书纸质版在交大内部提交流程，涉及如何打印，去哪里、找谁签字或盖章，最后交给谁的问题。 计划书打印 双面打印 一式两份 盖章 课题负责人签字 财务部门盖章 去主楼A 205 找相关老师办理用印单。（期间要复印计划书中的某些页，205就有复印机，可就地复印） 拿着上一步得到的用印单去主楼E1104 室找孙老师盖财务部门章。 提交完成上述两步之后，就可以将两份计划书提交到电信学院345 黄文老师处即可。之后由黄文老师同意将电信学院所有计划书提交到学校科技部门（主楼E 3楼）加盖科技部门公章，不过这个步骤就不需要办理纸质材料的个人考虑了。至此，事毕。]]></content>
      <categories>
        <category>工作</category>
      </categories>
      <tags>
        <tag>面上项目</tag>
        <tag>计划书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Conda常用命令小结]]></title>
    <url>%2Farchive%2F2018-03-12%2Fconda%E7%94%A8%E6%B3%95%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[检查conda 是否安装以及在你系统的path中 1conda -V 升级 conda 1conda update conda 创建一个虚拟环境 1conda create -n yourenvname python=x.x anaconda or 1conda create --name yourenvname python=x.x anaconda 激活你所创建的虚拟环境 1source activate yourenvname (in linux) or 1conda activate yourenvname (in windows) 在你创建的虚拟环境中安装 python package 1conda install -n yourenvname [package] 退出你当前的虚拟环境 1source deactivate yourenvname 删除一个虚拟环境 1conda remove -n yourenvname --all]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>conda</tag>
        <tag>anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas简明教程]]></title>
    <url>%2Farchive%2F2018-03-12%2FPandas%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[常用属性xxx 常用method12换列名pd.rename(columns = &#123;key:value&#125;, inplace = True) key：我们想替换的原始列名value：我们最终要的列名]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梯度下降算法]]></title>
    <url>%2Farchive%2F2018-02-07%2F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[[TOC] 什么是梯度？GDSGDMini-bacth SGD更新历史]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Gradient Descent</tag>
        <tag>SGD</tag>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的音乐]]></title>
    <url>%2Farchive%2F2018-01-04%2Fyear-01-04-%E6%88%91%E7%9A%84%E9%9F%B3%E4%B9%90%2F</url>
    <content type="text"><![CDATA[I love this songs.]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>music</tag>
        <tag>音乐</tag>
        <tag>感念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典视频]]></title>
    <url>%2Farchive%2F2018-01-04%2Fyear-01-04-%E7%BB%8F%E5%85%B8%E8%A7%86%E9%A2%91%2F</url>
    <content type="text"><![CDATA[全中国人民团结起来，艰苦奋斗，移风易俗，改造我们的国家。我们的工作必将写在人类的历史上，它将表明占人类总数四分之一的中国人民从此站立起来了！我们的目的一定要达到，我们的目的一定能够达到！ 毛泽东 政治 思$\cdot$享$—$《中国道路及未来》 一个为国而争的科学家和一个公知、美粉之间的对话。 投票不代表民主 胡鞍钢： 2050展望中国 金灿荣： 国际形势的特点与十九大后的中国外交 科技 有人觉得中国科技很弱，因为全世界除了中国只有一个国家：“外国” 科技猿人： 钱学森对当年的中国到底多珍贵？ 工业化及经济经融 破解中国工业化之谜 天下 未来十年的世界大变局- 金灿荣 一个英国人眼中的中国，人类未来伟大事业的希望。 新时代的中国信心$—$张维为]]></content>
      <categories>
        <category>some insightful</category>
      </categories>
      <tags>
        <tag>经典</tag>
        <tag>视频</tag>
        <tag>政治</tag>
        <tag>中国道路</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[几个男人的信念]]></title>
    <url>%2Farchive%2F2018-01-03%2Fbelief%2F</url>
    <content type="text"><![CDATA[毛主席 为全天下的受苦人，贡献自己的一切。 为人民服务！ 习大大 面向世界科技前沿 面向国民经济主战场 面向国家重大需求 孙中山 革命尚未成功，同志仍需努力。 林则徐 苟利国家生死以，岂因祸福避趋之。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>信念</tag>
        <tag>毛主席</tag>
        <tag>男人</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Simple Bu Tough-to-Beat Baseline for Sentence Embeddings -Note]]></title>
    <url>%2Farchive%2F2017-12-28%2Fyear-12-28-simple-SE%2F</url>
    <content type="text"><![CDATA[这篇论文的宏观描述 基于神经网络计算 word embedding在多个下游领域的成功极大地激发了为长文本生成语义嵌入（semantic embeddings）的方法，比如句子、段落、文档。 直接生成长文本的embedding 的方法竟然被一种很简单的策略击败： Wieting et al.(ICLR’2016)指出，这种复杂方法被超越了：采用 对已经的word embeddings做retraining, 结合logistic regression方法就可以达到这个目的。 在retraining的过程中需要结合一个外挂的数据库：paraphrase database(Gantikevitch et al., 2013) 本文的研究更进一步，显示出本文提出的完全无监督sentence embedding（句子嵌入）方法是一种强大的baseline 以现成的已经在大量无标签数据上（如：wikipedia）训练好的word embedding 为基础 通过一种对word embedding 加权平均的方式来表示句子，即：sentence embedding 对上述得到的“句子表示”（sentence embedding）稍作修改，用：PCA/SVD（当然也可以试试：autoencoder） 本文这种方法在文本相似任务中可以将性能提高：10%～30%。超越了复杂的有监督模型入：RNN 和 LSTM 本文的基本方法： 计算句子中词向量的加权平均 移除average vector 在 其 first principal component 方向的projection. 这里，一个词 w 的权重为 $\frac{a}{a+p(w)}$ 。 a 是一个参数，$p(w)$ 是 估计得到的词频（the estimated word frequency）. 这一项叫做 SIF–“smooth inverse frequency”. 针对每个句子，做前两部操作，从而得到所有句子的 sentence embeddings. 与其他模型、方法的渊源 与$TF-IDF$的关系 都是一种加权策略（weighting policy）, SIF 更是一种reweighing policy $ tf-idf $ 是IR领域著名方法之一，将一个query or sentence 看作document并做了一下假设： tf-idf: 解释一下 $ tf$ 和$idf$的意义。 一个词通常在一个query（sentence）中不多次重复出现。 一个document中term frequency可以表明一个词的重要程度。 一个term在corpus中所有document中被包含的次数，可以反应这个词的普遍性。 与word2vec的关系 在word2vec的基础之上，增加一个reweighting策略。 word2vec被误认为是没有采用加权策略的，但实际上在深度研究其实现之后，发现word2vec隐式地采用了加权策略。而且不同于$tf-idf$ , 反而是和本文方法比较类似。 相关工作 Word embeddings 可以捕获词的lexcial and semactic features 。 可以用neural network model从文本的表示中获取 也可以从词共现统计的低秩近似来获得 Random walk model for generating words in the documents. 我们的模型可以看作是：在这个模型中对隐变量的近似推理。 Long peices of text embeddings(Phrase/Sentence/Paraphrase) embedding Coordinate wise operation Unweighted average policy perform well in representing short phrase. RecNNs Based on a parse tree or not. Latent vector assumption. Le &amp; Mikolov, 2014 assume that each paragraph has a latent vector, namely paragraph vector.Skip-thought model Taking advantage of another lexicon.(外挂词典或知识库) Wieting et al., 2016 learnd paraphrastic sentence embeddings using: word averaging 基于paraphrase paris 的监督更新standard word embeddings. 在初始化和训练阶段都有supervision。 本文模型 - A simple method for sentence embeddingBrief introduction of the latent variable generative model for text. Arora et al., 2016该模型 将语料生成看做一个动态过程，且这一过程是由discourse vector $c_t\in R^d$ 的随机游走驱动的。(Treats corpus generation as a dynamic process , where the $t$-th word is produced at step $t$，the process is driven by the random walk of $c_t\in R^d$.) Discourse vector $c_t\in R^d$ ，discourse vector 表示“（文本）正在讲什么？” Each word in the corpus has a word vector $v_w\in R^d$ , which is the latent variable of the model， and it’s Time-invariant. $c_t \cdot v_w$ : correlations between the discourse and the word. $$P_r[w \ enitted\ at\ time\ t|c_t]\propto exp(\langle c_t\,,v_w\rangle). (1)$$ The discourse vector $c_t$ does a slow random walk, so that the nearby words are generated under similar discourses. Slow means that $c_{t+1}$ is obtained from $c_t$ by adding a small random displacement vector. 结论 该随机游走模型（Arora et al., 2016）可以放宽约束条件：允许在$c_t$中有偶尔的、大步长的跳跃，计算表明这样对于词共现的概率影响可以忽略不计。 在一些合理假设之下，随机游走生成模型 与 word2vec 和 Glove等工作很像。 ？？？ Our improved Random Walk Model明显地，人们很容易将sentence embedding 按照如下方式定义：在给定一个句子的前提下，对控制这个句子的discourse vector 做一个MAP估计。 做一个假设： The discourse vector $c_t$does not change much while the words in the sentence emitted. 因此我们用一个单独的discourse $c_s$ 去代替句子 $s$ 中的所有 $c_t$’s . In (Arora et al., 2016), it was shown that MAP estimate of $c_s$ is up to multiplication by scalar— the average of the embeddings of the words in the sentence. Change the models as follows: 考虑到两种情况：1）一些词总是出现在context之外 2）一些常见词的出现与discourse无关。提出2种“smoothing term” 首先，引入一个additive term, $\alpha p(w)$ in the log-linear model. $p(w)$ 是整个语料中词$w$ 的 unigram probability, $\alpha$ is scalar. 这就允许即使词向量与$c_s$ 的内积比较低的那些词也可以出现。 第二，我们引入一个 common discourse vector $c_0\in R^d$. Serves as a correction term for the most frequent discourse that is often related to syntax.Boosts the co-occurence probability of words that have a high component along $c_0$ . 给定了$c_s$ , 词$w$ 在句子$s$被射出的概率可以建模为如下形式:$$P_r[w\ emitted\ in\ sentence\ s\,|\,c_s]=\alpha p(w)+(1-\alpha)\frac{exp(\langle \hat c\,,v_w\rangle)}{Z_{\hat c_s} }$$ $$where\,,\hat c_s=\beta c_0+(1-\beta)c_s, c_0\perp c_s$$ $$and\,, Z_{\hat c_s}=\sum_{w\in \nu}exp(\langle \hat c_s\,,v_w\rangle)$$ Our models allow a word $w$ unrelated to the discourse $c_s$ to be omitted for two reasons: by chance from the term $\alpha p(w)$ if $s$ is correlated with the the common discourse vector $c_0$. Computing the sentence embedding 可以再表示为如下形式： ​ 计算$f_w(\hat c_s)$对 $\hat c_s$的一阶偏导数： 然后对上述梯度公式做Taylor展开： 因此，$\hat c_s$的最大似然估计近似等于： 根据上述结果，可以有以下结论： MLE 是一种对在句子中的词的词向量进行加权平均的近似。 注意到，对于频繁出现的词，权重$\frac{a}{a+p(w)}$ 会取一个比较小的值。因此很自然的对于频繁词（in the entire corpus）赋予一个比较低的权重。 为了估计$c_s$，通过 计算$\hat c_s$’s 的第一主成分(for a set of sentences)去估计$c_0 $ 换句话说， 最终的sentence embedding 要减去所有 $\hat c_s$’s 对其第一主成分的投影。 这一过程总结在Algorithm 1 里，如下： Algorithm 1 Sentence Embedding 本工作与Word2Vec中的下采样概率有什么关联？ 上图是在 Word2Vec 模型和我们模型中权重岁词频变化的曲线，可以看到两个模型中，权重与$p(w)$ 之间的关系十分相似。Word2Vec模型中， 梯度的期望是我们模型中估计得到的discourse vector 的一种近似。因此结论为： word2vec with sub-sampling gradient heuristic corresponds to a stochastic gradient update method for using our weighting scheme. 实验与结果分析文本相似任务（Textual Similarity Tasks） 数据集： 22 textual similarity datasets including all the datasets from SemEval STS tasks(2012-2015). SemEval 2015 Twitter task, SemEval 2014 Semantic Relatedness task. 这些任务的目标是：预测给定的两个句子之间的相似度。 评测标准(Evaluation criterion): Pearson’s coefficient. 实验设置. 采用以下设置来比较我们的模型与其他模型的优劣： 无监督学习 ST avg - GloVe tfidf-GloVe 半监督学习 avg - PSL. （from Wieting et al., 2015） 有监督学习 PP &amp; PP-proj.( from ) DAN (Iyyer et al. 2015) RNN iRNN LSTM (Gers et al., 2002) 结果 各方法性能对比： Better than LSTM and RNN. Comparable to DAN. 上述三种方法均为有监督学习。 结论 本文提出的简单模型甚至优于highly-tuned的有监督的复杂模型。 采用tf-idf加权策略相比不加权方法性能也有所提升，但依然没本文模型效果好。 半监督方法 PSL+WR取得了6各任务中4个最好结果。 同时注意到，各数据集的 top singular vectors $c_0$ 似乎大略地相当于语法信息或者常用词，在SICK数据集中与$c_0$ 最接近的词是 “justt”, “when”, “even”,…, “while.”, etc. 最终，在附录中 我们证明了两个idea 都对性能提升做出了贡献： 对于 GolVe 向量， 单独采用ISF加权策略就提升了5%， 单独采用 common component removal 提升了 10%. 两者都用提升了 13%. 权重参数对于性能的影响我们研究了我们的方法对于 权重参数、计算word vecotrs的方法、以及 对 估计的词的概率 $p(w)$ 这三者的敏感性。图如下： 结论： 参数a 的范围在 $10^{-4} - 10^{-3}$ 之间，性能最佳。 固定a = $10^{-3}$ 的前提下，在四个数据集中对$p(w)$做估计，在测试中性能相当。 我们的方法可以被应用到多种语料、多种计算词向量的方法中，这也显示了我们的方法有助于跨越不同的领域。 有监督任务采用我们方法得到的sentence embedding可以被当做特征而应用到下游的应用中。我们考虑了三种任务： SICK similarity task SICK entailment task SST binary classification task 方式如下： 固定 embeddings 而仅仅学习classifier. i.e., a linear projection (Kiros et al., 2015) 与 PP, DAM, RNN, LSTM等方法做了对比。 与 skip-thought 的方法做了对比（Lei Ba et al., 2016） 结果： 结论： 我们的方法better or comparable. 我们采用完全无监督方法得到的 sentence embedding 甚至好于DAN, RNN, LSTM等有监督训练所得的结果。 Skip-thought vectors 的维度比我们的高很多。 性能提升没有像在textual similarity tasks中那么显著。 原因可能是，在文本相似任务直接依赖cosine similarity, 而我们一处common components的做法比较符合cosine similarity的口味。而有监督任务中，标签提供了监督信息，使得分类器可以在监督之下挑选出有用的信息而忽略掉那些common的。 我们推测在情感分类任务中没有超越RNN 和 LSTM的原因是： 词向量，或者更加广泛地说 词义的分布式假设，由于反义词现象使得其对于捕获情感特征的能力受到制约。 为了解决这一方法，可以针对情感分析任务学习更好的word embeddings。比如：（Maas et al., 2011 &amp; Duyu Tang et al., 2014） 在我们的权重平均策略中， 诸如 “not”等否定词被大大地减少了权重。但是在情感分类任务重，这类词很重要，显然应该赋予较高的权重。 针对具体的任务（情感分析），设计独有的加权策略（or 学习权重） 句子中的词序信息的作用我们方法的一个有趣的特点是：它忽略了词序信息。 可是相较于RNN, LSTM 等可以潜在利用词序信息的方法，在这些benchmarks上我们却取得了better or comparable的方法，这就引出一个问题： 在这些benchmarks中，词序真的重要吗？ 实际上，词序还真的重要。 通过实验证明：在有监督任务上，把句子中的词随机搅乱，然后再训练和测试 RNN and LSTM. 结果，性能下降很多。 同时结合之前的发现：在有监督任务中，我们的方法- 忽略词序，但在两个任务中取胜，在一个任务中(sentiment analysis)性能低一些(但是比打乱词序之后的RNN 和 LSTM要好很多)。这就说明： 我们的方法一定在 exploiting semantics 方面强过 RNN and LSTM. 最后的最后 本文提供了一种简单的基于随机游走文本生成模型中 discourse vectors 获得sentence embedding 的方法。 它简单且无监督，性能优越，超过很多诸如：RNN 和 LSTM等方法。 其可用在下游任务中。 $$Thank\,\,\,you\,\,\,for\,\,\, reading!$$ 论文中待进一步研究的概念 first principal component (“common compinent removal”) what’s the exact meaning of latent variables? ramdom walk in text domain Maximum a posteriori (MAP)Estimation 复习Taylor级数单元，多元 变量。 补充Pearson’s coefficient 相关内容。 MLE 和 MAP 在哪个Context 下，是指对参数的estimate。 值得深入研究。 Perarson’s 系数简单了解一下，弄清原理和推论过程。 log-linear word production model (from Mnih and Hinton) 强相关性论文 Wieting et al., 2016 Arora et al., 2016 Lei Ba et al., 2016 Bownman et al., 2015 Kiros et al., 2015]]></content>
      <categories>
        <category>NLP</category>
        <category>text embedding</category>
      </categories>
      <tags>
        <tag>sentence representation</tag>
        <tag>embeddings</tag>
        <tag>baseline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在校大学生出国基本流程]]></title>
    <url>%2Farchive%2F2017-12-23%2F%E5%9C%A8%E6%A0%A1%E5%A4%A7%E5%AD%A6%E7%94%9F%E5%87%BA%E5%9B%BD%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[这是我交学生短期出国(开会)的流程总结，是2017年秋季的标准流程。随着时间的推移，流程可能有稍许变化，但基本组成应该不会变化太多。大家以后有需要的时候可以用作参考，为大家节约一点宝贵的时间。有需之时，此文仅仅作参考之用，最新信息还是要从学校或者实验室相关文件中获得。因此文所造成的物质、精神损失，本人概不负责 ^-^ ! 办理护照 所需资料： 身份证（必须） 户口本或者户口迁移证（不必须，带着是为了以防万一） 地点： 户口所在地的出入境管理处（可百度之） 作为大学生，如果要异地办理（在学校所在地办理护照，而户口在别的地方），必须在办理护照之前在所在学校办理在读证明。 费用： 以西安为例，收取160元的工本费 办理周期： 一般在6个工作日内可以拿到 可以选择快递至自己所在地址或者本人亲自带着身份证和回执单去出入境管理处领取。 学校手续办理申请流程 需准备的材料 “西安交通大学研究生访学计划申请表”（一份，下载） “研究生短期出国协议” （一式三份, 下载） “西安交通大学研究生因公出访审批表” （一份，下载） 资助证明(一份，下载，A4打印) 邀请信复印件(两份)、翻译件（一份） 论文首页(两份；申请人为第一作者或导师第一作者、申请人第二作者) 日程安排（5天之内）或超天申请（5天以上）一份 （要求详见出访审批表说明） 提交相关材料 提交以下材料电子版（单个文件大小不超过1MB）至工作信箱： “西安交通大学研究生访学计划申请表”（完成导师、学院签字盖章，PDF格式） 邀请信（PDF/JPEG格式） 论文首页（PDF/JPEG格式） 资助证明（word文档,内容填写完整） 提交申请2个工作日后，至主楼E座1310室邓老师处领取资助证明 交研究生院马老师（主楼E座207室；82668991）以下材料： “研究生短期出国协议” “西安交通大学学生出访审批表” （审核前需导师、学院签字盖章） 带正式邀请信的复印件、翻译件各一份；参会论文首页一份；日程安排（5天之内）或超天申请（5天以上）一份；连同研究生院已签字的“西安交通大学学生出访审批表”，交国际处出入境办（科学馆116室，82668521）办理出国批件。（出国批件必须在出国之前办理） 如需在读证明，主楼A座202室门前自助打印机使用校园一卡通自助打印，A202办公室盖章。 借款流程（借款很麻烦，不推荐！） 需先预定机票，取得机票相关信息（如：价格、账户信息等） 至研究生院马老师处申请经费（主楼A座202室，需附出国批件复印件、签证） 至2楼财务大厅继续办理借款手续。 最好是自己先垫付，然后回来报销。借款很麻烦，实验室师兄亲身经验。 借款范围往返国际旅费，会议期间住宿费（以邀请信时间为准）。其中，机票费用为财务处转账，每人每日住宿按照财务处有关规定执行。使用“一流大学建设项目”经费。 资助内容 资助标准 往返国际旅费 亚洲以外地区： 1万元 会议期间住宿费（以邀请信时间为准） 亚洲地区（除港澳台）：8000元 会议期间住宿费（以邀请信时间为准） 港澳台地区：4000元 报销流程走学校层面报销（每周四下午办理） 面向学院、系所完成一次公开报告，撰写会议总结(模板 )，提交学院负责老师（学院教学主管王枫老师）留存；开具《出国项目成果报告单》，学院负责老师（教学主管王枫老师）审核，签字盖章。 携票据、《出国项目成果报告单》至研究生院马老师处（主楼A座202室，电话 82668991）办理报销手续（国内购机票者须附购票发票）。费用超出部分自理。 务必于会议结束回校后一月内完成报销手续。 走实验室内部 所有票据收集齐全，交给陈丽莉老师即可。 签证办理下面的内容以新加坡为例（相对容易）。 中国不是新加坡的免签国家（截至2017/10/9为止），因此入境新加坡需要办理签证。但是新加坡大使馆和各地领事馆从2014年起已经不亲自受理签证办理事宜，都是交由其制定的旅行社办理。目前在西安还没有其注定的旅行社，但是可由西安康辉国际西安旅行社代为办理前往新加坡的签证。 流程如下： 途径一：通过西安本地旅行社代为办理-费用大致为350元/位本人所选代为办理签证的旅行社为：西安康辉旅行社 准备材料： 身份证复印件 两张、两寸白底证件照（要求Kodak冲洗的质量，不能用随便打印的那种照片。切记切记！） 户口本（or 户口迁移证）复印件 学生证复印件 注意，是学生证，不是学生卡！ 在读证明（交大主楼A-202门口自助打印机打印） 护照（扫描件） 咱们打印机也可以扫描。也可以用全能王扫描好之后再打印出来。但绝不能用复印的，切记！ 银行卡1万元以上、存期三个月，需银行开具证明。 途径二：可直接通过成都旅行社办理签证。大概300/位，如果加急，可以在3天内完成，价格：399/元 需要准备材料与途径一相同。]]></content>
      <categories>
        <category>工作</category>
      </categories>
      <tags>
        <tag>出国</tag>
        <tag>手续</tag>
        <tag>流程</tag>
        <tag>大学生</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Farchive%2F2017-08-11%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
